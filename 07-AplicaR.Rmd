
# Bibliotecas de R para Inferência Bayesiana {#R}

Nessa seção serão apresentadas algumas bibliotecas do R para inferência Bayesiana, em especial, `LaplacesDemon` e `Stan`, que são bibliotecas utilizadas para simular dados da posteriori. Alguns dos gráficos apresentados nessa seção serão construídos com bibliotecas específicas para avaliar amostras da posteriori, `ggmcmc` e `bayesplot`. 

## O Modelo de Regressão Linear

Inicialmente, será apresentado como exemplo o modelo de regressão linear, possivelmente um dos métodos mais usados nas aplicações de inferência estatística. Considere $n$ observações de uma variável aleatória de interesse (chamada de *variável dependente* ou *variável resposta*) e de $p-1$ características associadas a cada uma dessas observações (chamadas de *variáveis dependentes* ou *explicativas* ou *covariáveis*), supostamente fixadas. Um modelo de regressão linear pode ser escrito como

$$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$
com
$~\boldsymbol{Y} = \left[\begin{array}{c} Y_1\\ Y_2\\ \vdots\\ Y_n \end{array}\right]~~$;
$~~~\boldsymbol{X} = \left[\begin{array}{cccc} 1 & x_{11} & \cdots & x_{1,p-1}\\ 1 & x_{21} & \cdots & x_{2,p-1}\\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & \cdots & x_{n,p-1} \end{array}\right]~~$;
$~~~\boldsymbol{\beta} = \left[\begin{array}{c} \beta_1\\ \beta_2\\ \vdots\\ \beta_p \end{array}\right]~~$;
$~~~\boldsymbol{\epsilon} = \left[\begin{array}{c} \epsilon_1\\ \epsilon_2\\ \vdots\\ \epsilon_n \end{array}\right]~~$;
$~~~\boldsymbol{Z} = \left[\boldsymbol{X,Y}\right]~~$,

em que $\boldsymbol{Z}$ é a matriz de dados (observada), $\boldsymbol{\beta}$ é o vetor de parâmetros e $\epsilon_i$ é o *"erro aleatório"* associado a $i$-ésima observação, supostamente c.i.i.d. com distribuição $\textit{Normal}(0,\sigma^2)$.

De forma equivalente, o modelo pode ser escrito como $\boldsymbol{Y}|\boldsymbol{X},\boldsymbol{\beta},\sigma \sim \textit{Normal}_{~n}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ com $\boldsymbol{\mu}=\boldsymbol{X}\boldsymbol{\beta}$ e $\boldsymbol{\Sigma}=\sigma^2\boldsymbol{I}$.

$~$

Na abordagem *frequentista*, se $\boldsymbol{X}'\boldsymbol{X}$ é não singular, os estimadores de máxima verossimilhança para os parâmetros $(\boldsymbol{\beta},\sigma^2)$ são, respectivamente,  $\hat{\boldsymbol{\beta}} = (\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\boldsymbol{Y}$ e $s^2 = \dfrac{(\boldsymbol{Y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})'(\boldsymbol{Y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})}{n-p}$.

$~$

**Exemplo.** Vamos considerar um simples exemplo de regressão linear, com apenas uma covariável. Para isso, considere as variáveis `speed` e `dist` do conjunto de dados `cars`, disponível no R. Um ajuste usando a abordagem frequentista é apresentado a seguir.

```{r boringreg}
# a boring regression
fit = lm(speed ~ 1 + dist, data = cars)
coef(summary(fit)) # estimativa dos betas
(summary(fit)$sigma)**2 # estimativa do sigma^2
ggplot(cars, aes(y=speed, x=dist)) + theme_bw() +
  geom_point() + geom_smooth(method=lm)
```

$~$

$~$

Sob a abordagem *bayesiana*, a distribuição Normal-Inversa Gama (*NIG*) é uma priori conjugada para $\boldsymbol{\theta} = (\boldsymbol{\beta},\sigma^2)$ neste modelo. Assim,
$$(\boldsymbol{\beta},\sigma^2) \sim \textit{NIG}(\boldsymbol{\beta}_0, \boldsymbol{V}_0, a_0, b_0)~.$$

Isto é,

$\boldsymbol{\beta} | \sigma^2 \sim Normal_p\left(\boldsymbol{\beta}_0,\sigma^2\boldsymbol{V}_0\right)~~$; $~~ \sigma^2 \sim InvGamma\left(a_0,b_0\right)$

ou, equivalentemente,

$\boldsymbol{\beta} \sim T_p\left(2a_0; \boldsymbol{\beta}_0,\frac{b_0 \boldsymbol{V}_0}{a_0}\right)  ~~$; $~~  \sigma^2 | \boldsymbol{\beta} \sim InvGamma\left(a_0 + \frac{p}{2},b_0 + \frac{\left(\boldsymbol{\beta}-\boldsymbol{\beta}_0\right)^T \boldsymbol{V}_0^{-1} \left(\boldsymbol{\beta}-\boldsymbol{\beta}_0\right) }{2}\right)~~$,

com $\boldsymbol{\beta}_0 \in \mathbb{R}^p$, $\boldsymbol{V}_0$ matriz simétrica positiva definida e $a_0, b_0 \in \mathbb{R}_+$.

$~$

Então:
$$(\boldsymbol{\beta},\sigma^2)|\boldsymbol{Z} \sim \textit{NIG}(\boldsymbol{\beta}_1, \boldsymbol{V}_1, a_1, b_1)$$

com
$\boldsymbol{\beta}_1 = \boldsymbol{V}_1\left(\boldsymbol{V}_0^{-1}\boldsymbol{\beta}_0 + \boldsymbol{X}^T\boldsymbol{X} \hat{\boldsymbol{\beta}}\right) ~~$; $~~ \boldsymbol{V}_1 = \left(\boldsymbol{V}_0^{-1} + \boldsymbol{X}^T\boldsymbol{X}\right)^{-1}~~$;
$a_1 = a_0+\frac{n}{2} ~~$; $~~ b_1 = b_0 + \frac{\boldsymbol{\beta}_0^T\boldsymbol{V}_0^{-1}\boldsymbol{\beta}_0 + \boldsymbol{Y}^T\boldsymbol{Y} - \boldsymbol{\beta}_1^T\boldsymbol{V}_1^{-1}\boldsymbol{\beta}_1}{2}$.

$~$

**Observação.** Uma das maneiras de representar falta de informação nesse contexto é utilizar a priori de Jeffreys, $f(\boldsymbol \theta) = \Big|~\mathcal{I}(\boldsymbol \theta)~\Big|^{1/2} \propto 1/\sigma^2$. Nesse caso, distribuição a posteriori é $$(\boldsymbol{\beta},\sigma^2)\Big|\boldsymbol{Z} \sim \textit{NIG}\left(\hat{\boldsymbol{\beta}}, \left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}, \dfrac{n-p}{2}, \dfrac{(n-p)s^2}{2}\right)~.$$

$~$


$~$


**Exemplo.** Considere que, a priori, $(\boldsymbol{\beta},\sigma^2) \sim \textit{NIG}\left(\boldsymbol{\beta}_0, \boldsymbol{V}_0, a_0, b_0\right)$, com $~\boldsymbol{\beta}_0 = \left[\begin{array}{c} 0\\0\end{array}\right]~$;
$~\boldsymbol{V}_0 = \left[\begin{array}{cc} 100 & 0\\ 0 & 100\end{array}\right] ~$; $~a_0 = 3~$; $~b_0 = 100~$.

A seguir são apresentadas as distribuições marginais dos parâmetros, a distribuição marginal e as regiões HPD bivariadas do parâmetro $\boldsymbol{\beta}$.

```{r reglin_margpost}
x = cars$dist   # variável resposta
y = cars$speed  # variável explicativa
n = length(x)   # n=50
X = cbind(1,x)  # Matrix de planejamento
p = ncol(X)     # p=2
g = n-p         # gl=48
beta_est = solve(t(X)%*%X)%*%(t(X)%*%y) # (-17.6, 3.9)
sigma_est =
  as.double(t(y-X%*%beta_est)%*%(y-X%*%beta_est)/(n-p)) #236.5
beta0 = c(0,0)                     # média priori betas
V0 = matrix(c(100,0,0,100),ncol=2) # matriz de escala beta
a0 = 3                             # priori sigma
b0 = 100                           # priori sigma
# parâmetros da posteriori
V1 = solve(solve(V0) + t(X)%*%X)
beta1 = V1%*%(solve(V0)%*%beta0 + t(X)%*%X%*%beta_est)
a1 = a0 + n/2
b1 = as.double(b0 + (t(beta0)%*%solve(V0)%*%beta0 + t(y)%*%y - t(beta1)%*%solve(V1)%*%beta1)/2)
V = b1*V1/a1 # Matrix de escala da posteriori marginal de beta

beta1lim=c(beta1[1]-qt(0.9999,2*a1)*sqrt(V[1,1]),beta1[1]+qt(0.9999,2*a1)*sqrt(V[1,1]))
beta2lim=c(beta1[2]-qt(0.9999,2*a1)*sqrt(V[2,2]),beta1[2]+qt(0.9999,2*a1)*sqrt(V[2,2]))
sigma2lim=c(extraDistr::qinvgamma(0.0001,a1,b1),extraDistr::qinvgamma(0.9999,a1,b1))

b1plot <- ggplot(data.frame(x=beta1lim), aes(x=x), colour = "0.Posterior") +
  stat_function(fun = mnormt::dmt,args = list(mean = beta1[1], S = V[1,1], df=2*a1)) +
  theme_bw() + xlab(expression(beta[1])) + ylab("Posterior")
b2plot <- ggplot(data.frame(x=beta2lim), aes(x=x), colour = "0.Posterior") +
  stat_function(fun = mnormt::dmt, args = list(mean = beta1[2], S = V[2,2], df=2*a1)) +
  theme_bw() + xlab(expression(beta[2])) + ylab("Posterior")
s2plot <- ggplot(data.frame(x=sigma2lim),aes(x=x), colour = "0.Posterior")+
  stat_function(fun = extraDistr::dinvgamma, args = list(alpha = a1, beta = b1)) +
  theme_bw() + xlab(expression(sigma^2)) + ylab("Posterior")
ggpubr::ggarrange(b1plot,b2plot,s2plot,ncol=1)
```


```{r reglin_postbi}
# posteriori marginal bivariada dos betas
posterior <- function(theta0,theta1) { apply(cbind(theta0,theta1),1,function(w){ mnormt::dmt(w, mean=c(beta1), S=V, df=2*a1)  }) }
# Gráfico da posteriori marginal bivariada dos betas
grx <- seq(beta1lim[1], beta1lim[2],length.out=200)
gry <- seq(beta2lim[1], beta2lim[2],length.out=200)
z1 <- outer(grx,gry,posterior)
#persp(grx,gry,z1)
plotly::plot_ly(alpha=0.1) %>%
  plotly::add_surface(x=grx, y=gry, z=t(z1), colorscale = list(c(0,'#BA52ED'), c(1,'#FCB040')), showscale = FALSE)
```



```{r reglin_HPDbi}
# Curvas de Probabilidade
l = c(0.1,0.3,0.5,0.8,0.9,0.95,0.99)
z1v = sort(as.vector(z1),decreasing = TRUE)
v1 <- z1v/sum(z1v)
a=0; j=1; l1=NULL
for(i in 1:length(v1)) {
  a <- a+v1[i]
  if(j<=length(l) & a>l[j]) {
    l1 <- c(l1,z1v[i-1])
    j <- j+1
  }
}
contour(grx,gry,z1,col=colors()[455],main="Regiões HPD para os Betas",xlab=expression(beta[1]),ylab=expression(beta[2]),levels=l1,labels=l)
points(beta1[1],beta1[2],col="darkred",pch=16,cex=0.5)
```

$~$

$~$


## Laplace's Demon

LaplacesDemon é uma biblioteca do R que oferece diversos algoritimos implementados de MCMC, permitindo fazer Inferência Bayesiana aproximada. Os algoritimos de MCMC disponíveis são

1.  Automated Factor Slice Sampler (AFSS)
1.  Adaptive Directional Metropolis-within-Gibbs (ADMG)
1.  Adaptive Griddy-Gibbs (AGG)
1.  Adaptive Hamiltonian Monte Carlo (AHMC)
1.  Adaptive Metropolis (AM)
1.  Adaptive Metropolis-within-Gibbs (AMWG)
1.  Adaptive-Mixture Metropolis (AMM)
1.  Affine-Invariant Ensemble Sampler (AIES)
1.  Componentwise Hit-And-Run Metropolis (CHARM)
1.  Delayed Rejection Adaptive Metropolis (DRAM)
1.  Delayed Rejection Metropolis (DRM)
1.  Differential Evolution Markov Chain (DEMC)
1.  Elliptical Slice Sampler (ESS)
1.  Gibbs Sampler (Gibbs)
1.  Griddy-Gibbs (GG)
1.  Hamiltonian Monte Carlo (HMC)
1.  Hamiltonian Monte Carlo with Dual-Averaging (HMCDA)
1.  Hit-And-Run Metropolis (HARM)
1.  Independence Metropolis (IM)
1.  Interchain Adaptation (INCA)
1.  Metropolis-Adjusted Langevin Algorithm (MALA)
1.  Metropolis-Coupled Markov Chain Monte Carlo (MCMCMC)
1.  Metropolis-within-Gibbs (MWG)
1.  Multiple-Try Metropolis (MTM)
1.  No-U-Turn Sampler (NUTS)
1.  Oblique Hyperrectangle Slice Sampler (OHSS)
1.  Preconditioned Crank-Nicolson (pCN)
1.  Random Dive Metropolis-Hastings (RDMH)
1.  Random-Walk Metropolis (RWM)
1.  Reflective Slice Sampler (RSS)
1.  Refractive Sampler (Refractive)
1.  Reversible-Jump (RJ)
1.  Robust Adaptive Metropolis (RAM)
1.  Sequential Adaptive Metropolis-within-Gibbs (SAMWG)
1.  Sequential Metropolis-within-Gibbs (SMWG)
1.  Slice Sampler (Slice)
1.  Stochastic Gradient Langevin Dynamics (SGLD)
1.  Tempered Hamiltonian Monte Carlo (THMC)
1.  t-walk (twalk)
1.  Univariate Eigenvector Slice Sampler (UESS)
1.  Updating Sequential Adaptive Metropolis-within-Gibbs (USAMWG)
1.  Updating Sequential Metropolis-within-Gibbs (USMWG)

$~$

Manuais do [LaplacesDemon](https://cran.r-project.org/web/packages/LaplacesDemon/LaplacesDemon.pdf)

* [Tutorial](https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/LaplacesDemonTutorial.pdf)

* [Inferência Bayesiana](https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/BayesianInference.pdf)

* [Exemplos](https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/Examples.pdf)

$~$

Voltando ao exemplo de regressão linear:

+ Especificação do modelo

```{r LaplacesDemon}
require(LaplacesDemon)

parm.names=as.parm.names(list(beta=rep(0,p), sigma2=0))
pos.beta=grep("beta", parm.names)
pos.sigma=grep("sigma2", parm.names)

MyData <- list(J=p, X=X, y=y, mon.names="LP",
  parm.names=parm.names,pos.beta=pos.beta,pos.sigma=pos.sigma)

Model <- function(parm, Data)
{
  ### Parameters
  beta <- parm[Data$pos.beta]
  sigma2 <- interval(parm[Data$pos.sigma], 1e-100, Inf)
  parm[Data$pos.sigma] <- sigma2
  ### Log-Prior
  sigma.prior <- dinvgamma(sigma2, a0, b0, log=TRUE)
  beta.prior <- dmvn(beta, beta0, sigma2*V0, log=TRUE)
  ### Log-Likelihood
  mu <- tcrossprod(Data$X, t(beta))
  LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE))
  ### Log-Posterior
  LP <- LL + beta.prior + sigma.prior
  Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP,
                   yhat=rnorm(length(mu), mu, sigma2), parm=parm)
  return(Modelout)
}

Initial.Values <- c(beta_est,sigma_est)

burnin <- 2000
thin <- 3
N=(2000+burnin)*thin
```

---

+ **Exemplo 1:** Metropolis-within-Gibbs (MWG)

```{r LaplacesDemon_MWG}
set.seed(666)
Fit1 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
  Covar=NULL, Iterations=N, Status=N/5, Thinning=thin,
  Algorithm="MWG", Specs=NULL)
#names(Fit1)
print(Fit1)
Post1 <- data.frame(Fit1$Posterior1,Algorithm="1.MWG")
colnames(Post1) <- c("beta1","beta2","sigma2","Algorithm")
#head(Post1)
plot(Fit1, BurnIn=0, MyData, PDF=FALSE, Parms=NULL)
#plot(Fit1, BurnIn=burnin, MyData, PDF=FALSE, Parms=NULL)
```

---

+ **Exemplo 2:** Adaptative Metropolis-within-Gibbs (AMWG)

```{r LaplacesDemon_AMWG}
set.seed(666)
Fit2 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
  Covar=NULL, Iterations=N, Status=N/5, Thinning=thin,
  Algorithm="AMWG", Specs=NULL)
#names(Fit2)
#print(Fit2)

Post2 <- data.frame(Fit2$Posterior1,Algorithm="2.AMWG")
colnames(Post2) <- c("beta1","beta2","sigma2","Algorithm")
#head(Post2)

#plot(Fit2, BurnIn=burnin, MyData, PDF=FALSE, Parms=NULL)
plot(Fit2, BurnIn=0, MyData, PDF=FALSE, Parms=NULL)
```

```{r LaplacesDemon_Compar1}
Post <- rbind(Post1,Post2)
b1plot <- ggplot(Post) +
  stat_function(aes(colour="0.Posterior"), fun = mnormt::dmt,args = list(mean = beta1[1], S = V[1,1], df=2*a1)) +
  geom_density(aes(beta1, colour = Algorithm)) + theme_bw() +
  xlab(expression(beta[1])) + ylab("Posterior") + labs(colour = "Method")
b2plot <- ggplot(Post) +
  stat_function(aes(colour="0.Posterior"), fun = mnormt::dmt, args = list(mean = beta1[2], S = V[2,2], df=2*a1)) +
  geom_density(aes(beta2, colour = Algorithm)) + theme_bw() +
  xlab(expression(beta[2])) + ylab("Posterior") + labs(colour = "Method")
s2plot <- ggplot(Post)+
  stat_function(aes(colour="0.Posterior"), fun = extraDistr::dinvgamma, args = list(alpha = a1, beta = b1)) +
  geom_density(aes(sigma2, colour = Algorithm)) + theme_bw() +
  xlab(expression(sigma^2)) + ylab("Posterior") + labs(colour = "Method")
ggpubr::ggarrange(b1plot,b2plot,s2plot,
                  ncol=1,common.legend=TRUE,legend="bottom")
```

---

+ **Exemplo 3:** Consort e Automated Factor Slice Sampler (AFSS)

```{r LaplacesDemon_AFSS}
Consort(Fit2)
Initial.Values <- as.initial.values(Fit2)

set.seed(666)
Fit3 <- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=N, Status=N/5, Thinning=thin,
     Algorithm="AFSS", Specs=list(A=Inf, B=NULL, m=100,n=0, w=1))
Post3 <- data.frame(Fit3$Posterior1,Algorithm="3.AFSS")
colnames(Post3) <- c("beta1","beta2","sigma2","Algorithm")
plot(Fit3, BurnIn=0, MyData, PDF=FALSE, Parms=NULL)
```

```{r LaplacesDemon_Compar2}
Post <- rbind(Post1,Post2,Post3)
b1plot <- ggplot(Post) +
  stat_function(aes(colour="0.Posterior"), fun = mnormt::dmt,args = list(mean = beta1[1], S = V[1,1], df=2*a1)) +
  geom_density(aes(beta1, colour = Algorithm)) + theme_bw() +
  xlab(expression(beta[1])) + ylab("Posterior") + labs(colour = "Method")
b2plot <- ggplot(Post) +
  stat_function(aes(colour="0.Posterior"), fun = mnormt::dmt, args = list(mean = beta1[2], S = V[2,2], df=2*a1)) +
  geom_density(aes(beta2, colour = Algorithm)) + theme_bw() +
  xlab(expression(beta[2])) + ylab("Posterior") + labs(colour = "Method")
s2plot <- ggplot(Post)+
  stat_function(aes(colour="0.Posterior"), fun = extraDistr::dinvgamma, args = list(alpha = a1, beta = b1)) +
  geom_density(aes(sigma2, colour = Algorithm)) + theme_bw() +
  xlab(expression(sigma^2)) + ylab("Posterior") + labs(colour = "Method")
ggpubr::ggarrange(b1plot,b2plot,s2plot,
                  ncol=1,common.legend=TRUE,legend="bottom")
```


```{r LaplacesDemon_HPDbeta2}
p.interval(Post3$beta2, HPD=TRUE, MM=FALSE, plot=TRUE)
```


```{r LaplacesDemon_ComparBi}
set.seed(666)
S0 <- as.matrix(extraDistr::rinvgamma(N/thin-burnin,a1,b1))
M0 <- apply(S0,1,function(s){mnormt::rmnorm(1,mean=beta1,varcov=s*V1)})
Post0 <- data.frame(t(M0),S0,"0.Posterior")
colnames(Post0) <- c("beta1","beta2","sigma2","Algorithm")
Post = rbind(Post0,Post1,Post2,Post3)
ggplot(Post) + theme_bw() +
  geom_point(aes(beta1,beta2,colour=Algorithm), shape=1) +
  facet_wrap(Algorithm ~ .) + guides(colour = "none")
```

---

## Stan

O Stan é uma plataforma de modelagem estatística de alto desempenho. Em particular, permite fazer inferência bayesiana usando o método de Monte Carlo Hamiltoniano^[Para uma interessante ilustração do método, visite esse [link](https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html).] (HMC) e a variação No-U-Turn Sampler (NUTS). Esses recursos convergem para distribuições alvo de altas dimensões muito mais rapidamente que métodos mais simples, como o amostrador de Gibbs ou outras variações do método de Metropolis-Hastings. A linguagem utilizada é independente da plataforma e existem bibliotecas para R (`rstan`) e Python.

Principais manuais do [Stan](https://mc-stan.org/):

+ [Manual de Referência](https://mc-stan.org/docs/2_27/reference-manual/index.html)

+ [Guia do Usuário (com vários Exemplos)](https://mc-stan.org/docs/2_27/stan-users-guide/index.html)

+ [Funções e distribuições](https://mc-stan.org/docs/2_27/functions-reference/index.html)


$~$

**Voltando ao Exemplo**

```{r stan_LM}
library(rstan)
# Parametros do método
Initial.Values <- c(beta_est,sigma_est)
burnin <- 2000
thin <- 3
N=(2000+burnin)*thin
# Conjunto de dados
stan_data <- list(N = n, J = p, y = y, x = X)

# Especificação do modelo
rs_code <- '
  data {
    int<lower=1> N;
    int<lower=1> J;
    matrix[N,J] x;
    vector[N] y;
  }
  parameters {
    vector[J] beta;
    real<lower=0> sigma2;
  }
  model {
    sigma2 ~ inv_gamma(3, 100);
    beta ~ normal(0, sqrt(sigma2*100));
    y ~ normal(x * beta, sqrt(sigma2));
}'

stan_mod <- stan(model_code = rs_code, data = stan_data, init=Initial.Values,
            chains = 1, iter = N, warmup = burnin, thin = thin)
```

```{r rstan_LM_Compar1}
posterior <- rstan::extract(stan_mod)
Post4 <- data.frame(posterior$beta[,1],posterior$beta[,2],posterior$sigma2,Algorithm="4.RStan")
colnames(Post4) <- c("beta1","beta2","sigma2","Algorithm")

# gráficos posterioris marginais
Post <- rbind(Post3,Post4) %>%
  mutate(Algorithm=ifelse(Algorithm=="3.AFSS","1.LaplacesDemons","2.RStan"))
b1plot <- ggplot(Post) +
  stat_function(aes(colour="0.Posterior"), fun = mnormt::dmt,args = list(mean = beta1[1], S = V[1,1], df=2*a1)) +
  geom_density(aes(beta1, colour = Algorithm)) + theme_bw() +
  xlab(expression(beta[1])) + ylab("Posterior") + labs(colour = "Method")
b2plot <- ggplot(Post) +
  stat_function(aes(colour="0.Posterior"), fun = mnormt::dmt, args = list(mean = beta1[2], S = V[2,2], df=2*a1)) +
  geom_density(aes(beta2, colour = Algorithm)) + theme_bw() +
  xlab(expression(beta[2])) + ylab("Posterior") + labs(colour = "Method")
s2plot <- ggplot(Post)+
  stat_function(aes(colour="0.Posterior"), fun = extraDistr::dinvgamma, args = list(alpha = a1, beta = b1)) +
  geom_density(aes(sigma2, colour = Algorithm)) + theme_bw() +
  xlab(expression(sigma^2)) + ylab("Posterior") + labs(colour = "Method")
ggpubr::ggarrange(b1plot,b2plot,s2plot,
                  ncol=1,common.legend=TRUE,legend="bottom")
```

```{r rstan_LM_Compar2}
Post = rbind(Post0,Post4) %>%
  mutate(Algorithm=ifelse(Algorithm=="4.RStan","1.RStan","0.Posterior"))
ggplot(Post) + theme_bw() +
  geom_point(aes(beta1,beta2,colour=Algorithm), shape=1) +
  facet_wrap(Algorithm ~ .) + guides(colour = "none")
```


$~$

```{r rstan_LM_graf}
library(ggmcmc)
stan_mod2 <- stan(model_code = rs_code, data = stan_data, init=Initial.Values,
            chains = 2, iter = N, warmup = burnin, thin = thin)
```

$~$

*1. Histograma* com as cadeias geradas combinadas.

```{r ggmcmc_hist}
ggs(stan_mod2) %>% ggs_histogram(.)
```
*2. Gráficos das Densidades* sobrepostos com cores diferentes por cadeia, permite comparar se as cadeias convergiram para distribuições semelhantes.

```{r ggmcmc_dens}
ggs(stan_mod2) %>% ggs_density(.)
```
*3. Séries Temporais* das cadeias geradas. É esperado que as cadeias geradas apresentem distribuições semelhantes em torno de uma mesma média, indicando assim que atingiu-se a "estacionariedade".

```{r ggmcmc_trace}
ggs(stan_mod2) %>% ggs_traceplot(.)
```
*4. Gráfico de Médias Móveis*. É esperado que as curvas das médias das cadeias geradas se aproximem rapidamente de um mesmo valor.

```{r ggmcmc_run}
ggs(stan_mod2) %>% ggs_running(.)
```

*5. Densidades parcial e completa sobrepostas*. Compara a última parte da cadeia (por padrão, os últimos 10% dos valores, em verde) com a cadeia inteira (em preto). Idealmente, as partes inicial e final da cadeia devem ser amostradas na mesma distribuição alvo, de modo que as densidades sobrepostas devem ser semelhantes.

```{r ggmcmc_part}
ggs(stan_mod2) %>% ggs_compare_partial(.)
```

*6. Gráfico de Autocorrelação*. Espera-se alta correlação apenas no primeiro lag. Quando há um comportamento diferente do esperado, deve-se aumentar o tamanho dos saltos (*thin*) entre as observações da cadeia gerada que serão consideradas na amostra final.

```{r ggmcmc_autocor}
ggs(stan_mod2) %>% ggs_autocorrelation(.)
```

*6. Correlação Cruzada*. Quando há alta correlação entre os parâmetros é possível que a convergência da cadeia seja mais lenta.

```{r ggmcmc_crosscor}
ggs(stan_mod2) %>% ggs_crosscorrelation(.)
```

```{r ggmcmc_pairs}
ggs(stan_mod2) %>% ggs_pairs(., lower = list(continuous = "density"))
```


$~$

---

$~$

<!-- <!-- https://datascienceplus.com/bayesian-regression-with-stan-part-1-normal-regression/ -->


## MLG


O modelos lineares generalizados (MLG) são uma extensão natural dos modelos lineares para casos em que a distribuição da variável resposta não é normal. Como exemplo, vamos considerar o particular caso onde a resposta é binária, conhecido como *regressão logística*.

Considere $Y_1,\ldots,Y_n$ condicionalmente independentes tais que $Y_i|\theta_i \sim \textit{Ber}(\theta_i)$, em que $\theta_i$ é tal que $\log\left(\dfrac{\theta_i}{1-\theta_i}\right) = \boldsymbol x_i' \boldsymbol\beta$ ou, em outras palavras, $\theta_i = \dfrac{1}{1+e^{\boldsymbol x_i' \boldsymbol\beta}} = \dfrac{e^{-\boldsymbol x_i' \boldsymbol\beta}}{1+e^{-\boldsymbol x_i' \boldsymbol\beta}}$ com $\boldsymbol x_i$ as covariáveis da $i$-ésima observação e o vetor de parâmetros $\boldsymbol\beta=(\beta_1,\ldots,\beta_p)$.

$~$

**Exemplo.** Considere as variáveis *vs* (0 = motor em forma de V, 1 = motor reto) e *mpg* (milhas/galão(EUA)) do conjunto de dados `mtcars` do R. Suponha um modelo de regressão logística para a variável resposta *vs* com a covariável *mpg* em que, a priori, $\beta_i \sim \textit{Laplace}(0,b_i)$, $i=1,2$, independentes. Deste modo, a posteriori é dada por

$f(\boldsymbol\beta | \boldsymbol{y},\boldsymbol{x})$
$\propto f(\boldsymbol{y}|\boldsymbol\beta,\boldsymbol{x})f(\boldsymbol\beta)$
$\propto \displaystyle\prod_{i=1}^{n} \left(\dfrac{1}{1+e^{\boldsymbol x_i' \boldsymbol\beta}}\right)^{y_i}\left(\dfrac{e^{\boldsymbol x_i' \boldsymbol\beta}}{1+e^{\boldsymbol x_i' \boldsymbol\beta}}\right)^{1-y_i} \prod_{j=1}^{p} \dfrac{1}{2b_i} e^{-\frac{|\beta_i|}{b_i}}$.


```{r stan_logreg}
require(tidyverse)
require(rstan)

dados <- tibble(mtcars)
# mpg:	Miles/(US)gallon ;  vs: Engine(0=V-shaped,1=straight)
dados %>% ggplot(aes(group=as.factor(vs),y=mpg,fill=as.factor(vs))) +
        geom_boxplot() + scale_fill_discrete(name="vs") + theme_bw()

y <- dados %>% select(vs) %>% pull()
x <- dados %>% select(mpg) %>% pull()
n <- length(y)
X <- as.matrix(cbind(1,x))
p <- ncol(X)

stan_data <- list(N = n, J = p, y = y, x = X)

rs_code <- '
  data {
    int<lower=1> N;
    int<lower=1> J;
    int<lower=0,upper=1> y[N];
    matrix[N,J] x;
  }
  parameters {
    vector[J] beta;
  }
  model {
    beta ~ double_exponential(0, 100);
    y ~ bernoulli_logit(x * beta);
  }'

N=2000
thin=10
burnin=1000

stan_log <- stan(model_code = rs_code, data = stan_data, init = c(0,0),
  chains = 2, iter = N*thin, warmup = burnin, thin = thin)

print(stan_log)

require(bayesplot)

post_log <- rstan::extract(stan_log, inc_warmup = TRUE, permuted = FALSE)


color_scheme_set("mix-brightblue-gray")
mcmc_trace(post_log,  pars = c("beta[1]", "beta[2]"), n_warmup = 0,
                facet_args = list(nrow = 2, labeller = label_parsed))

mcmc_acf(post_log, pars = c("beta[1]", "beta[2]"))

mcmc_areas(post_log,pars = c("beta[1]", "beta[2]"),prob=0.9)

ggpubr::ggarrange(
  mcmc_areas(post_log,pars = c("beta[1]"),prob=0.9),
  mcmc_areas(post_log,pars = c("beta[2]"),prob=0.9),ncol=1)

require(ggmcmc)
#ggs(stan_log) %>% ggmcmc(., file = "ggmcmc_log.html")
ggs(stan_log) %>% ggs_pairs(., lower = list(continuous = "density"))
```
<!-- [**Gráficos ggmcmc**](ggmcmc_log.html) -->


$~$

* Existe uma biblioteca para modelos lineares bayesianos usando o Stan chamada `rstanarm`. Nesta biblioteca, a função `stan_glm` pode ser utilizada para o ajuste de MLGs sob o ponto de vista bayesiano.

  - https://cran.r-project.org/web/packages/rstanarm/


$~$

---

$~$


## Modelos Dinâmicos

* A Biblioteca **walker** para do R que usa o RStan para fazer inferência bayesiana em modelos lineares com coeficientes variando no tempo (modelos dinâmicos).

* Modelo de Regressão Dinâmico Bayesiano

$$y_t = \boldsymbol x_t~\boldsymbol\beta_t + \epsilon_t ~,~~ \epsilon_t \sim \textit{Normal}(0,\sigma_y^2)$$
$$\boldsymbol\beta_{t+1} = \boldsymbol\beta_t + \boldsymbol\eta_t ~,~~ \boldsymbol\eta_t \sim \textit{Normal}_k(0,D)$$

onde

  - $y_t$: variável resposta no instante $t$;

  - $\boldsymbol x_t$: vetor com $k$ variáveis preditoras no instante $t$;

  - $\epsilon_t$ e $\boldsymbol\eta_t$: ruídos brancos;

  - $\boldsymbol\beta_t$: vetor dos $k$ coeficientes de regressão no instante $t$;

  - $D=\textit{diag}({\sigma}_{\eta_i})$;

  - $\boldsymbol\sigma=\left(\sigma_y,{\sigma}_{\eta_1},\ldots,{\sigma}_{\eta_k}\right)$: vetor de parâmetros de variância.

As distribuição a piori são dadas por
$$\beta_1 \sim \textit{Normal}(m_\beta,{s}_\beta^2)$$
$$\sigma_i^2 \sim \textit{Gama}({a}_{\sigma_i},{b}_{\sigma_i})$$

* Sobre a biblioteca `walker`:
  - https://cran.r-project.org/web/packages/walker/vignettes/walker.html
  - https://rdrr.io/cran/walker/man/walker.html

$~$

### Dados de Amoxicilina (fonte: CEA)

* Dados mensais de venda de Amoxicilina e resistência da bacteria *E. coli* no período de 01/2008 à 12/2016. 

* A venda de Amoxicilina foi avaliada pela média mensal das doses diárias definidas por 1000 habitantes-dia (DDD/1000hab/dia), que indica a quantidade média de determinado antibiótico que uma dada população consome diariamente (*IMS Health Brazil/Pfizer*).

* Para avaliar a resistência da bactéria *E. coli*  à Amoxicilina foram utilizados dados obtidos a partir de amostras da rede de apoio do laboratório DASA, que atende principalmente a rede privada de assistência à saúde mas também inclui hospitais que atendem pacientes pelo Sistema Único de Saúde (SUS). Foram
incluídas amostras resultantes de exames de sangue e urina positivas, avaliando a proporção de cepas isoladas resistentes à amoxicilina.

* **Objetivo.** Avaliar o impacto da regulamentação RDC 44 da ANVISA que
obriga a retenção da prescrição médica para a venda de antimicrobianos, implementada em 26 de Outubro de 2010.

```{r walker_amox}
library(lubridate)
dados = read_csv("./data/Amoxicillin.csv") %>% 
  select(Período=Periodo,Vendas=DDD1000hab_dia,Resistência=Resistencia_Amoxicilina.Clavulanato.k)
dados$Vendas[61:62]=mean(c(2.073005,2.173923))
mV=mean(dados$Vendas); sdV=sd(dados$Vendas)
mR=mean(dados$Resistência); sdR=sd(dados$Resistência)
Tr <- sdV/sdR # var. aux. para transformação dos eixos
dados %>% ggplot(aes(x=Período)) + theme_bw() + 
  geom_line(aes(y=Vendas,colour="Vendas"),lwd=1) + theme(legend.position="bottom") +
  geom_line(aes(x=Período,y=((Resistência-mR)*Tr+mV),colour="Resistência"),lwd=1)+
  labs(y="Vendas de Amoxicilina (DDD/1000 hab./dia)",colour="") + 
  scale_y_continuous(sec.axis = sec_axis(~./Tr+(mR-mV/Tr), name = "Prop. de Amostras de E.Colli Resistentes"))
```

Para verificar se o efeito da venda de antimicrobianos influencia na resistência
bacteriana e identificar possíveis mudanças após a implementação da lei, foi considerado um modelo de regressão dinâmico bayesiano, descrito a seguir.

$$(Y_t-\bar{Y_0}) = \beta_t (X_t-\bar{X_0}) + \epsilon_t ~,~~ \epsilon_t \sim Normal(0,\sigma_y^2)$$  

$$\beta_{t+1} = \beta_t + W_t ~,~~ W_t \sim Normal(0,\sigma_r^2)$$
$$\sigma_y^2 \sim Gama(2,0.0001)$$  
$$\beta_t \sim Normal(0,100)$$
$$\sigma_r^2 \sim Gama(2,0.0001)$$   
* $Y_t$: proporção de testes de resistência positiva do microbiano *E. coli* no instante $t$;  
* $X_t$: quantidade de doses consumidas de Amoxicilina (DDD/1000hab/dia) no instante $t$;  
* $\beta_t$: parâmetro que representa o efeito da venda do antimicrobiano na resistência bacteriana no instante $t$;  
* $\epsilon_t$ e $W_t$: ruídos brancos.  


```{r walker_amox_fit}
require(walker)
set.seed(666)
# resistência média e venda média do período anterior a RDC 44
mAntes = dados %>% filter(year(Período)<2011) %>% 
        summarise(mean(Vendas),mean(Resistência)) %>% t()
fit1 <- dados %>% 
  mutate(Vendas=Vendas-mAntes[1],Resistência=Resistência-mAntes[2]) %>%
  walker(data=., formula = Resistência ~ -1+rw1(~ -1+Vendas,beta=c(0,100),
    sigma=c(2,0.0001)),sigma_y_prior = c(2,0.0001), chain=2)
        #Default: chain=4, iter=2000, warmup=1000, thin=1

ggpubr::ggarrange(
  mcmc_areas(as.matrix(fit1$stanfit),regex_pars = c("sigma_rw1"),prob=0.9),
  mcmc_areas(as.matrix(fit1$stanfit),regex_pars = c("sigma_y"),prob=0.9),
  ncol=1)
```


```{r walker_amox_coefs}
plot_coefs(fit1, scales = "free", alpha=0.8) + theme_bw() +
  scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017))
```


```{r walker_amox_check}
pp_check(fit1, alpha=0.8) + theme_bw() +
  scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017))
```


```{r walker_amox_pred1}
set.seed(666)
new_data <- data.frame(Vendas=seq(dados$Vendas[108]-mAntes[1],-0.2,length.out=12)+c(0,rnorm(11,0,0.08)))
pred1 <- predict(fit1, new_data)
plot_predict(pred1, alpha=0.8) +
  scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017))
```


```{r walker_amox_pred2}
set.seed(666)
new_data <- data.frame(Vendas=seq(dados$Vendas[108]-mAntes[1],-1,length.out=12)+c(0,rnorm(11,0,0.08)))
pred1 <- predict(fit1, new_data)
plot_predict(pred1, alpha=0.8) +
  scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017))
```


#### Extensões: Efeitos mais suaves e modelos não gaussianos

Ao modelar os coeficientes de regressão como uma passeio aleatório simples, as estimativas posteriores desses coeficientes podem ter grandes variações de curto prazo que podem não ser realistas na prática. Uma maneira de impor mais suavidade às estimativas é alternar dos coeficientes do passeio aleatório para coeficientes de passeio aleatório de segunda ordem integrados:
$$\boldsymbol\beta_{t+1} = \boldsymbol\beta_t + \boldsymbol\nu_t$$
$$\boldsymbol\nu_{t+1} = \boldsymbol\nu_t + \boldsymbol\eta_t ~,~~ \boldsymbol\eta_t \sim \textit{Normal}_k(0,D)$$


```{r walker_amox_rw2}
require(walker)
set.seed(666)
fit2 <- dados %>% 
  mutate(Vendas=Vendas-mAntes[1],Resistência=Resistência-mAntes[2]) %>%
  walker(data=., formula = Resistência ~ -1+rw2(~ -1+Vendas, beta=c(0,100),
        sigma=c(2,0.0001),nu=c(0,100)),sigma_y_prior=c(2,0.0001), chain=2)
        #Default: chain=4, iter=2000, warmup=1000, thin=1

ggpubr::ggarrange(
  mcmc_areas(as.matrix(fit2$stanfit), regex_pars = c("sigma_rw2"), prob=0.9),
  mcmc_areas(as.matrix(fit2$stanfit), regex_pars = c("sigma_y"), prob=0.9),ncol=1)
```


```{r walker_amox_coefs2}
plot_coefs(fit2, scales = "free", alpha=0.8) + theme_bw() +
  scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017))
```

```{r walker_amox_check2}
pp_check(fit2, alpha=0.8) + theme_bw() +
  scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017))
```

```{r walker_amox_pred21}
set.seed(666)
new_data <- data.frame(Vendas=seq(dados$Vendas[108]-mAntes[1],-0.2,length.out=12)+c(0,rnorm(11,0,0.08)))
pred2 <- predict(fit2, new_data)
plot_predict(pred2, alpha=0.8) +
  scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017))
```


```{r walker_amox_pred22}
set.seed(666)
new_data <- data.frame(Vendas=seq(dados$Vendas[108]-mAntes[1],-1,length.out=12)+c(0,rnorm(11,0,0.08)))
pred2 <- predict(fit2, new_data)
plot_predict(pred2, alpha=0.8) +
  scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017))
```
$~$

* A função `walker_glm` estende o pacote para lidar com observações de com distribuição de *Poisson* e *Binomial*, usando a metodologia similar à mencionada acima.




$~$

### Dados de Covid-19

* **Dados de Covid-19**: [Brasil.io](https://brasil.io/)  
    + *Data*: dia/mes/ano.  
    + *Casos Novos*: novos casos registrados no dia.  
    + *Mortes*: mortes registradas no dia.  

* **Dados de Mobilidade**: [Google](https://www.google.com/covid19/mobility/)  
    + *Data*: dia/mes/ano.  
    + *Varejo e Lazer*: Tendências de mobilidade de lugares como restaurantes, cafés, shopping centers, parques temáticos, museus, bibliotecas e cinemas.  
    + *Mercados e Farmácias*: Tendências de mobilidade de lugares como mercados, armazéns de alimentos, feiras, lojas de alimentos gourmet, drogarias e farmácias.  
    + *Parques*: Tendências de mobilidade de lugares como parques nacionais, praias públicas, marinas, parques para cães, praças e jardins públicos.  
    + *Transporte*: Tendências de mobilidade de lugares como terminais de transporte público, por exemplo, estações de metrô, ônibus e trem.  
    + *Trabalho*: Tendências de mobilidade de locais de trabalho.  
    + *Residência*: Tendências de mobilidade de áreas residenciais.  

<!-- * Agradeço à *Renata Massami Hirota* pela compilação dos dados! -->

```{r dados_covid}
require(walker)
# dados1 = read_csv("./data/covid19-Brasil.io-SPSP.csv") %>% 
#   select(epidemiological_week,date,order_for_place,
#          last_available_confirmed,new_confirmed,
#          last_available_confirmed_per_100k_inhabitants,
#          last_available_deaths,new_deaths,
#          last_available_death_rate,estimated_population)
# dados2 = rbind(read_csv("./data/2020_BR_Region_Mobility_Report.csv") %>% 
#     filter(sub_region_1=="State of São Paulo",sub_region_2=="São Paulo"),
#    read_csv("./data/2021_BR_Region_Mobility_Report.csv") %>% 
#     filter(sub_region_1=="State of São Paulo",sub_region_2=="São Paulo"))%>%
#   select(date, retail_and_recreation_percent_change_from_baseline,
#                grocery_and_pharmacy_percent_change_from_baseline,
#                parks_percent_change_from_baseline,
#                transit_stations_percent_change_from_baseline,
#                workplaces_percent_change_from_baseline,
#                residential_percent_change_from_baseline)
# dados = inner_join(dados1,dados2,by="date")
# write_csv(dados, file="./data/Covid_SPSP.csv")

dados = read_csv("./data/Covid_SPSP.csv")

# Faz gráfico de duas séries temporais ajustando os eixos
# dados é da forma (tempo,série1,série2)
series = function(dados,name1=names(dados)[2],name2=names(dados)[3]){
  # aux para eixo secundário
  aux=(max(dados[[2]])-min(dados[[2]]))/(max(dados[[3]])-min(dados[[3]])) 
  dados %>% ggplot(aes(x = dados[[1]])) + theme_bw() +
    geom_line(aes(y = dados[[2]], colour = name1)) +
    geom_line(aes(y = (dados[[3]]-min(dados[[3]]))*aux+min(dados[[2]]), colour = name2)) +
    scale_y_continuous(sec.axis=
      sec_axis(~./aux-min(dados[[2]])/aux+min(dados[[3]]),name=name2)) +
    labs(x="",y=name1,colour="") + theme(legend.position = "bottom")
}

g1=dados %>% 
  filter(date>="2020-03-23") %>% # uma semana após início da quarentena
  select(date,new_confirmed,
      retail_and_recreation_percent_change_from_baseline) %>% 
  series(.,"Casos Novos","Varejo e Lazer")

g2=dados %>% 
  filter(date>="2020-03-23") %>% # uma semana após início da quarentena
  select(date,new_confirmed,
      grocery_and_pharmacy_percent_change_from_baseline) %>% 
  series(.,"Casos Novos","Mercados e Farmácias")

g3=dados %>% 
  filter(date>="2020-03-23") %>% # uma semana após início da quarentena
  select(date,new_confirmed,
      parks_percent_change_from_baseline) %>% 
  series(.,"Casos Novos","Parques")

g4=dados %>% 
  filter(date>="2020-03-23") %>% # uma semana após início da quarentena
  select(date,new_confirmed,
      transit_stations_percent_change_from_baseline) %>% 
  series(.,"Casos Novos","Transporte")

g5=dados %>% 
  filter(date>="2020-03-23") %>% # uma semana após início da quarentena
  select(date,new_confirmed,
      workplaces_percent_change_from_baseline) %>% 
  series(.,"Casos Novos","Trabalho")

g6=dados %>% 
  filter(date>="2020-03-23") %>% # uma semana após início da quarentena
  select(date,new_confirmed,
      residential_percent_change_from_baseline) %>% 
  series(.,"Casos Novos","Residencia")
g1;g2;g3;g4;g5;g6
```


```{r plotseriesxy}
ds =  dados %>%
  mutate(y=new_confirmed,x1=retail_and_recreation_percent_change_from_baseline,x2=grocery_and_pharmacy_percent_change_from_baseline,x3=parks_percent_change_from_baseline,x4=dados$transit_stations_percent_change_from_baseline,x5=workplaces_percent_change_from_baseline, x6=residential_percent_change_from_baseline, t=date, y2=new_deaths) %>%
  arrange(date) %>%
  select(t,y,x1,x2,x3,x4,x5,x6,y2) %>%
  filter(t>="2020-03-23") %>% # uma semana após início da quarentena
  mutate(x=x6-x4)             # "Índice de Mobilidade" (inventei!)
  
seriesxy=ds %>% select(t,y,x) %>% 
  series(.,name1="Casos Novos",name2="Índice de Mobilidade")
seriesxy
```

```{r plot_coefs, echo=FALSE}
plot_coefs2= function(object,level=0.05,alpha=0.33,transform=identity,
                     scales="fixed",add_zero=TRUE,col=NA){
    coef_data <- transform(rstan::extract(object$stanfit, pars = "beta_rw", 
        permuted = TRUE)$beta)
    if (object$distribution != "gaussian") {
        coef_data <- coef_data[sample(1:nrow(coef_data), size = nrow(coef_data), 
            replace = TRUE, prob = rstan::extract(object$stanfit, pars = "weights", 
                permuted = TRUE)$weights), , , drop = FALSE]
    }
    dimnames(coef_data) <- list(iter = 1:nrow(coef_data), beta = colnames(object$xreg_rw), 
        time = as.numeric(time(object$y)))
    coef_data <- as.data.frame(as.table(coef_data))
    names(coef_data)[4] <- "value"
    coef_data$time <- as.numeric(levels(coef_data$time))[coef_data$time]
    quantiles <- summarise(group_by(coef_data, time, beta), lwr = quantile(.data$value, 
        prob = level), median = quantile(.data$value, prob = 0.5), 
        upr = quantile(.data$value, prob = 1 - level))
    p <- ggplot(data = quantiles, mapping = aes(x = .data$time, 
        y = .data$median, ymin = .data$lwr, ymax = .data$upr)) + 
        facet_wrap(~beta, scales = scales, ncol=col) + geom_ribbon(aes_(color = "beta", 
        fill = "beta"), alpha = alpha, linetype = 0) + 
        geom_line(aes_(color = "beta")) + labs(y = NULL) + 
        theme(legend.position = "none") + scale_color_manual(name = "", 
        values = c(beta = color_scheme_get()[[2]])) + scale_fill_manual(name = "", 
        values = c(beta = color_scheme_get()[[1]]))
    if (add_zero) 
        p <- p + geom_hline(yintercept = 0, linetype = "dashed")
    p
}
```



```{r covid1}
require(walker)
fit1 <- ds %>% #mutate(x=x-mean(x),y=y-mean(y)) %>%
  walker(data=., formula = y ~ -1+rw1(~ 1+x,beta=c(0,100), 
    sigma=c(2,0.0001)),sigma_y_prior = c(2,0.0001), chain=1)
        #Default: chain=4, iter=2000, warmup=1000, thin=1

labelx = ds %>% select(t) %>% 
  mutate(x=seq(1,nrow(ds)),dia1=lubridate::day(t),
         labx=lubridate::month(t, label = TRUE)) %>% 
  filter(dia1==1) %>% select(x,labx) %>% 
  add_row(x=214,labx="nov",.after=7) %>% 
  add_row(x=267,labx="jan",.after=9) %>% 
  add_row(x=c(385,414),labx=c("mai","jun"))

ggpubr::ggarrange(
  mcmc_areas(fit1$stanfit,pars = c("sigma_rw1[1]"),prob=0.9),
  mcmc_areas(fit1$stanfit,pars = c("sigma_rw1[2]"),prob=0.9),
  mcmc_areas(fit1$stanfit,pars = c("sigma_y"),prob=0.9),
  ncol=1,align="v")

plotcoef=plot_coefs2(fit1,scales="free",alpha=0.8,col=1) + theme_bw() +
  scale_x_continuous(breaks=labelx$x,labels=labelx$labx)
plotcoef

ggpubr::ggarrange(seriesxy,plotcoef,heights=c(1,2),
          ncol = 1, align = "v",common.legend=T,legend="bottom")

pp_check(fit1, alpha=0.8) + theme_bw() +
  scale_x_continuous(breaks=labelx$x,labels=labelx$labx)

# predição com aumento da taxa de isolamento
new_data <- data.frame(x=seq(34,64,length.out=30))
pred1 <- predict(fit1, new_data)
plot_predict(pred1, alpha=0.8) +
  scale_x_continuous(breaks=labelx$x,labels=labelx$labx)

#predição com diminuição da taxa de isolamento
new_data <- data.frame(x=seq(34,4,length.out=30))
pred2 <- predict(fit1, new_data)
plot_predict(pred2, alpha=0.8) +
  scale_x_continuous(breaks=labelx$x,labels=labelx$labx)
```

<!-- ```{r covid_glm} -->
<!-- require(walker) -->
<!-- wglm <- ds %>% #mutate(x=x-mean(x),y=y-mean(y)) %>% -->
<!--   walker_glm(data=., formula = y2 ~ -1+rw1(~ -1+x,beta=c(0,100),  -->
<!--     sigma=c(2,0.0001)),chain=1,distribution="poisson") -->
<!--         #Default: chain=4, iter=2000, warmup=1000, thin=1 -->

<!-- ggpubr::ggarrange( -->
<!--   #mcmc_areas(wglm$stanfit,pars = c("beta_fixed[1]"),prob=0.9), -->
<!--   mcmc_areas(wglm$stanfit,pars = c("sigma_rw1[1]"),prob=0.9), -->
<!--   ncol=1,align="v") -->

<!-- coef_wglm=plot_coefs(wglm,scales="free",alpha=0.8) + theme_bw() + -->
<!--   scale_x_continuous(breaks=labelx$x,labels=labelx$labx) -->
<!-- coef_wglm -->

<!-- ggpubr::ggarrange(seriesxy,coef_wglm,#heights=c(1,2), -->
<!--           ncol = 1, align = "v",common.legend=T,legend="bottom") -->

<!-- pp_check(wglm, alpha=0.8) + theme_bw() + -->
<!--   scale_x_continuous(breaks=labelx$x,labels=labelx$labx) -->

<!-- # predição com aumento da taxa de isolamento -->
<!-- new_data <- data.frame(x=seq(34,64,length.out=30)) -->
<!-- pred1 <- predict(wglm, new_data) -->
<!-- plot_predict(pred1, alpha=0.8) + -->
<!--   scale_x_continuous(breaks=labelx$x,labels=labelx$labx) -->

<!-- #predição com diminuição da taxa de isolamento -->
<!-- new_data <- data.frame(x=seq(34,4,length.out=30)) -->
<!-- pred2 <- predict(wglm, new_data) -->
<!-- plot_predict(pred2, alpha=0.8,level=0.5) + -->
<!--   scale_x_continuous(breaks=labelx$x,labels=labelx$labx) -->
<!-- ``` -->

