[["index.html", "Fundamentos de Inferência Bayesiana 1 Prefácio", " Fundamentos de Inferência Bayesiana Victor Fossaluza e Luís Gustavo Esteves 2021-05-27 1 Prefácio Esse documento foi criado com base nos cursos de Inferência Bayesiana ministrados por nós no Instituto de Matemática e Estatística da Universidade de São Paulo (IME-USP). Essas notas devem ser usadas como um roteiro de estudos e não irão necessariamente apresentar todo o conteúdo dessas disciplinas. Além disso, esta é uma versão preliminar que está bem longe da versão final, de modo que podem haver muitos erros e, assim, correções ou sugestões serão sempre muito bem vindas! "],["ProbSubj.html", "2 Probabilidade Subjetiva 2.1 Definição Axiomática 2.2 Interpretações de Probabilidade 2.3 Relação de Crença \\(\\precsim\\) 2.4 Medida de Probabilidade que representa \\(\\precsim\\) 2.5 Medida de Probabilidade Condicional", " 2 Probabilidade Subjetiva A construção de probabilidade subjetiva apresentada aqui pode ser encontrada no livro Optimal Statistical Decisions (DeGroot 1970). \\(\\Omega\\): espaço amostral, conjunto não vazio. \\(\\mathcal{A}\\): \\(\\sigma\\)-álgebra de subconjuntos de \\(\\Omega\\), isto é, \\(\\Omega \\in \\mathcal{A}\\); \\(A \\in \\mathcal{A} \\Longrightarrow A^{c} \\in \\mathcal{A}\\); \\(\\displaystyle A_1, A_2, \\ldots \\in \\mathcal{A} \\Longrightarrow \\bigcup_{i\\geq1} A_i \\in \\mathcal{A}\\). Os elementos de \\(\\mathcal{A}\\) são chamados de eventos e serão denotados por \\(A, B, C, \\ldots, A_1, A_2, \\ldots\\) 2.1 Definição Axiomática \\(P: \\mathcal{A} \\longrightarrow [0,1]\\) é uma medida de probabilidade se \\(P(\\Omega) = 1\\); \\(\\displaystyle A_1, A_2, \\ldots \\in \\mathcal{A}\\) com \\(A_i \\bigcap A_j = \\varnothing\\) , \\(\\displaystyle P\\left(\\bigcup_{i \\geq 1} A_i\\right) = \\sum_{i \\geq 1} P\\left(A_i\\right)\\). 2.2 Interpretações de Probabilidade Interpretação Clássica (De Moivre, Laplace) baseia-se na equiprobabilidade dos resultados; \\(P(A) = \\frac{|A|}{|\\Omega|}\\). Exemplo: um lançamento de moeda, \\(A\\) = cara, \\(P(A) = \\frac{1}{2}\\). \\(~\\) Interpretação Frequentista (Venn, von Mises, Reichenbach, etc.) quase unânime na primeira metade do século XX e ainda é a mais aceita; baseia-se na regularidade das frequências relativas (lei dos grandes números); \\(P(A) = lim \\frac{A_n}{n}\\), onde \\(A_n\\) é o número de ocorrências de \\(A\\) em \\(n\\) realizações idênticas e independentes do experimento; Supõe que é possível repetir indefinidamente o experimento nas mesmas circustâncias. Exemplo: um lançamento de moeda, \\(A\\) = cara. \\(~\\) Interpretação Lógica (Keynes, Jeffreys, Carnap, etc.) medida de vínculo parcial entre uma evidência e uma hipótese; baseia-se em relações objetivas entre proposições. Exemplo: considere duas proposições: até agora todos os lançamentos resultaram em cara e será realizado um novo lançamento. Pode-se afirmar que provavelmente o resultado do novo lançamento será cara. \\(~\\) Interpretação Subjetivista (Ramsey, de Finetti, Savage, etc) probabilidade como medida subjetiva de crença; baseada na experiência de cada indivíduo, portanto única. Exemplo: suponha que Bruno lançou uma moeda 3 vezes e todos os resultados foram cara. Esse indivíduo, em posse dessa informação, pode acreditar que o resultado cara é mais provável que coroa. Contudo, quando pergunta sobre a probabilidade de cara ao seu colega Olavo, ignorante com relação a moeda, ele responde que é 1/2. \\(~\\) 2.3 Relação de Crença \\(\\precsim\\) \\(\\precsim\\) : relação de crença em \\(\\mathcal{A}\\times\\mathcal{A}\\) \\(A \\prec B\\) : acredito mais em \\(B\\) que em \\(A\\) (\\(B \\succ A\\)) \\(A \\sim B\\) : acredito igualmente em \\(B\\) e \\(A\\) \\(A \\precsim B\\) : acredito em \\(B\\) pelo menos tanto quanto em \\(A\\) Objetivo: sob certas condições em \\(\\precsim\\), obter uma medida de probabilidade \\(P\\) que representa (concorda) com \\(\\precsim\\). \\[A \\precsim B ~ \\Longleftrightarrow ~ P(A) \\leq P(B)\\] \\(~\\) Suposições sobre \\(\\precsim\\) SP1: Para \\(A, B \\in \\mathcal{A}\\), exatamente uma das afirmações a seguir deve valer: \\[A \\prec B ~,~ B \\prec A ~\\textrm{ou}~ A \\sim B.\\] \\(~\\) SP2: \\(A_1, A_2, B_1, B_2 \\in \\mathcal{A}\\) tais que \\(A_1 \\cap A_2 = B_1 \\cap B_2 = \\varnothing\\) e \\(A_i \\precsim B_i\\), \\(i=1,2\\). Então \\[A_1 \\cup A_2 \\precsim B_1 \\cup B_2 .\\] Além disso, se \\(A_i \\prec B_i\\) para algum \\(i\\), então \\(A_1 \\cup A_2 \\prec B_1 \\cup B_2 .\\) \\(~\\) SP3: Se \\(A\\) é um evento, então \\(\\varnothing \\precsim A\\). Além disso, \\(\\varnothing \\prec \\Omega\\). \\(~\\) SP4: Se \\(A_1, A_2, \\ldots\\) uma sequência decrescente de eventos, isto é, \\(A_n \\supseteq A_{n+1}, \\forall n\\), e \\(B\\) tal que \\(B \\precsim A_n, \\forall n\\) então \\[B \\precsim \\bigcap_{n \\geq 1} A_n.\\] \\(~\\) Lema 1: \\(A, B, D \\in \\mathcal{A}\\) tais que \\(A \\cap D = B \\cap D = \\varnothing\\). Então \\[A \\precsim B ~\\Leftrightarrow~ A \\cup D \\precsim B \\cup D\\] Demo: (\\(\\Rightarrow\\)) \\(A \\precsim B \\Rightarrow A \\cup D \\precsim B \\cup D\\) (SP2) (\\(\\Leftarrow\\)) \\(B \\prec A \\Rightarrow B \\cup D \\prec A \\cup D\\) (SP2) \\(~\\) Teorema 1: Se \\(A \\precsim B\\) e \\(B \\precsim D\\) então \\(A \\precsim D\\). Demo: (i) \\((1) \\cup (2) \\cup (4) \\cup (5) \\precsim (1) \\cup (2) \\cup (3) \\cup (6)\\) \\(~\\Rightarrow~ (4) \\cup (5) \\precsim (3) \\cup (6)\\). (ii) Analogamente, \\((2) \\cup (6) \\precsim (4) \\cup (7)\\) De (i) e (ii) e pelo Lema 1, \\((4) \\cup (5) \\cup (2) \\cup (6) \\precsim (3) \\cup (6) \\cup (4) \\cup (7)\\) \\(~\\Rightarrow~ (2) \\cup (5) \\precsim (3) \\cup (7)\\) \\({~\\Rightarrow~ (2) \\cup (5) \\cup (1) \\cup(4) \\precsim (3) \\cup (7) \\cup (1) \\cup(4)}\\). \\(~\\) Teorema 2 (generalização do SP2): Se \\(A_1, \\ldots, A_n\\) são eventos disjuntos e \\(B_1, \\ldots, B_n\\) são também eventos disjuntos tais que \\(A_i \\precsim B_i\\), para \\(i=1,\\ldots,n\\), então \\[\\bigcup_{i=1}^{n} A_i \\precsim \\bigcup_{i=1}^{n} B_i.\\] Se \\(A_i \\prec B_i\\) para algum i, então \\(\\bigcup_{i=1}^{n} A_i \\prec \\bigcup_{i=1}^{n} B_i.\\) Demo: Basta aplicar SP2 \\(n-1\\) vezes. \\(~\\) Teorema 3: Se \\(A \\precsim B\\) então \\(A^c \\succsim B^c\\). Demo: Do Lema 1, \\(A \\cup (A^c \\cap B^c) \\precsim B \\cup (A^c \\cap B^c)\\) \\(\\Rightarrow B^c \\cup (A \\cap B) \\precsim A^c \\cup (A \\cap B)\\) \\(\\Rightarrow B^c \\precsim A^c\\). \\(~\\) Resultado: Para todo evento \\(A\\), \\(A \\precsim \\Omega\\). Demo: Por SP3, \\(\\varnothing \\precsim A^c\\). Tomando \\(D=A\\) no Lema 1, \\(\\varnothing \\cup A \\precsim A^c \\cup A \\Rightarrow A \\precsim \\Omega\\). \\(~\\) Teorema 4: Se \\(A \\subseteq B\\) então \\(A \\precsim B\\). Demo: Suponha, \\(B \\prec A\\). Tomando \\(D=B^c\\) no Lema 1, \\(B \\cup B^c \\prec A \\cup B^c \\Rightarrow \\Omega \\prec A \\cup B^c\\). Absurdo! \\(~\\) Exemplo 1: \\(\\omega_0 \\in \\Omega\\). \\(A \\precsim B \\Leftrightarrow \\{\\omega_0 \\in B\\) ou \\(\\omega_0 \\notin (A \\cup B)\\}\\). Mostre que \\(\\precsim\\) obedece às SP1 a SP4. (SP1) \\(A \\precsim B \\Leftrightarrow \\omega_0 \\in B \\cup (A \\cup B)^c\\) \\(\\Rightarrow B \\prec A \\Leftrightarrow \\omega_0 \\in B^c \\cap (A \\cup B)\\) \\(\\Leftrightarrow \\omega_0 \\in A \\cap B^c.\\) Analogamente, \\(A \\prec B \\Leftrightarrow \\omega_0 \\in B \\cap A^c.\\) \\(A \\sim B \\Leftrightarrow A \\precsim B\\) e \\(B \\precsim A\\) \\(\\Leftrightarrow \\omega_0 \\in [B \\cup (A \\cup B)^c] \\cap [A \\cup (A \\cup B)^c]\\) \\(\\Leftrightarrow \\omega_0 \\in (A \\cap B) \\cup (A \\cup B)^c.\\) \\(~\\) (SP2) \\(A_i \\precsim B_i , i=1,2 \\Leftrightarrow\\) \\(\\omega_0 \\in [B_1 \\cup (A_1 \\cup B_1)^c] \\cap [B_2 \\cup (A_2 \\cup B_2)^c]\\) \\(\\Leftrightarrow \\omega_0 \\in [(B_1 \\cup B_2) \\cap D^c] \\cup (A_1 \\cup B_1 \\cup A_2 \\cup B_2)^c,\\) com \\(D = (A_1 \\cap B_2) \\cup (A_2 \\cap B_1).\\) \\(A_1 \\cup A_2 \\precsim B_1 \\cup B_2 \\Leftrightarrow\\) \\(\\omega_0 \\in (B_1 \\cup B_2) \\cup (A_1 \\cup A_2 \\cup B_1 \\cup B_2)^c\\) Como \\((B_1 \\cup B_2) \\cap D^c \\subseteq (B_1 \\cup B_2)\\), vale o SP2. \\(~\\) (SP3) \\(\\varnothing \\precsim A \\Leftrightarrow \\omega_0 \\in A \\cup (\\varnothing \\cup A)^c\\) \\(\\Leftrightarrow \\omega_0 \\in A \\cup A^c = \\Omega.\\) Como \\(\\Omega\\) é não-vazio, \\(\\exists \\omega_0 \\in \\Omega\\) e, portanto, \\(\\varnothing \\prec \\Omega\\). \\(~\\) (SP4) Exercício! \\(~\\) Exemplo 2: \\(\\Omega = \\mathbb{N}\\), \\(\\mathcal{A} = \\mathcal{P}(\\mathbb{N})\\). \\(A \\precsim B \\Leftrightarrow \\{B\\) é infinito ou \\(A\\) e \\(B\\) são finitos com \\(|A| \\leq |B|\\}\\). Verifique se \\(\\precsim\\) satisfaz SP1 a SP4. \\(~\\) Teorema 5: Se \\(A_1 \\subseteq A_2 \\subseteq \\ldots\\) é uma sequência crescente de eventos e \\(B\\) é tal que \\(A_n \\precsim B, \\forall n\\) então \\[\\bigcup_{n \\geq 1} A_n \\precsim B.\\] Demo: \\(A_n^c \\supseteq A_{n+1}^c\\) e, pelo Teo 3, \\(A_n^c \\succsim B^c\\), \\(\\forall n\\). Por SP4, \\(\\bigcap_{n \\geq 1} A_n^c \\succsim B^c\\) \\(\\Rightarrow \\bigcup_{n \\geq 1} A_n \\precsim B.\\) \\(~\\) Teorema 6: \\(\\left(A_n\\right)_{n \\geq 1}\\) e \\(\\left(B_n\\right)_{n \\geq 1}\\) sequências tais que \\(A_i \\cap A_j = B_k \\cap B_l = \\varnothing\\), \\(\\forall i \\neq j\\), \\(\\forall k \\neq l\\). \\[A_i \\precsim B_i, \\forall i ~\\Rightarrow~ \\bigcup_{n \\geq 1} A_n \\precsim \\bigcup_{n \\geq 1} B_n.\\] Se existe ao menos um \\(j\\) tal que \\(A_j \\prec B_j\\) então \\(\\displaystyle{ \\bigcup_{n \\geq 1} A_n \\prec \\bigcup_{n \\geq 1} B_n }.\\) Demo: Da extensão de SP2, temos que \\(\\displaystyle{ \\bigcup_{i = 1}^n A_i \\precsim \\bigcup_{i = 1}^n B_i }\\), \\(\\forall n \\geq 1\\) \\(~\\Rightarrow~ \\displaystyle{ \\bigcup_{i = 1}^n A_i \\precsim \\bigcup_{i = 1}^{\\infty} B_i }\\), \\(\\forall n \\geq 1\\) \\(~\\Rightarrow~ \\displaystyle{ \\bigcup_{i = 1}^{\\infty} A_i \\precsim \\bigcup_{i = 1}^{\\infty} B_i }~\\) (Teo 5) \\(\\exists n_0\\) tal que \\(A_{n_0} \\prec B_{n_0}\\). De SP2, temos que, para \\(n \\geq n_0\\), \\(\\displaystyle \\bigcup_{i = 1}^{n_0} A_i = \\bigcup_{i = 1}^{n_0-1} A_i \\cup A_{n_0} \\prec \\bigcup_{i = 1}^{n_0-1} B_i \\cup B_{n_0} = \\bigcup_{i = 1}^{n_0} B_i\\) \\(~\\Rightarrow~ \\displaystyle \\bigcup_{i = 1}^{n_0} A_i \\prec \\bigcup_{i = 1}^{n_0} B_i.\\) Da primeira parte, temos que \\(\\displaystyle{ \\bigcup_{i = n_0+1}^{\\infty} A_i \\precsim \\bigcup_{i = n_0+1}^{\\infty} B_i } ~\\) e, por SP2, \\(\\displaystyle \\bigcup_{i = 1}^{n_0} A_i \\cup \\bigcup_{i = n_0+1}^{\\infty} A_i \\prec \\bigcup_{i = 1}^{n_0} B_i \\cup \\bigcup_{i = n_0+1}^{\\infty} B_i\\) provando o resultado. \\(~\\) 2.4 Medida de Probabilidade que representa \\(\\precsim\\) \\(~\\) SP5: Existe uma variável aleatória \\(X: \\Omega \\longrightarrow \\mathbb{R}\\), \\(\\mathcal{A}\\)-mensurável, tal que \\(X(\\omega) \\in [0,1], \\forall \\omega \\in \\Omega\\) e, se \\(I_1\\) e \\(I_2\\) são intervalos contidos em \\([0,1]\\), \\(\\{X \\in I_1\\} \\precsim \\{X \\in I_2\\} \\Leftrightarrow \\lambda(I_1) \\leq \\lambda(I_2)~.\\) Se \\(I=[a,b] \\subseteq [0,1]\\), \\(\\lambda(I) = b-a\\) é o comprimento do intervalo \\(I\\) (medida de Lebesgue). Experimento auxiliar ; \\(X \\sim\\) Uniforme[0,1]. \\(\\{X \\in [a,b]\\}\\) \\(\\sim \\{X \\in (a,b]\\}\\) \\(\\sim \\{X \\in [a,b)\\}\\) \\(\\sim \\{X \\in (a,b)\\}\\). \\(~\\) Teorema 7: Seja \\(A \\in \\mathcal{A}\\). Então \\(\\exists! a^* \\in [0,1]\\) tal que \\(A \\sim \\{X \\in [0,a^*]\\}\\). Demo: Seja \\(U(A) = \\left\\{ a \\in [0,1] : A \\precsim \\{X \\in [0,a]\\} \\right\\}\\). \\(1 \\in U(A)\\) pois \\(\\Omega = \\{X \\in [0,1]\\} \\succsim A\\) \\(~\\Rightarrow~ U(A) \\neq \\varnothing\\). Tome \\(a^* = \\inf U(A)\\). \\(~\\) (i) Considere \\((a_n)_{n \\geq 1}\\), \\(a_n \\in [0,1], \\forall n \\geq 1\\), tal que \\(a_n \\geq a_{n+1} \\geq a^*\\) e \\(a_n \\downarrow a^*\\). Então, \\(\\forall n \\geq 1\\) , \\(\\{X \\in [0,a_n]\\} \\succsim A\\). Por SP4, \\(\\displaystyle \\bigcap_{n=1}^\\infty \\{X \\in [0,a_n]\\} \\succsim A\\) \\(~\\Rightarrow~ \\{X \\in [0,a^*]\\} \\succsim A\\) \\(~\\) (ii) Se \\(a^*=0\\) , \\(\\{X \\in [0,0]\\} \\sim \\varnothing \\precsim A\\) (por SP3). Se \\(a^* &gt; 0\\) , considere \\((a_n)_{n \\geq 1}\\) com \\(a_n \\leq a_{n+1} &lt; a^*\\) e \\(a_n \\uparrow a^*\\). \\(\\{X \\in [0,a_n]\\} \\precsim A, \\forall n \\geq 1\\) e, pelo Teo 5, \\(\\displaystyle \\bigcup_{n=1}^{\\infty} \\{X \\in [0,a_n]\\} \\precsim A\\) \\(~\\Rightarrow~ \\{X \\in [0,a^*)\\} \\sim \\{X \\in [0,a^*]\\} \\precsim A\\). \\(~\\) De (i) e (ii), temos que \\(A \\sim \\{X \\in [0,a^*]\\}\\). \\(~\\) \\(a^*\\) é único pois se \\(a_1 &lt; a^* &lt; a_2\\) são outros valores quaisquer, segue que \\(\\{X \\in [0,a_1]\\} \\prec \\{X \\in [0,a^*]\\} \\prec \\{X \\in [0,a_2]\\}\\) e só um desses eventos pode ser equivalente à \\(A\\). \\(~\\) Teorema 8: A probabilidade do evento \\(A\\), \\(P(A)\\), é definida como \\(a^* \\in [0,1]\\) tal que \\(A \\sim \\{X \\in [0,a^*]\\}\\). Assim, \\(A \\sim \\left\\{X \\in \\left[0,P(A)\\right]\\right\\}\\). A função de probabilidade assim definida satisfaz: \\[A \\precsim B ~\\Leftrightarrow~ P(A) \\leq P(B).\\] Demo: Do Teo 7, \\(A \\sim \\left\\{X \\in \\left[0,P(A)\\right]\\right\\}\\) e \\(B \\sim \\left\\{X \\in \\left[0,P(B)\\right]\\right\\}\\). \\(A \\precsim B\\) \\(~\\Leftrightarrow~ \\left\\{X \\in \\left[0,P(A)\\right]\\right\\} \\precsim \\left\\{X \\in \\left[0,P(B)\\right]\\right\\}\\) \\(~\\Leftrightarrow~ \\lambda \\left([0,P(A)]\\right) \\leq \\lambda \\left([0,P(B)]\\right)\\) \\(~\\Leftrightarrow~ P(A) \\leq P(B).\\) \\(~\\) Teorema 9: A função \\(P: \\mathcal{A} \\longrightarrow [0,1]\\) que, para cada \\(A \\in \\mathcal{A}\\), associa \\(P(A)\\) tal que \\(A \\sim \\left\\{X \\in \\left[0,P(A)\\right]\\right\\}\\) é uma medida de probabilidade (no sentido \\(\\sigma\\)-aditiva). Demo: (i) \\(P(A) \\geq 0\\). \\(\\Omega \\sim \\{X \\in [0,1]\\}\\Rightarrow P(\\Omega)=1\\). \\(\\varnothing \\sim \\{X \\in [0,0]\\} \\Rightarrow P(\\varnothing)=0\\) \\(\\varnothing \\precsim A \\Rightarrow 0 \\leq P(A)\\). \\(~\\) (ii) Seja \\(A\\) e \\(B\\) tal que \\(A \\cap B = \\varnothing\\). Vamos mostrar que \\(P(A \\cup B) = P(A) + P(B)\\). Pelo Teo 8, \\(A \\sim \\{ X \\in [0,P(A)]\\}\\), \\(B \\sim \\{ X \\in [0,P(B)]\\}\\), \\(A \\cup B \\sim \\{ X \\in [0,P(A \\cup B)]\\}\\). Como \\(A \\subseteq A \\cup B\\) e, por SP3, \\(A \\precsim A \\cup B\\), vale que \\(P(A) \\leq P(A \\cup B)\\). Vamos verificar que \\(B \\sim \\left\\{X \\in \\left(P(A),P(A \\cup B) \\right]\\right\\}\\). Suponha, por absurdo, \\(B \\prec \\left\\{X \\in \\left(P(A),P(A \\cup B) \\right]\\right\\}\\). \\(A \\precsim \\{X \\in [0,P(A)]\\}\\) \\(~\\overset{SP2}{\\Longrightarrow}~\\) \\(A \\cup B \\prec \\{X \\in [0,P(A)]\\} \\cup \\left\\{X \\in \\left(P(A),P(A \\cup B) \\right]\\right\\}\\) \\(~\\Rightarrow~ A \\cup B \\prec \\left\\{X \\in [0,P(A)] \\cup \\left(P(A),P(A \\cup B) \\right]\\right\\}\\) \\(~\\Rightarrow~ A \\cup B \\prec \\left\\{X \\in \\left[0,P(A \\cup B) \\right]\\right\\}~\\) (Absurdo!) Analogamente, \\(B \\succ \\left\\{X \\in \\left(P(A),P(A \\cup B) \\right]\\right\\}\\) é absurdo! Logo, \\(B \\sim \\left\\{X \\in \\left(P(A),P(A \\cup B) \\right]\\right\\} \\sim \\left\\{X \\in \\left[0, P(A \\cup B)-P(A) \\right]\\right\\}\\). Como \\(B \\sim \\left\\{X \\in \\left[0,P(B)\\right]\\right\\}\\), temos que \\(P(A \\cup B) = P(A) + P(B)\\). \\(~\\) Corolário 1: Se \\(A_1, \\ldots, A_n\\) são eventos disjuntos, então \\(P\\left(\\bigcup_{i=1}^{n} A_i\\right) = \\sum_{i=1}^{n} P\\left(A_i\\right)\\). Demo: Basta repetir o argumento da segunda parte da demonstração anterior \\(n-1\\) vezes. \\(~\\) Teorema 10: Seja \\(A_1 \\supseteq A_2 \\supseteq \\ldots\\) uma seq. decrescente de eventos tais que \\(\\bigcap_{i=1}^{\\infty} A_i = \\varnothing\\). Então \\(\\displaystyle \\lim_{n \\uparrow \\infty} P(A_n) = 0\\). Demo: \\(A_1 \\supseteq A_2 \\supseteq \\ldots\\) \\(\\Rightarrow\\) \\(P(A_1) \\geq P(A)_2 \\geq \\ldots\\). Além disso, \\(\\displaystyle \\lim_{n \\uparrow \\infty} P(A_n) = b\\). Como \\(P(A_n) \\geq b\\), \\(\\forall n\\), segue que \\(A_n \\succsim \\{X \\in [0,b]\\}\\), \\(\\forall n\\). Por SP4, \\(\\varnothing = \\bigcap_{i=n}^{\\infty} A_i \\succsim \\{X \\in [0,b]\\}\\). Se \\(b&gt;0\\), então \\(\\{X \\in [0,b]\\} \\succ \\{X \\in [0,b/2]\\} \\succsim \\varnothing\\). Como essa relação contradiz a anterior, temos que \\(b\\) deve ser igual a \\(0\\). \\(~\\) Teorema 9: (conclusão) Usando o Corolário 1 e o Teorema 10 é possível concluir a demonstração do Teorema 9, mostrando que \\(P\\) é \\(\\sigma\\)-aditiva, isto é, \\[P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P\\left(A_i\\right) ~,~~ A_i \\cap A_j = \\varnothing, \\forall i \\neq j.\\] Demo: Seja \\((A_n)_{n \\geq 1}\\) sequência de eventos disjuntos. Segue do Corolário 1 que (i) \\(\\displaystyle P\\left(\\bigcup_{i=1}^{\\infty} A_n\\right) = \\sum_{i=1}^{n} P\\left(A_i\\right) + P\\left(\\bigcup_{j=n+1}^{\\infty} A_j\\right)\\), \\(n=1,2,\\ldots\\) Considere \\(\\displaystyle B_n=\\bigcup_{j=n+1}^{\\infty} A_j\\), \\(n \\geq 1\\), uma sequência decrescente de eventos tais que \\(\\displaystyle \\bigcap_{n=1}^{\\infty} B_n = \\varnothing\\). Pelo Teorema 10, segue que \\(\\displaystyle \\lim_{n\\uparrow \\infty} P(B_n) = 0\\). Assim, tomando o limite do lado direito de (i), segue que \\(\\displaystyle P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right)\\) \\(=\\displaystyle \\lim_{n\\uparrow \\infty} \\sum_{i=1}^{n} P\\left(A_i\\right) + \\lim_{n\\uparrow \\infty} P\\left(B_n\\right)\\) \\(=\\displaystyle \\sum_{i=1}^{\\infty} P\\left(A_i\\right)\\). \\(~\\) Teorema 11: Se a relação de crença \\(\\precsim\\) obedece SP1 a SP5 então \\(\\exists !~ P: \\mathcal{A} \\rightarrow [0,1]\\), medida de probabilidade, tal que \\(P\\) representa \\(\\precsim~.\\) Demo: Já foi mostrado que \\(P\\) é uma medida de probabilidade \\(\\sigma\\)-aditiva, de modo que apenas resta mostrar a unicidade de \\(P\\). Considere que existe uma outra medida \\(P&#39;\\) que concorde com a relação \\(\\precsim\\). Como \\(X\\sim\\text{Unif}(0,1)\\), \\(P&#39;\\left(\\left\\{X\\in[0,a]\\right\\}\\right)=a\\). Se \\(A\\) é um evento, existe um único \\(a^*\\) tal que \\(A\\sim\\{X\\in[0,a^*]\\}\\) e, como \\(P&#39;\\) concorda com a relação \\(\\precsim\\), \\(P&#39;(A)\\) \\(=P&#39;\\left(\\left\\{X\\in[0,a^*]\\right\\}\\right)\\) \\(=a^*\\) \\(=P(A)~.\\) \\(~\\) 2.5 Medida de Probabilidade Condicional Nova Relação: \\((A|D) \\precsim (B|D)\\) (Sabendo que \\(D\\) ocorreu, \\(B\\) é preferível a \\(A\\)). Para \\(D = \\Omega\\), temos o caso anterior: \\(A \\precsim B\\) \\(\\Leftrightarrow (A|\\Omega) \\precsim (B|\\Omega)\\). Suponha que vale as suposições SP1 a SP5 e, adicionalmente, SP6: \\((A|D) \\precsim (B|D) \\Leftrightarrow (A \\cap D) \\precsim (B \\cap D)\\) \\(~~\\Big( (A \\cap D|\\Omega) \\precsim (B \\cap D|\\Omega) \\Big)\\) \\(~\\) Propriedades decorrentes de SP1 a SP6: \\(\\forall A,B,D\\), \\((A|D) \\precsim (B|D)\\) ou \\((B|D) \\precsim (A|D)\\). Se \\((A|D) \\precsim (B|D)\\) e \\((B|D) \\precsim (E|D)\\) então \\((A|D) \\precsim (E|D)\\). \\(A,B,D,E\\) com \\(A \\cap D \\cap E \\sim B \\cap D \\cap E \\sim \\varnothing\\). \\((A|D) \\precsim (B|D)\\) \\(\\Leftrightarrow\\) \\((A \\cup E|D) \\precsim (B \\cup E|D)\\). \\((A|D) \\precsim (B|D)\\) \\(\\Leftrightarrow\\) \\((A^c|D) \\succsim (B^c|D)\\). Seja \\(B, D\\) e \\((A_n)_{n \\geq 1}\\) tal que \\(A_n \\supseteq A_{n+1}\\). \\((B|D) \\precsim (A_n|D)\\), \\(\\forall n\\), então \\(\\displaystyle (B|D) \\precsim \\left(\\bigcap_{n=1}^{\\infty} A_n|D\\right)\\). \\((A_n)_{n \\geq 1}\\) e \\((B_n)_{n \\geq 1}\\) tal que \\(A_i \\cap A_j \\sim A_k \\cap A_l \\sim \\varnothing\\), \\(i \\neq j\\), \\(k \\neq l\\), e \\((A_n|D) \\precsim (B_n|D), \\forall n\\). Então \\(\\displaystyle \\left(\\bigcup_{n=1}^{\\infty} A_n | D \\right) \\precsim \\left(\\bigcup_{n=1}^{\\infty} B_n | D \\right)\\) \\(~\\) Teorema 12: \\(\\forall A, B, D \\in \\mathcal{A}\\), considere \\(\\precsim\\) satisfazendo SP1 a SP6. Então \\(P: \\mathcal{A} \\rightarrow [0,1]\\) de modo que para cada \\(A \\in \\mathcal{A}\\) é associada \\(P(A) \\in [0,1]\\) tal que \\(A \\sim \\left\\{X \\in \\left[0,P(A)\\right]\\right\\}\\) é uma medida de probabilidade que representa \\(\\precsim\\), isto é, \\[(A|\\Omega) \\precsim (B|\\Omega) \\Leftrightarrow P(A) \\leq P(B).\\] Além disso, se \\(D \\in \\mathcal{A}\\) é tal que \\(P(D) \\geq 0\\), então \\[(A|D) \\precsim (B|D) \\Leftrightarrow P(A|D) \\leq P(B|D),\\] onde \\(P(\\cdot|D): \\mathcal{A} \\rightarrow [0,1]\\) é uma medida de probabilidade tal que \\[P(A|D) = \\frac{P(A \\cap D)}{P(D)}.\\] Referências "],["Bayes.html", "3 Introdução à Inferência Bayesiana 3.1 Conceitos Básicos 3.2 Teorema de De Finetti 3.3 Suficiência 3.4 Distribuição a Priori 3.5 Alguns Princípios de Inferência", " 3 Introdução à Inferência Bayesiana 3.1 Conceitos Básicos Inferência Estatística: fazer afirmações sobre quantidades não observáveis em um determinado contexto. \\(\\theta\\) : parâmetro - quantidade desconhecida de interesse (não-observável em determinado contexto). \\(\\Theta\\) : espaço paramétrico - conjunto onde \\(\\theta\\) toma valores (supostamente conhecido). \\(E=\\left(\\boldsymbol X, \\theta, \\left\\{f(\\boldsymbol x|\\theta)\\right\\}\\right)\\): experimento - tornar visível algo que antes era invisível ou, mais especificamente no nosso contexto, observar uma realização \\(\\boldsymbol x \\in \\mathfrak{X}\\) de um vetor aleatório \\(\\boldsymbol X\\) com alguma distribuição \\(f(\\boldsymbol x|\\theta)\\). Essa distribuição pertence, na maioria dos casos, à uma família de distribuições fixada mas que depende do parâmetro desconhecido de interesse \\(\\theta\\). Note que na grande maioria dos problemas do dia a dia de um estatístico ele se utiliza de resultados experimentais para fazer afirmações sobre \\(\\theta\\) e este, por sua vez, é não-observável em geral. \\(\\mathfrak{X}\\) : espaço amostral - conjunto onde \\(\\boldsymbol X\\) toma valores (supostamente conhecido). \\(\\mathcal{F}\\) : \\(\\sigma\\)-álgebra de (sub)conjuntos de \\(\\mathfrak{X}\\). Neste espaço amostral, defini-se uma família \\(\\mathcal{P}=\\{P(\\cdot|\\theta): \\theta \\in \\Theta\\}\\), isto é, um conjunto de distribuições (condicionais) para \\(\\boldsymbol X\\) indexadas por \\(\\theta\\). \\((\\mathfrak{X},\\mathcal{F},\\mathcal{P})\\) : modelo estatístico (clássico). \\(V_x(\\theta)=f(\\boldsymbol x |\\theta)\\) : função de verossimilhança. \\(~\\) 3.1.1 Inferência Frequentista (ou Clássica) \\(\\theta\\) é considerado fixo (apesar de desconhecido) e, portanto, não recebe uma distribuição de probabilidade. Baseia-se no \" princípio\" da amostragem repetida (interpretação frequentista de probabilidade), isto é, supõe que é possivel realizar infinitas vezes o experimento. Assim, o \\(\\boldsymbol x\\) é apenas um dos possiveis resultados (hipóteticos) do experimento. Probabilidade somente é definida em (uma \\(\\sigma\\)-álgebra de) \\(\\mathfrak{X}\\). 3.1.2 Inferência Bayesiana Baseia-se na interpretação subjetivista de probabilidade, de modo que a SUA incerteza sobre algo desconhecido deve ser quantificada (traduzida) em termos de probabilidade. Assim, SUA incerteza sobre o parâmetro (desconhecido) é representada por uma distribuição de probabilidade, \\(\\theta\\) é tratado como uma variável aleatória (v.a.) e SUA distribuição para \\(\\theta\\) antes da realização do experimento, \\(f(\\theta),\\) é chamada de distribuição a priori. Note que a atribuição de uma distribuição a prior para \\(\\theta\\) independe da natureza do parâmetro, ele pode ser a proporção de indivíduos que avalia positivamente o governo atual (quantidade essa que muda a todo instante) ou ainda a milésima casa do \\(\\pi\\) (algum número de 0 a 9, fixo porém desconhecido no momento dessa leitura). A atualização de SUA incerteza sobre \\(\\theta,\\) incorporando uma nova informação trazida pelos dados \\(\\boldsymbol x\\) (representada por \\(f(\\boldsymbol x| \\theta)\\)) é feita pelo Teorema de Bayes: Teorema de Bayes: \\[\\underbrace{f(\\theta| \\boldsymbol x)}_{dist. posteriori}=~~\\dfrac{f(\\theta)f(\\boldsymbol x|\\theta)}{\\displaystyle \\int_{\\Theta}f(\\boldsymbol x|\\theta)dP_\\theta} ~\\propto~ \\underbrace{f(\\theta)}_{priori}\\overbrace{f(\\boldsymbol x|\\theta)}^{verossimilhança}.\\] Toda a inferência sobre \\(\\theta\\) será baseada exclusivamente em \\(f(\\theta| \\boldsymbol x)\\), não sendo necessário considerar pontos amostrais que poderiam mas não foram observados (como é feito na inferência frequentista). \\(~\\) Observação: será utilizada a notação geral para integral (de Lebesgue): \\[\\displaystyle \\int_{\\Theta}f(\\boldsymbol x|\\theta)dP_\\theta = \\left\\{ \\begin{array}{ll} \\displaystyle \\int_{\\Theta}f(\\boldsymbol x|\\theta) f(\\theta) d\\theta ~&amp;~ \\text{(caso abs. contínuo)}\\\\ \\displaystyle \\sum_{\\Theta}f(\\boldsymbol x|\\theta) f(\\theta) ~&amp;~ \\text{(caso discreto)} \\end{array}\\right.\\] \\(~\\) Exemplo 1a. Suponha que existem duas moedas, uma delas tem \\(\\theta =1/2\\) (honesta) e a outra \\(\\theta=3/4\\) (viesada). Uma moeda é escolhida e é feito um lançamento da moeda selecionada. Nesse experimento, tem-se \\(X|\\theta \\sim Ber(\\theta)\\), com \\(\\Theta=\\{1/2,3/4\\}\\) e \\(\\mathfrak{X}=\\{0,1\\}\\). Como chutar o valor de \\(\\theta\\)? Considere que não existe razão para você acreditar que há algum tipo de preferência na escolha de uma ou outra moeda, isto é, considere que a priori \\(f(\\theta=1/2)\\) \\(=f(\\theta=3/4)\\) \\(=1/2\\). Suponha que o lançamento resultou em cara (\\(x=1\\)). Então \\(f(\\theta = 3/4|X=1)\\) \\(=\\dfrac{f(X=1|\\theta=3/4)f(\\theta=3/4)}{\\sum_\\theta f(X=1|\\theta)f(\\theta)}\\) \\(=\\dfrac{\\dfrac{3}{4}\\dfrac{1}{2}}{\\dfrac{3}{4}~\\dfrac{1}{2}+\\dfrac{1}{2}~\\dfrac{1}{2}}=\\) \\(\\dfrac{3/4}{5/4}=\\dfrac{3}{5}\\) \\(= 1-\\underbrace{f(\\theta=1/2|X=1)}_{2/5}\\). Se, no entando, o resultado do lançamento da moeda fosse coroa (\\(x=0\\)), teríamos \\(P(\\theta=3/4|X=0)\\) \\(=\\dfrac{\\dfrac{1}{4}~\\dfrac{1}{2}}{\\dfrac{1}{4}~\\dfrac{1}{2}+\\dfrac{1}{2}~\\dfrac{1}{2}}\\) \\(=\\dfrac{1/2}{1/2+2/2}=\\dfrac{1}{3}\\). Assim, se sua decisão for escolher o valor mais provável de \\(\\theta\\) após observar \\(x\\), a conclusão seria que a moeda é viesada \\((\\theta=3/4)\\) se for observado cara \\((x=1)\\) e que a moeda é honesta \\((\\theta=1/2)\\) se o resultado for coroa \\((x=0)\\). \\(~\\) Exemplo 1b. Considere agora que serão realizados \\(n\\) lançamentos da moeda, de modo que agora tem-se \\(X|\\theta \\sim Bin(n,\\theta)\\), \\(\\theta \\in \\{1/2,3/4\\}\\), \\(x \\in \\{0,1,\\ldots,n\\}\\). Suponha que observa-se \\(X=x\\). \\(f(\\theta=3/4|X=x)\\) \\(=\\dfrac{f(x|\\theta=3/4)f(\\theta=3/4)}{\\displaystyle \\sum_{\\theta\\in \\{1/2,3/4\\}}f(x|\\theta)f(\\theta)}\\) \\(=\\dfrac{\\displaystyle \\binom{n}{x}\\left(\\dfrac{3}{4}\\right)^x\\left(\\dfrac{1}{4}\\right)^{n-x}\\dfrac{1}{2}}{\\displaystyle \\binom{n}{x}\\left(\\dfrac{3}{4}\\right)^x\\left(\\dfrac{1}{4}\\right)^{n-x}\\dfrac{1}{2}+\\displaystyle\\binom{n}{x}\\left(\\dfrac{1}{2}\\right)^x\\left(\\dfrac{1}{2}\\right)^{n-x}\\dfrac{1}{2}}\\) \\(=\\dfrac{1}{1+\\left(\\dfrac{2^n}{3^x}\\right)}\\) \\(=\\dfrac{3^x}{3^x + 2^n}\\). theta = c(0.5,0.75) prior=0.5 # priori P(theta[1]) = 1-P(theta[2]) n=5; post = function(x){ (prior*dbinom(x,n,theta)) / sum(prior * dbinom(x,n,theta)) } tibble(x=as.factor(rep(seq(0,n),each=length(theta))), x1=rep(theta,(n+1)),x2=rep(theta,(n+1)),y1=0, y2=as.vector(apply(matrix(seq(0,n)),1,post))) %&gt;% ggplot() + geom_hline(yintercept=0.5,col=&quot;darkgrey&quot;,lty=3) + geom_segment(aes(x=x1, xend=x2, y=y1,yend=y2,colour=x),lwd=2) + xlab(expression(theta)) + ylab(expression(paste(&quot;P(&quot;,theta,&quot;|x)&quot;))) + theme_bw()+ gganimate::transition_states(x) \\(~\\) Note que o Exemplo 1.a é um caso particular desse exemplo com \\(n=1\\). Se novamente sua decisão é baseada no valor mais provável de \\(\\theta\\), deve-se escolher \\(\\theta=3/4\\) se \\(f(\\theta=3/4|X=x) &gt; f(\\theta=1/2|X=x)\\) \\(\\Longleftrightarrow f(\\theta=3/4|X=x) &gt; \\dfrac{1}{2}\\) \\(\\Longleftrightarrow \\dfrac{3^x}{3^x + 2^n} &gt; \\dfrac{1}{2}\\) \\(\\Longleftrightarrow {3^x} &gt; {2^n}\\) \\(\\Longleftrightarrow \\dfrac{x}{n} = \\bar{x} &gt; \\log_3{2}\\approx 0,63\\). \\(~\\) Exemplo 1c. Considere que uma moeda será lançada \\(n\\) vezes mas que \\(\\theta\\) é desconhecido, de modo que \\(\\Theta = [0,1]\\). Para simplificar, vamos assumir \\(f(\\theta)=\\mathbb{I}_{[0,1]}(\\theta)\\), isto é, \\(\\theta \\sim Unif(0,1)\\sim Beta(1,1)\\). Essa priori corresponde ao caso em que você acredita que todos os valores possíveis para \\(\\theta\\) são igualmente prováveis, assim como nos exemplos anteriores. Novamente, \\(X|\\theta \\sim Bin(n,\\theta)\\) \\(f(\\theta|x)\\) \\(=\\dfrac{f(x|\\theta)f(\\theta)}{\\displaystyle\\int_0^1 f(x|\\theta)f(\\theta)d\\theta}\\) \\(=\\dfrac{\\displaystyle\\binom{n}{x}~\\theta^x(1-\\theta)^{n-x} ~~\\mathbb{I}_{[0,1]}(\\theta)}{\\displaystyle\\int_0^1\\binom{n}{x}~\\theta^x(1-\\theta)^{n-x}d\\theta}=\\) \\(\\dfrac{\\tfrac{\\Gamma(1+x+1+n-x)}{\\Gamma(1+x)\\Gamma(1+n-x)}~~\\theta^x(1-\\theta)^{n-x}~~\\mathbb{I}_{[0,1]}(\\theta)}{\\underbrace{\\displaystyle \\int_0^1\\tfrac{\\Gamma(1+x+1+n-x)}{\\Gamma(1+x)\\Gamma(1+n-x)}~~\\theta^x(1-\\theta)^{n-x}d\\theta}_{1}}\\) \\(=\\tfrac{\\Gamma(1+x+1+n-x)}{\\Gamma(1+x)\\Gamma(1+n-x)}~~\\theta^x(1-\\theta)^{n-x}~~\\mathbb{I}_{[0,1]}(\\theta)\\). Logo \\(\\theta|x \\sim Beta(1+x,1+n-x)\\). Nesse exemplo, o valor mais provável (com maior densidade a posteriori) para \\(\\theta\\) é a moda da distribuição, \\(Moda(\\theta|x)\\) \\(= \\dfrac{(1+x)-1}{(1+x)+(1+n-x)-2}\\) \\(= \\dfrac{x}{n}\\) \\(=\\bar{x}\\). Suponha que foi observado \\(n=5\\) e \\(x=2\\), a posteriori é \\(\\theta|x=2 \\sim Beta(3,4)\\) e a moda é \\(Moda(\\theta|x)\\) \\(=\\frac{1+x-1}{1+1+n-2}\\) \\(=\\frac{2}{5}\\) \\(=0,4\\); Algumas medidas resumo da distribuição posterior para esse exemplo são \\(Moda(\\theta|x)\\) \\(=\\dfrac{1+x-1}{1+1+n-2}\\) \\(=\\dfrac{2}{5}\\) \\(=0,4\\); \\(E[\\theta|x]\\) \\(=\\dfrac{1+x}{1+1+n}\\) \\(=\\dfrac{3}{7}\\) \\(=0,43\\); \\(Med(\\theta|x)\\) \\(\\approx \\dfrac{1+x-1/3}{1+1+n-2/3}\\) \\(=\\dfrac{8/3}{19/3}\\) \\(\\approx 0,42\\); \\(Var(\\theta|x)\\) \\(=\\dfrac{(1+x)(1+n-x)}{(1+1+n)^2(1+1+n+1)}\\) \\(=\\dfrac{12}{392}\\) \\(\\approx 0,031\\). \\(~\\) Exemplo 1d. Por fim, suponha que no exemplo anterior, sua opinião a priori é representada por uma distribuição beta qualquer com parâmetros \\(a\\) e \\(b\\), \\(a,b &gt; 0\\). Desta forma, \\(X|\\theta \\sim Bin(n,\\theta)\\) e \\(\\theta\\sim Beta(a,b)\\). Calculando a distribuição a posteriori de forma similar ao exemplo anterior, temos que \\(\\theta|X=x \\sim Beta(a+x,b+n-x)\\). Note que o exemplo anterior é o caso particular em que \\(a=b=1~.\\) \\(~\\) require(transformr) theta = seq(0,1,0.01) a=2; b=2; n=5 vero1 = as.vector(apply(matrix(seq(0,n)),1, function(x){dbeta(theta,1+x,1+n-x)})) post1 = as.vector(apply(matrix(seq(0,n)),1, function(x){dbeta(theta,a+x,b+n-x)})) tibble(x=as.factor(rep(seq(0,n),each=length(theta))), theta=rep(theta,(n+1)),post=post1,vero=vero1) %&gt;% ggplot() + geom_line(aes(x=theta,y=dbeta(theta,a,b),linetype=&quot;Prior&quot;,colour=&quot;Prior&quot;),lwd=1) + geom_line(aes(x=theta,y=post,linetype=&quot;Posterior&quot;,colour=x),lwd=1.3) + geom_line(aes(x=theta,y=vero,linetype=&quot;Verossimilhança&quot;,colour=x),lwd=1) + xlab(expression(theta)) + ylab(expression(paste(&quot;f(&quot;,theta,&quot;|x)&quot;))) + theme_bw()+labs(linetype=&quot;&quot;)+ gganimate::transition_states(x) \\(~\\) Suponha agora que \\(a=b=2\\), \\(n=5\\) e \\(x=2\\), de modo que \\(\\theta|x=2 \\sim Beta(4,5)\\). Algumas medidas resumo da distribuição posterior para esse exemplo são \\(Moda(\\theta|x)\\) \\(=\\dfrac{a+x-1}{a+b+n-2}\\) \\(=\\dfrac{3}{7}\\) \\(\\approx 0,428\\); \\(E[\\theta|x]\\) \\(=\\dfrac{a+x}{a+b+n}\\) \\(=\\dfrac{4}{9}\\) \\(\\approx 0,444\\); \\(Med(\\theta|x)\\) \\(\\approx \\dfrac{a+x-1/3}{a+b+n-2/3}\\) \\(=\\dfrac{11/3}{25/3}\\) \\(\\approx 0,440\\); \\(Var(\\theta|x)\\) \\(=\\dfrac{(a+x)(b+n-x)}{(a+b+n)^2(a+b+n+1)}\\) \\(=\\dfrac{20}{810}\\) \\(\\approx 0,0247\\). \\(~\\) \\(~\\) 3.2 Teorema de De Finetti Definição. Uma coleção finita \\(X_1,X_2,\\ldots,X_n\\) de quantidades aleatórias é dita permutável se a distribuição de \\(\\left(X_{\\pi_1},\\ldots,X_{\\pi_n}\\right)\\) é a mesma para toda permutação \\(\\boldsymbol\\pi=(\\pi_1,\\ldots,\\pi_n)\\) dos índices \\((1,\\ldots,n)\\). Uma coleção infinita de quantidades aleatórias é permutável se toda subcoleção é permutável. \\(~\\) Segue da definição que cada uma das variáveis \\(X_1,\\ldots,X_n\\) tem a mesma distribuição marginal. Além disso, \\((X_i,X_j)\\) têm mesma distribuição que \\((X_k,X_l)\\), \\(\\forall i\\neq j\\) e \\(k\\neq l\\), e assim por diante. \\(~\\) Proposição. Uma coleção \\(C\\) de variáveis aleatórias é permitável se, e somente se, para todo \\(n\\) finito menor ou igual ao tamanho da coleção \\(C\\), toda \\(n\\)-upla (sequência ordenada de \\(n\\) elementos) de elementos distintos de \\(C\\) têm a mesma distribuição que toda outra \\(n\\)-upla. \\(~\\) Exemplo 1. Considere uma coleção \\(X_1,X_2,\\ldots\\) uma sequência (finita ou infinita) de variáveis aleatórias independentes e identicamente distribuidas (v.a. i.i.d). Note que \\(f(x_1,\\ldots,x_n)=\\displaystyle\\prod_{i=1}^nf(x_i)~,\\) \\(\\forall n~,\\) de modo que \\(\\left(X_{i_1},\\ldots,X_{i_n}\\right)\\) têm a mesma distribuição de \\(\\left(X_{j_1},\\ldots,X_{j_n}\\right)\\), para \\(i_1\\neq\\ldots\\neq i_n\\) e \\(j_1\\neq\\ldots\\neq j_n\\). Então, toda coleção de v.a. i.i.d é permutável. \\(~\\) Exemplo 2: Foi visto no exemplo anterior que a suposição que uma sequência de v.a. é i.i.d. implica que tal sequência é também permutável. Sabe-se também que independência implica em correlação nula, \\(\\rho=0\\). Será então que v.a. identicamente distribuídas e não correlacionadas são também permutáveis? \\(~\\) \\(X_1~\\) / \\(~X_2\\) \\(-1\\) \\(0\\) \\(+1\\) \\(f(x_1)\\) \\(-1\\) \\(0.10\\) \\(0.05\\) \\(0.15\\) \\(0.3\\) \\(0\\) \\(0.15\\) \\(0.20\\) \\(0.05\\) \\(0.4\\) \\(+1\\) \\(0.05\\) \\(0.15\\) \\(0.10\\) \\(0.3\\) \\(f(x_2)\\) \\(0.3\\) \\(0.4\\) \\(0.3\\) \\(1.0\\) \\(~\\) \\(cor(X_1,X_2)\\) \\(=\\frac{\\text{Cov}\\left(X_1,X_2\\right)}{\\sqrt{\\text{Var}(X_1)\\text{Var}(X_2)}}\\) \\(=\\frac{\\text{E}\\left[\\left(X_1-\\text{E}[X_1]\\right)\\left(X_2-\\text{E}[X_2]\\right)\\right]}{\\sqrt{\\text{Var}(X_1)\\text{Var}(X_2)}}\\) \\(=\\frac{\\text{E}\\left[X_1X_2\\right]-\\text{E}[X_1]\\text{E}[X_2]}{\\sqrt{\\text{Var}(X_1)\\text{Var}(X_2)}}\\) \\(E(X_1)=E(X_2)=0\\) \\(E(X_1X_2)=-1\\cdot0,2+0+1\\cdot0,2=0\\) \\(\\Rightarrow cor(X_1,X_2)=0\\) \\((X_1,X_2)\\) são identicamente distribuídas e não correlacionadas mas não são permutáveis pois, por exemplo, \\(P\\big((X_1,X_2)=(1,-1)\\big)~\\neq~P\\big((X_2,X_1)=(1,-1)\\big)~.\\) \\(~\\) Exemplo 3: Suponha que \\(X_1,X_2,\\ldots\\) são condicionalmente i.i.d. dado \\(Y=y\\) com densidade \\(f(x_i|y),\\;i=1,2,\\ldots\\) e \\(Y\\) tem densidade \\(h(y)\\). Então \\(X_1,X_2,\\ldots\\) são permutaveis. \\(f_{X_{i_1},\\ldots,X_{i_n}}(x_1,\\ldots,x_n)\\) \\(=\\displaystyle\\int\\prod_{j=1}^nf(x_j|y)h(y)dy,\\) para qualquer \\(n\\)-upla \\(X_{i_1},\\ldots,X_{i_n}\\). Note que o lado direito não depende dos rótulos \\(i_1,\\ldots,i_n\\). \\(~\\) \\(~\\) Teorema de Representação de De Finetti. (para v.a. Bernoulli) Uma sequência infinita \\(\\left(X_n\\right)_{n\\geq 1}\\) de v.a. Bernoulli é permutável se, e somente se, existe uma v.a \\(\\theta\\) em \\([0,1]\\) tal que, condicional a \\(\\theta\\), \\(\\left(X_i\\right)_{n\\geq 1}\\) são i.i.d. \\(Ber(\\theta)\\). Além disso, se a sequência é permutável, então a distribuição de \\(\\theta\\) é única e \\(\\displaystyle\\bar{X}_n = \\dfrac{1}{n}\\sum_{i=1}^\\infty X_i ~\\underset{n\\uparrow\\infty}{\\overset{q.c.}{\\longrightarrow}}~\\theta~.\\) \\(P(X_1=x_1,\\ldots,X_n=x_n)\\) \\(=\\displaystyle\\int_0^1\\theta^{\\sum x_i}(1-\\theta)^{n-\\sum x_i}~dF(\\theta)\\) \\(=\\displaystyle\\int_0^1\\prod_{i=1}^n\\underbrace{\\theta^{x_i}(1-\\theta)^{1-x_i}}_{f(x_i|\\theta)}~f(\\theta)~d\\theta~,\\) onde \\(F(\\theta)=\\displaystyle\\lim_{n\\uparrow\\infty}~\\text{P}\\left(\\dfrac{\\sum_iX_i}{n}\\leq \\theta\\right)~.\\) \\(~\\) \\(~\\) Exemplo 4: (1.19/1.20 - Schervish) Seja \\(\\left(X_n\\right)_{n\\geq 1}\\) v.a. Bernoulli. \\(~\\) Considere que o Estatístico 1 acredita que \\(P_1(X_1=x_1,\\ldots,X_n=x_n)\\) \\(=\\dfrac{12}{x+2}\\dfrac{1}{\\binom{n+4}{x+2}}~,\\) de modo que \\(P_1(X_1=1)\\) \\(=\\dfrac{12}{3}\\dfrac{3!~2!}{5!}\\) \\(=\\dfrac{4}{10}\\) \\(=0,4~.\\) Por outro lado, o Estatístico 2 acredita que \\(P_2(X_1=x_1,\\ldots,X_n=x_n)\\) \\(=\\dfrac{1}{(n+1)\\binom{n}{x}}\\) e, então, \\(P_2(X_1=1)=\\dfrac{1}{2}=0,5~.\\) \\(~\\) Contudo, pelo Teorema de Finetti, ambos acreditam que o limite \\(\\theta=\\displaystyle\\lim_{n\\uparrow\\infty}\\frac{1}{n}\\sum_{i=1}^nX_i\\) existe com probabilidade 1 e que \\(P(X_1=1|\\theta)=\\theta\\), mas não tem opiniões diferentes sobre \\(\\theta\\). \\(~\\) Suponha agora que foi observado \\(\\boldsymbol{x}=(x_1,\\ldots,x_{20})\\) com \\(\\displaystyle\\sum_{i=1}^{20}x_i=14\\). Então, \\(P_i(X_{21}=1|X_1=x_1,\\ldots,X_{20}=x_{20})\\) \\(=\\dfrac{P_i(X_1=x_1,\\ldots,X_{20}=x_{20},X_{21}=1)}{P_i(X_1=x_1,\\ldots,X_{20}=x_{20})}\\) de modo que, \\(P_1(X_{21}=1|\\mathbf X=\\mathbf x)=\\) \\(\\dfrac{\\dfrac{12}{17}\\dfrac{1}{\\binom{25}{17}}}{\\dfrac{12}{16}\\dfrac{1}{\\binom{24}{16}}}\\) \\(=\\dfrac{16}{17}\\dfrac{\\dfrac{24!}{16!8!}}{\\dfrac{25!}{17!8!}}\\) \\(=\\dfrac{16}{17}\\dfrac{17}{25}\\) \\(=\\dfrac{16}{25}=0,64\\) \\(P_2(X_{21}=1|\\mathbf X=\\mathbf x)\\) \\(=\\dfrac{\\dfrac{1}{22\\binom{21}{15}}}{\\dfrac{1}{21\\binom{20}{14}}}\\) \\(=\\dfrac{21}{22}\\dfrac{\\dfrac{20!}{14!6!}}{\\dfrac{21\\cdot20!}{15\\cdot 14!6!}}\\) \\(=\\dfrac{21}{22}\\dfrac{15}{21}\\) \\(=\\dfrac{15}{22}=0,68\\) \\(~\\) \\(~\\) Definição. Seja \\(X_1,\\ldots,X_n\\) uma sequência de variáveis aleatórias permutáveis. A função de distribuição empírica é definida como \\(F_n(x) = \\displaystyle\\dfrac{1}{n}\\sum_{i=1}^{n} \\mathbb{I}(x_i\\leq x)~.\\) \\(~\\) \\(~\\) Teorema de Representação de De Finetti Uma sequência de v.a.s \\(\\{X_n\\}_{n\\geq 1}\\) assumindo valores em (um subconjunto de) \\(\\mathbb R\\) é permutável se, e somente se, existe uma medida de probabilidade sobre (uma \\(\\sigma\\)-álgebra do) conjunto de funções de distribuições que sorteia uma \\(F\\) e, dada esta \\(F\\), os elementos da sequência \\(\\{X_n\\}_{n\\geq 1}\\) são i.i.d. com distribuição \\(F\\). Isto é, \\(F_{\\mathbf X}(x_1,\\ldots,x_n)=\\displaystyle\\int\\prod_{i=1}^n F(x_i)d\\mu(F)\\), \\(\\forall n\\). Além disso, \\(F_n\\underset{n\\uparrow\\infty}{\\longrightarrow}F\\) e a distribuição de \\(F=\\underset{n\\uparrow\\infty}{lim}F_n\\) é única e é \\(\\mu\\). \\(~\\) \\(~\\) 3.3 Suficiência Muitas vezes, a quantidade de dados é muito grande e desejamos resumir a informação trazida pelos dados. Uma forma de fazê-lo sem perder informação sobre o parâmetro de interesse é usar uma estatística suficiente. \\(~\\) Definição. Dizemos que uma função da amostra \\(T:\\mathfrak{X} \\rightarrow \\mathbb{R}^p\\) é uma estatística suficiente (do ponto de vista frequentista) se \\(f\\left(\\boldsymbol x | T(\\boldsymbol x),\\theta\\right) = f\\left(\\boldsymbol x | T(\\boldsymbol x)\\right)\\). \\(~\\) Em palavras, conhecendo o valor da estatística suficiente, a distribuição da amostra (do v.a. \\(\\boldsymbol X\\)) não depende mais do parâmetro \\(\\theta\\). Isso quer dizer que a informação disponível na amostra \\(\\boldsymbol X\\) sobre \\(\\theta\\) está contida em \\(T(\\boldsymbol X)\\). Obter uma estatística suficiente nem sempre é uma tarefa fácil mas o resultado a seguir, conhecido como critério da fatoração permite identificar estatísticas suficientes. \\(~\\) Teorema. A estatística \\(T:\\mathfrak{X} \\rightarrow \\mathbb{R}^p\\) é suficiente para a família de distribuições \\(\\left\\{f(\\cdot|\\theta):\\theta \\in \\Theta\\right\\}\\) se, e somente se, para todo \\(x \\in \\mathfrak{X}\\) e para todo \\(\\theta \\in \\Theta\\), podemos escrever \\(f\\left(\\boldsymbol x | \\theta\\right)\\) \\(= u(\\boldsymbol x) v\\left(T(\\boldsymbol x),\\theta\\right)\\), onde \\(u\\) é uma função positiva que não depende de \\(\\theta\\) e \\(v\\) é uma função não-negativa e depende de \\(\\boldsymbol x\\) somente através de \\(T(\\boldsymbol x)\\). \\(~\\) Exemplo. Seja \\(X_1,\\ldots,X_n\\) v.a. tais que, condicional ao conhecimento de \\(\\theta\\), são c.i.i.d. com \\(X_1|\\theta \\sim Exp(\\theta)\\). Então, \\(f(\\boldsymbol x|\\theta)\\) \\(=\\prod f(x_i|\\theta)\\) \\(=\\prod \\theta e^{-\\theta x_i} ~\\mathbb{I}_{\\mathbb{R+}}(x_i)\\) \\(=\\theta^n e^{-\\theta \\sum x_i} ~\\prod ~\\mathbb{I}_{\\mathbb{R+}}(x_i)\\) \\(= v\\left(\\sum x_i, \\theta\\right) u(\\boldsymbol x)\\). Portanto, \\(T(\\boldsymbol x) = \\sum x_i\\) é estatística suficiente para \\(\\theta\\). De fato, como \\(T(\\boldsymbol X)\\) \\(= \\sum X_i | \\theta\\) \\(\\sim Gama(n,\\theta)\\) e \\(\\left\\{X_1=x_1,\\ldots,X_n=x_n\\right\\}\\) \\(\\subseteq \\left\\{T(\\boldsymbol X) = \\sum X_i = \\sum x_i = t\\right\\}~,\\) \\(f\\left(\\boldsymbol x| T(\\boldsymbol x),\\theta\\right)\\) \\(=\\dfrac{f\\left(\\boldsymbol{x},T(\\boldsymbol{x})|\\theta\\right)}{f\\left(T(\\boldsymbol{x})|\\theta\\right)}\\) \\(=\\dfrac{f\\left(\\boldsymbol{x}|\\theta\\right)}{f\\left(t|\\theta\\right)}\\) \\(=\\dfrac{\\theta^n e^{\\theta \\sum x_i} ~\\prod ~\\mathbb{I}_{\\mathbb{R+}}(x_i)}{\\frac{\\theta^n}{\\Gamma(n)}t^{n-1} e^{\\theta t} ~\\prod ~\\mathbb{I}_{\\mathbb{R+}}(x_i)}\\) \\(= \\dfrac{\\Gamma(n)}{t^{n-1}} ~\\mathbb{I}_{\\mathbb{R}_+}\\left(t\\right)~,\\) que não depende de \\(\\theta\\). \\(~\\) Sob o enfoque bayesiano, a definição de suficiência é um pouco mais intuitiva que a frequentista. Definição: Dizemos que uma função da amostra \\(T:\\mathfrak{X} \\rightarrow \\mathbb{R}^p\\) é uma estatística suficiente (no sentido bayesiano) se \\(f\\left(\\theta | T(\\boldsymbol x)\\right) = f\\left(\\theta | \\boldsymbol x\\right)\\), para todo \\(x \\in \\mathfrak{X}\\). \\(~\\) Voltando ao exemplo, suponha agora que, a priori, \\(\\theta \\sim Gama(a,b)\\). Então, \\(f(\\theta| \\boldsymbol x)\\) \\(\\propto f(\\boldsymbol x|\\theta)f(\\theta)\\) \\(\\propto \\theta^n e^{-\\theta \\sum x_i} ~~\\theta^{a-1}e^{-b\\theta}\\) \\(\\propto \\theta^{a+n-1} e^{-(b+\\sum x_i)\\theta}\\) Seja \\(T = T(\\boldsymbol X) = \\sum X_i\\), temos que \\(T|\\theta\\sim Gamma(n,\\theta)\\), de modo que \\(f\\left(\\theta| T(\\boldsymbol x)=t\\right)\\) \\(\\propto f(t|\\theta)f(\\theta)\\) \\(\\propto \\theta^n t^{n-1} e^{\\theta t} ~~\\theta^{a-1}e^{-b\\theta}\\) \\(\\propto \\theta^{a+n-1} e^{-(b+t)\\theta}\\) , com \\(t=\\sum x_i\\). Assim, \\(\\theta|\\boldsymbol x\\) \\(\\sim \\theta|T(\\boldsymbol x)\\) \\(\\sim Gamma\\left(a+n,b+\\sum x_i\\right)\\) e, portanto, \\(T(\\boldsymbol X) = \\sum X_i\\) é estatística suficiente para \\(\\theta\\). \\(~\\) Pelo teorema da fatoração, temos que \\(f\\left(\\boldsymbol x | \\theta\\right)\\) \\(= u(\\boldsymbol x) v\\left(T(\\boldsymbol x),\\theta\\right)\\) e, portanto \\(f(\\theta|\\boldsymbol x)\\) \\(\\propto f(\\theta) f\\left(\\boldsymbol x | \\theta\\right)\\) \\(\\propto f(\\theta) v\\left(T(\\boldsymbol x),\\theta\\right)~,\\) que só depende de \\(\\boldsymbol x\\) por meio de \\(T(\\boldsymbol x)\\). Para os casos mais comuns, as definições são equivalentes (Schervish 2012). \\(~\\) Um dos princípios de inferência estatística é o princípio da suficiência. Segundo este, se \\(T\\) é uma estatística suficiente para \\(\\theta\\) e se dois pontos amostrais \\(\\boldsymbol x, \\boldsymbol y \\in \\mathfrak{X}\\) são tais que \\(T(\\boldsymbol x)=T(\\boldsymbol y)\\) então as inferências baseadas nesses pontos devem ser as mesmas. Adiante, retomaremos esse princípio de forma mais formal. \\(~\\) 3.4 Distribuição a Priori A priori é sempre subjetiva (assim como a escolha do modelo estatístico)! Por exemplo, dizer que os dados seguem uma distribuição normal, é uma escolha subjetiva, muitas vezes baseadas nas facilidades matemáticas que essa distribuição proporciona. Do mesmo modo, suponha que dois indivíduos que consideram que a distribuição do parêmetro é simétrica, com mesmas suposições sobre média e variância. O primeiro pode optar por representar sua distribuição usando uma distribuição Normal, enquanto o segundo pode utilizar uma distribuição T ou Cauchy. Não existe opinião errada, existem opiniões diferentes, dado o nível de conhecimento e as experiências prévias do indivíduo. Contudo, algumas boas práticas devem ser consideradas como, por exemplo, tomar cuidado para não atribuir probabilidade nula a pontos possíveis do espaço paramétrico. A priori deve ser sua opinião apenas sobre o parâmetro \\(\\theta\\) e não deve depender de fatores como o desenho do experimento ou o objetivo do estudo. 3.4.1 Método do Histograma Muitas vezes, para extrair o conhecimento de um especialista, podemos dividir o espaço paramétrico em regiões e pedir para o especialista ordenar esses conjuntos, utilizando pesos que refletem a crença que o parâmetro esteja em cada uma daquelas regiões. Exemplo 1. (Albert (2009), pág 27) Seja \\(\\theta\\) uma proporção desconhecida \\((\\Theta=[0,1])\\); Considere a partição \\(T = \\left\\{[0,0.1), [0.1,0.2), \\ldots, [0.9,1] \\right\\}\\); Suponha que um especialistas atribui pesos \\(p=(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0)\\) a esse intervalos; A piori, nesse caso, é o histograma apresentado a seguir. p=c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0) prior = c(0,p/(sum(p))) tibble(theta=seq(0,1,0.1), prior) %&gt;% ggplot(data=.) + geom_step(aes(x=theta,y=prior),direction=&quot;vh&quot;,color=&quot;red&quot;,lwd=1.5) Voltando ao exemplo da moeda, suponha novamente que foram observados \\(x=2\\) sucessos em \\(n=5\\) lançamentos. A posteriori nesse caso pode ser obtida multiplicando a distribuição a priori pela verossimilhança e padronizando a função obtida. Assim: n=5 x=2 p = c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0) p = p/(sum(p)) theta = seq(0,1,0.01) prior = c(rep(p,each=10),0)/sum(c(rep(p,each=10),0)) vero = dbinom(x,n,theta)/sum(dbinom(x,n,theta)) post = (prior * vero)/sum(prior * vero) pH = tibble(theta=rep(theta,3),dens=c(prior,vero,post),Dist=rep(c(&#39;1.priori&#39;,&#39;2.verossimilhança&#39;,&#39;3.posteriori&#39;),each=101)) %&gt;% ggplot(data=.) + geom_line(aes(x=theta,y=dens,colour=Dist),lwd=1.5) pH \\(~\\) 3.4.2 Elicitação de Hiperparâmetros Nessa abordagem, a priori é obtida da seguinte maneira: Escolha uma família de distribuições conveniente. O conceito de conveniência aqui pode levar em conta, por exemplo, o suporte da distribuição, se é flexível o suficiente para acomodar diversos tipos de opinião, se permite a obtenção analítica da posteriori e assim por diante; Obtenha um conjunto de medidas resumo (como média, variância, quantis, etc.); Utilize as medidas resumo para calcular hiperparâmetros da distribuição escolhida. \\(~\\) Exemplo: Na seção anterior, a priori dada pelo histograma tem média \\(m=0.31\\) e variância aproximadamente \\(v=0.02\\). Podemos utilizar como priori, por exemplo, uma distribuição beta com essa média e variância, já que a beta tem um suporte conveniente e facilita as contas, como também já vimos. Assim, vamos considerar uma distribuição \\(Beta(a,b)\\) e escolher \\(a\\) e \\(b\\) satisfazendo: \\(E[\\theta]\\) \\(=\\dfrac{a}{a+b}\\) \\(=m\\) \\(\\Longleftrightarrow b=\\left(\\dfrac{1-m}{m}\\right)a\\) \\(Var(\\theta)\\) \\(=\\dfrac{ab}{(a+b)^2(a+b+1)}\\) \\(=0.02\\) \\(\\Longleftrightarrow a=\\dfrac{m(m-m^2-v)}{v}\\) Resolvendo o sistema temos, de forma geral, que \\(a=\\dfrac{m(m-m^2-v)}{v}\\) e \\(b=\\dfrac{(1-m)(m-m^2-v)}{v}\\). Assim, no nosso exemplo, teríamos uma \\(Beta(3,6.7)\\). Além disso, já vimos que, nesse caso, a distribuição a posteriori é \\(Beta(3+x,6.7+n-x)\\). Considerando novamente \\(n=5\\) e \\(x=2\\), temos: n=5; x=2 m=0.31; v=0.02 a=m*(m-m^2-v)/v; b=(1-m)*(m-m^2-v)/v p = c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0) p = p/(sum(p)) theta = seq(0,1,0.01) prior = dbeta(theta,a,b)/sum(dbeta(theta,a,b)) vero = dbinom(x,n,theta)/sum(dbinom(x,n,theta)) post = dbeta(theta,a+x,b+n-x)/sum(dbeta(theta,a+x,b+n-x)) priorH = c(rep(p,each=10),0)/sum(c(rep(p,each=10),0)) tibble(theta=rep(theta,4),dens=c(prior,vero,post,priorH), Dist=rep(c(&#39;1.Priori Beta&#39;,&#39;2.Verossimilhança&#39;,&#39;3.Posteriori&#39;,&#39;0.Priori Histograma&#39;),each=101)) %&gt;% ggplot(data=.) + geom_line(aes(x=theta,y=dens,colour=Dist),lwd=1.5) \\(~\\) 3.4.3 Prioris Conjugadas Como visto no exemplo da moeda, em que a distribuição a priori era \\(Beta(a,b)\\), a posteriori era facilmente obtida e também estava na classe das distribuições \\(Beta\\). Em particular, quando observa-se \\(x\\) sucessos em \\(n\\) realizações de ensaios de Bernoulli, a distribuição a posteriori é \\(Beta(a+x,b+n-x)\\). Isso ocorre pois essa distribuição pertence à uma classe bastante espefícica de distribuições a priori, chamadas distribuições conjugadas. \\(~\\) Definição Seja \\(\\mathcal{P}=\\{f(x|\\theta):\\;\\theta \\in \\Theta\\}\\) uma família de distribuições (condicionais) para \\(\\boldsymbol{X}\\) e considere \\(\\mathcal{C}=\\{h(\\theta|a):\\;a\\in A\\}\\) uma família de distribuições para \\(\\theta\\). Dizemos que (a família) \\(\\mathcal{C}\\) é conjugada para \\(\\mathcal{P}\\) se, \\(\\forall \\;h(\\theta)\\in \\mathcal{C},\\) \\(h(\\theta|\\boldsymbol{x})\\propto f(\\boldsymbol x|\\theta)h(\\theta) \\in \\mathcal{C},\\forall \\boldsymbol x \\in \\mathfrak{X}.\\) \\(~\\) Resultado 1. Seja \\(X\\) v.a. tal que, condicional ao conhecimento de \\(\\theta,\\) \\(X|\\theta \\sim Bin(n,\\theta).\\) Considere que, a priori, \\(\\theta \\sim Beta(a,b).\\) Então, \\(\\theta|X=x \\sim Beta(a+x,b+n-x).\\) Portanto, a família \\(\\mathcal{C}=\\{Beta(a_1,a_2):\\;(a_1,a_2)\\in \\mathbb{R}^2_+\\}\\) é conjugada para \\(\\mathcal{P}=\\{Bin(n,\\theta):\\;\\theta \\in [0,1]\\}.\\) \\(~\\) Esse resultado também vale se \\(X_1,\\ldots,X_n\\) são v.a.s condicionalmente independentes e identicamente distribuidas (c.i.i.d.) com \\(X_i|\\theta \\sim Ber(\\theta)\\) \\(X_i|\\theta\\sim Geo(\\theta),\\) \\(i=1,\\ldots,n \\; c.i.i.d.\\) \\(X_i|\\theta \\sim BinNeg(k,\\theta)\\) \\(\\theta\\sim Beta(a,b)\\Rightarrow\\) \\(\\theta|\\boldsymbol X=\\boldsymbol x \\sim Beta(a+s,b+f)\\) em que \\(s\\) é o número de sucessos e \\(f\\) é o número de fracassos. \\(~\\) Resultado 2. (generalização do resultado anterior para o caso em que o número de categorias é maior que 2) Seja \\(\\boldsymbol X | \\boldsymbol \\theta \\sim Multinomial(n,\\boldsymbol \\theta)\\), isto é, sua função de probabilidade é dada por \\[f(\\boldsymbol x| \\boldsymbol \\theta)= \\binom{n}{x_1,x_2,\\ldots,x_k}~\\prod_{i=1}^{k-1}\\theta^i~\\underbrace{\\left(1-\\sum_{i=1}^{k-1}\\theta_i\\right)^{\\displaystyle n-\\sum_{i=1}^{k-1}x_i}}_{\\displaystyle \\theta_k^{~~x_k}}\\] em que \\(\\theta_i\\in [0,1]\\) com \\(\\sum_{i=1}^K\\theta_i=1\\), \\(x_i \\in \\{0,1,\\ldots,n\\}\\) com \\(\\sum_{i=1}^nx_i=n\\) e \\(\\displaystyle \\binom{n}{x_1,x_2,\\ldots,x_k}=\\dfrac{n!}{x_1!x_2!\\ldots x_k!}\\). Considere que, a priori, \\(\\boldsymbol \\theta \\sim Dirichlet(a_1,\\ldots,a_k),\\) \\(a_i &gt; 0, i=1,\\ldots,k\\), isto é, a f.d.p. a priori para \\(\\boldsymbol \\theta\\) é dada por \\[f(\\boldsymbol \\theta) = \\dfrac{\\Gamma(\\sum_{i=1}^K a_i)}{\\Gamma(a_1)\\Gamma(a_2)\\ldots\\Gamma(a_k)}\\prod_{i=1}^{k-1}\\theta_i^{a_i-1}\\bigg(\\underbrace{1-\\sum_{i=1}^{k-1}\\theta_i}_{\\theta_k}\\bigg)^{a_k-1}.\\] Então, a distribuição a posteriori para \\(\\boldsymbol \\theta\\) é \\(\\boldsymbol \\theta|\\boldsymbol X = \\boldsymbol x \\sim Dirichlet (a_1+x_1,\\ldots,a_k+x_k)\\). \\(~\\) Demo: Para verificar o resultado, basta ver que \\(f(\\boldsymbol\\theta|\\boldsymbol x)\\) \\(=\\dfrac{f(\\boldsymbol x| \\boldsymbol \\theta)f(\\boldsymbol \\theta)}{\\int_\\Theta f(\\boldsymbol x| \\boldsymbol \\theta)f(\\boldsymbol \\theta)d\\boldsymbol \\theta}\\) \\(\\propto f(\\boldsymbol x| \\boldsymbol \\theta)f(\\boldsymbol \\theta)\\) \\(\\propto \\prod_{i=1}^{k-1}\\theta_i^{(a_i+x_i-1)}\\left(1-\\sum_{i=1}^{k-1}\\theta_i\\right)^{(a_k+x_k)-1}\\) \\(~\\) Resultado 3. Seja \\(X_1,\\ldots,X_n\\) v.a. c.i.i.d tais que \\(X_i|\\theta \\sim Unif(0,\\theta)\\) e considere que, a priori,\\(\\theta \\sim Pareto(a,b)\\). Então \\(\\theta|\\boldsymbol X = \\boldsymbol x \\sim Pareto\\left(a+n,max\\{b,x_{(n)}\\}\\right)\\). \\(~\\) Demo: \\(f(\\boldsymbol x|\\theta)\\) \\(\\overset{ci}{=}\\prod_{i=1}^nf(x_i|\\theta)\\) \\(\\overset{id}{=}\\prod_{i=1}^n\\dfrac{1}{\\theta}\\mathbb{I}_{[0,\\theta]}(x_i)\\) \\(=\\dfrac{1}{\\theta^n}\\mathbb{I}_{[0,\\theta]}(x_{(n)})\\) \\(=\\dfrac{1}{\\theta^n}\\mathbb{I}_{[x_{(n)},+\\infty)}(\\theta)\\) em que \\(x_{(n)}=max\\{x_1,\\ldots,x_n\\}\\). \\(~\\) \\(f(\\theta)=\\dfrac{ab^a}{\\theta^{a+1}}\\mathbb{I}_{[b,+\\infty]}(\\theta)\\). Então \\(f(\\theta| \\boldsymbol x)\\) \\(\\propto f(\\boldsymbol x|\\theta)f(\\theta)\\) \\(=\\dfrac{1}{\\theta^{a+n+1}}\\mathbb{I}_{[x_{(n)},+\\infty)}(\\theta)\\mathbb{I}_{[b,+\\infty)}(\\theta)\\) \\(=\\dfrac{1}{\\theta^{a+n+1}}\\mathbb{I}_{[max\\{b,x_{(n)}\\},+\\infty)}(\\theta)\\) \\(~\\) \\(\\Rightarrow \\theta|\\boldsymbol X = \\boldsymbol x \\sim Pareto(a+n,max\\{b,x_{(n)}\\})\\). \\(~\\) Resultado 4. Seja \\(X_1,\\ldots,X_n,Y_1,\\ldots,Y_m\\) v.a. condicionalmente independentes tais que \\(X_i|\\theta\\sim Exp(\\theta),i=1,\\ldots,n\\) e \\(Y_j|\\theta \\sim Poisson(\\theta),j=1,\\ldots,m\\). Considere que, a priori, \\(\\theta \\sim Gama(a,b)\\). Então \\(\\theta| \\boldsymbol x,\\boldsymbol y \\sim Gama(a+n+\\sum_jy_j~,~b+m+\\sum_ix_i)\\). Demo: \\(f(\\boldsymbol x, \\boldsymbol y|\\theta)\\overset{ci}{=}f(\\boldsymbol x|\\theta)f(\\boldsymbol y|\\theta)\\overset{ci}{=}\\) \\(\\prod_{i=1}^nf(x_i|\\theta)\\prod_{j=1}^mf(y_i|\\theta)=\\) \\(\\prod_{i=1}^n\\theta e^{-\\theta x_i}\\prod_{j=1}^m\\dfrac{\\theta^{y_j}e^{-\\theta}}{y_j!}=\\) \\(\\dfrac{1}{\\prod_{j=1}^my_j!}\\theta^{n+\\sum_j y_j}e^{-(m+\\sum_ix_i)\\theta}\\) \\(~\\) \\(f(\\theta)=\\dfrac{b^a}{\\Gamma(a)}\\theta^{a-1}e^{-b\\theta}\\) \\(~\\) \\(f(\\theta| \\boldsymbol{x,y})\\propto f(\\boldsymbol x, \\boldsymbol y|\\theta)f(\\theta)\\propto\\) \\(\\theta^{[a+n+\\sum_jy_j]-1}e^{-[b+m+\\sum_ix_i]\\theta}\\) \\(~\\) \\(\\Rightarrow \\theta| \\boldsymbol x,\\boldsymbol y \\sim Gama(a+n+\\sum_jy_j,b+m+\\sum_ix_i)\\) \\(~\\) Resultado 5. Seja \\(~\\mathcal{P}=\\{f(\\boldsymbol x|\\theta):\\; \\theta \\in \\Theta\\}~\\) e \\(~\\mathcal{C}=\\{h(\\theta|a):\\;a\\in A\\}~\\) uma família conjugada para \\(\\mathcal{P}\\). Considere \\(\\mathcal{M}=\\{h(\\theta)=\\sum_{i=1}^mw_ih_i(\\theta):\\) \\(h_i \\in \\mathcal{C} \\; e \\; w_i&gt;0,\\; \\sum_{i=1}^m w_i=1\\}\\). Então \\(\\mathcal{M}\\) é família conjugada para \\(\\mathcal{P}\\). Demo: Como \\(\\mathcal{C}\\) é conjugada para \\(\\mathcal{P}\\), para toda função \\(h_i \\in \\mathcal{C}\\), temos que \\(f_i(\\theta|\\boldsymbol x)\\propto h_i(\\theta)f(\\boldsymbol x|\\theta)\\in \\mathcal{C}\\). Então \\(~\\) \\(h\\in \\mathcal{M}\\) \\(~\\Rightarrow~ f(\\theta|\\boldsymbol x)\\) \\(~\\propto~ h(\\theta)f(\\boldsymbol x|\\theta)\\) \\(~\\propto~\\sum_{i=1}^m w_i\\underbrace{h_i(\\theta)f(\\boldsymbol x|\\theta)}_{\\in \\mathcal{C}}\\) \\(~\\propto~\\sum_{i=1}^m w_i^*f_i(\\theta|\\boldsymbol x)\\in \\mathcal{M}\\). \\(~\\) Exemplo. Seja \\(X|\\theta \\sim Bin(n,\\theta)\\) e \\(f(\\theta)\\) \\(=wf_1(\\theta)+(1-w)f_2(\\theta)\\), com \\(f_1\\sim Beta(a_1,b_1)\\) e \\(f_2\\sim Beta(a_2,b_2)\\). \\(~\\) \\(f(\\theta|x)\\) \\(=\\dfrac{f(x|\\theta)f(\\theta)}{\\int_0^1f(x|\\theta)f(\\theta)}\\) \\(=\\dfrac{f(x|\\theta)[wf_1(\\theta)+(1-w)f_2(\\theta)]}{w\\int_0^1f_1(\\theta)f(x|\\theta)d\\theta+(1-w)\\int_0^1f_2(\\theta)f(x|\\theta)d\\theta}\\) \\(\\propto\\dfrac{w\\binom{n}{x}\\frac{\\Gamma(a_1+b_1)}{\\Gamma(a_1)\\Gamma(b_1)}\\theta^{a_1+x-1}(1-\\theta)^{b_1+n-x-1}+(1-w)\\binom{n}{x}\\frac{\\Gamma(a_2+b_2)}{\\Gamma(a_2)\\Gamma(b_2)}\\theta^{a_2+x-1}(1-\\theta)^{b_2+n-x-1}}{\\underbrace{w\\binom{n}{x}\\frac{\\Gamma(a_1+b_1)}{\\Gamma(a_1)\\Gamma(b_1)}\\frac{\\Gamma(a_1+x)\\Gamma(b_1+n-x)}{\\Gamma(a_1+b_1+n)}}_{A}+\\underbrace{(1-w)\\binom{n}{x}\\frac{\\Gamma(a_2+b_2)}{\\Gamma(a_2)\\Gamma(b_2)}\\frac{\\Gamma(a_2+x)\\Gamma(b_2+n-x)}{\\Gamma(a_2+b_2+n)}}_{B}}\\) \\(\\propto~\\underbrace{\\dfrac{A}{A+B}}_{w^*}Beta(a_1+x,b_1+n-x)+\\underbrace{\\dfrac{B}{A+B}}_{1-w^*}Beta(a_2+x,b_2+n-x)\\). \\(~\\) Primeiramente, suponha que \\(n=5\\), e temos uma mistura das distribuições \\(Beta(5,12)\\) e \\(Beta(10,3)\\), com \\(w=0.5\\). O gráfico a seguir apresenta as distribuições a priori, a verossimilhança e a posteriori para cada possível valor de \\(x\\) em \\(\\left\\{0,1,\\ldots,5\\right\\}\\). a1=5; b1=12 a2=10; b2=3 n=5 w=0.5 theta = seq(0,1,0.01) A = as.vector(apply(matrix(seq(0,n)),1, function(x){w*choose(n,x)*gamma(a1+b1)/(gamma(a1)*gamma(b1))* (gamma(a1+x)*gamma(b1+n-x))/gamma(a1+b1+n)})) B = as.vector(apply(matrix(seq(0,n)),1, function(x){(1-w)*choose(n,x)*gamma(a2+b2)/(gamma(a2)*gamma(b2))* (gamma(a2+x)*gamma(b2+n-x))/gamma(a2+b2+n)})) w2 = A/(A+B) prior2 = as.vector(apply(matrix(seq(0,n)),1, function(x){w*dbeta(theta,a1,b1)+ (1-w)*dbeta(theta,a2,b2)})) post2 = as.vector(as.matrix(mapply(function(x,w2){ w2*dbeta(theta,a1+x,b1+n-x)+ (1-w2)*dbeta(theta,a2+x,b2+n-x)},seq(0,n),w2))) #vero = as.vector(apply(matrix(seq(0,n)),1, # function(x){dbinom(x,prob=theta,size=n)})) # Verossimilhança proporcional visualmente melhor vero = as.vector(apply(matrix(seq(0,n)),1, function(x){dbeta(theta,x+1,n-x+1)})) tibble(x=as.factor(rep(seq(0,n),each=length(theta))), w2=rep(w2,each=length(theta)), theta=rep(theta,(n+1)),vero=vero,prior=prior2,post=post2) %&gt;% ggplot() + geom_line(aes(x=theta,y=post, colour=x),lwd=1.5) + geom_line(aes(x=theta,y=prior,colour=&quot;Prior&quot;),lwd=1,lty=2) + geom_line(aes(x=theta,y=vero,colour=&quot;Verossimilhança&quot;),lwd=1,lty=2)+ xlab(expression(theta)) + ylab(expression(paste(&quot;f(&quot;,theta,&quot;|x)&quot;)))+ theme_bw() + gganimate::transition_states(x) \\(~\\) Agora, suponha que \\(n=5\\) e foi observado \\(x=2\\). Novamente, considere a mistura das distribuições \\(Beta(5,12)\\) e \\(Beta(10,3)\\) mas agora com pesos \\(w\\) variando no conjunto \\(\\left\\{0,0.1,\\ldots,0.9,1\\right\\}\\). n=5; x=2 w = seq(0,1,0.1) A = as.vector(apply(matrix(w),1, function(w){w*choose(n,x)*gamma(a1+b1)/(gamma(a1)* gamma(b1))*(gamma(a1+x)*gamma(b1+n-x))/gamma(a1+b1+n)})) B = as.vector(apply(matrix(w),1, function(w){(1-w)*choose(n,x)*gamma(a2+b2)/(gamma(a2)* gamma(b2))*(gamma(a2+x)*gamma(b2+n-x))/gamma(a2+b2+n)})) w2 = A/(A+B) prior2 = as.vector(apply(matrix(w),1,function(w){ w*dbeta(theta,a1,b1)+(1-w)*dbeta(theta,a2,b2)})) post2 = as.vector(as.matrix(mapply(function(w,w2){ w2*dbeta(theta,a1+x,b1+n-x)+ (1-w2)*dbeta(theta,a2+x,b2+n-x)},w,w2))) vero = as.vector(apply(matrix(rep(x,2*n+1)),1, function(x){dbeta(theta,x+1,n-x+1)})) z&lt;-length(w) tibble(w=as.factor(rep(w,each=length(theta))), w2=rep(w2,each=length(theta)), theta=rep(theta,z), prior = prior2, post = post2, vero = vero) %&gt;% ggplot(colour = w) + geom_line(aes(x=theta,y=post, colour=w),lwd=1.5) + geom_line(aes(x=theta,y=prior,colour=&quot;Priori&quot;)) + geom_line(aes(x=theta,y=vero,colour=&quot;Verossimilhança&quot;),lwd=1,lty=2)+ xlab(expression(theta)) + ylab(expression(paste(&quot;f(&quot;,theta,&quot;|x)&quot;)))+ theme_bw() + gganimate::transition_states(w) \\(~\\) \\(~\\) 3.4.4 Prioris Não-Informativas Priors não-informativas são tentativas de representar formalmente um estado de ignorância. Contudo, não existe uma forma única de representar ignorância, tampouco uma priori objetiva. Além disso, é bastante raro um cenário onde não há nenhuma informação a priori. De qualquer modo, serão apresentadas aqui algumas formas de representar falta de informação mas a escolha da priori será sempre subjetiva. \\(~\\) 3.4.4.1 Priori de Bayes-Laplace Princípio da Razão Insuficiente. Quando não existe razão suficiente para acreditar mais em algum subconjunto do espaço paramétrico \\(\\Theta\\), deve-se adotar equiprobabilidade. \\(~\\) Exemplo 1. Se \\(\\Theta=\\left\\{\\theta_1,\\theta_2,\\ldots,\\theta_k\\right\\}\\) então a priori de Bayes-Laplace é \\(f(\\theta)=1/k\\), \\(\\theta \\in \\Theta~.\\) \\(~\\) Exemplo 2. Se \\(\\Theta=\\left[a,b\\right]\\) então a priori de Bayes-Laplace é \\(f(\\theta)=1/(b-a)\\), \\(\\theta \\in \\Theta~.\\) \\(~\\) \\(f(\\theta|\\boldsymbol{x})\\) \\(= \\dfrac{f(\\theta)f(\\boldsymbol{x}|\\theta)}{\\int_\\Theta f(\\theta)f(\\boldsymbol{x}|\\theta)~d\\theta}\\) \\(= \\dfrac{c~f(\\boldsymbol{x}|\\theta)}{c~\\int_\\Theta f(\\boldsymbol{x}|\\theta)~d\\theta}\\) \\(= \\dfrac{f(\\boldsymbol{x}|\\theta)}{\\int_\\Theta f(\\boldsymbol{x}|\\theta)~d\\theta}\\) \\(\\propto f(\\boldsymbol{x}|\\theta)~.\\) \\(~\\) As principais críticas da priori de Bayes-Laplace são A distribuição é imprópria quando o espaço paramétrico \\(\\Theta\\) não é finito ou limitado. Por exemplo, \\(\\Theta=\\mathbb{N}\\), \\(\\Theta=\\mathbb{Z}\\) ou \\(\\Theta=\\mathbb{R}\\). Nesses casos, a priori de Bayes-Laplace é \\(f(\\theta)\\propto \\mathbb{I}_\\Theta(\\theta)\\), que não é uma distribuição de probabilidade. Não é invariante a reparametrizações. Considere, por exemplo, \\(f(\\theta)\\) uma f.d.p. a priori para \\(\\theta\\) e \\(g\\) uma transformação um-a-um (injetora) de \\(\\theta\\) tal que \\(\\psi=g(\\theta)\\). A distribuição de \\(\\psi\\) pode ser calculada por \\(f_\\psi(\\psi) = f\\left(g^{-1}(\\psi)\\right)\\left|\\dfrac{dg^{-1}(\\psi)}{d\\psi}\\right|~.\\) Assim, se \\(g\\) é uma transformação não linear e a distribuição a priori para \\(\\theta\\) é uniforme, a distribuição para \\(\\psi\\) não é uniforme, em geral. \\(~\\) 3.4.4.2 Priori de Jeffreys Seja \\(g\\) uma transformação um-a-um do parâmetro \\(\\theta\\) e defina \\(\\psi=g(\\theta)\\). Considere uma função \\(h:\\mathfrak{X}\\times\\Theta\\longrightarrow\\mathbb{R}\\). Uma classe de distribuições a priori invariantes pode ser definida por \\[f(\\theta) \\propto \\left(\\text{Var}_{X|\\theta}\\left[\\dfrac{\\partial h(\\boldsymbol X | \\theta)}{\\partial\\theta}~\\bigg|~\\theta\\right]\\right)^{1/2}~.\\] Demo. Para mostrar a invariância do método, considere o caso contínuo em que \\[f_\\psi(\\psi) = f\\left(g^{-1}(\\psi)\\right)\\left|\\dfrac{\\partial g^{-1}(\\psi)}{\\partial\\psi}\\right|~.\\] Seja \\(h^*(x,\\psi)=h\\left(x,g^{-1}(\\psi)\\right)\\). Então \\(\\dfrac{\\partial h^*(x,\\psi)}{\\partial\\psi}\\) \\(=\\dfrac{\\partial h\\left(x,g^{-1}(\\psi)\\right)}{\\partial\\psi}\\) \\(=\\left.\\dfrac{\\partial h(x,\\theta)}{\\partial\\theta}\\right|_{\\theta=g^{-1}(\\psi)}\\cdot\\dfrac{\\partial g^{-1}(\\psi)}{\\partial\\psi}~,\\) e, portanto, \\(\\text{Var}\\left[\\dfrac{\\partial h^*(\\boldsymbol{X},\\psi)}{\\partial\\psi}~\\bigg|~\\theta=g^{-1}(\\psi)\\right]\\) \\(=\\text{Var}\\left[\\dfrac{\\partial h(\\boldsymbol{X},\\theta)}{\\partial\\theta}~\\bigg|~\\theta=g^{-1}(\\psi)\\right]\\cdot\\left[\\dfrac{\\partial g^{-1}(\\psi)}{\\partial\\psi}\\right]^2\\) \\(=\\left[f\\left(g^{-1}(\\psi)\\right)\\left(\\dfrac{\\partial g^{-1}(\\psi)}{\\partial\\psi}\\right)\\right]^2~,\\) de modo que \\(f_\\psi(\\psi)\\) \\(=f\\left(g^{-1}(\\psi)\\right)\\left|\\dfrac{\\partial g^{-1}(\\psi)}{\\partial\\psi}\\right|\\) \\(=\\text{Var}\\left[\\dfrac{\\partial h^*(\\boldsymbol{X},\\psi)}{\\partial\\psi}~\\bigg|~\\theta=g^{-1}(\\psi)\\right]^{1/2}~.\\) \\(~\\) A escolha mais usual para \\(h\\) é \\(h(\\boldsymbol{x},\\theta)=\\log f(\\boldsymbol{x}|\\theta)~.\\) Assim, como \\(E\\left[\\dfrac{\\partial \\log f(\\boldsymbol{X}|\\theta)}{\\partial\\theta}~\\bigg|~\\theta\\right]=0\\), temos \\(f(\\theta)\\) \\(\\propto\\text{Var}\\left[\\dfrac{\\partial \\log f(\\boldsymbol{X}|\\theta)}{\\partial\\theta}~\\bigg|~\\theta\\right]^{1/2}\\) \\(=\\text{E}\\left[\\left(\\dfrac{\\partial \\log f(\\boldsymbol{X}|\\theta)}{\\partial\\theta}\\right)^2~\\bigg|~\\theta\\right]^{1/2}\\) \\(=\\left[\\mathcal{I}(\\theta)\\right]^{1/2}~,\\) onde \\(\\mathcal{I}(\\theta)\\) é a Informação de Fisher de \\(\\theta\\). Neste caso, \\(f(\\theta)\\propto\\big|\\mathcal{I}(\\theta)\\big|^{1/2}\\) é chamada priori de Jeffreys. Uma motivação para o método de Jeffreys é que a informação de Fisher \\(\\mathcal{I}(\\theta)\\) é um indicador da quantidade de informação trazida pelo modelo (observações) sobre o parâmetro \\(\\theta\\). Favorecer os valores de \\(\\theta\\) para o qual \\(\\mathcal{I}(\\theta)\\) é grande supostamente minimiza a influência da priori. \\(~\\) Exemplo 1. Considere novamente o experimento de lançar uma moeda \\(n\\) vezes e contar o número de caras, isto é, \\(X|\\theta \\sim \\text{Bin}(n,\\theta)\\). Então, \\(f(x|\\theta)=\\displaystyle\\binom{n}{x}\\theta^x(1-\\theta)^{n-x}\\) \\(\\Longrightarrow~ \\log f(x|\\theta)=\\log\\binom{n}{x}+x\\log\\theta+(n-x)\\log(1-\\theta)\\) \\(~\\) \\(\\dfrac{\\partial\\log f(x|\\theta)}{\\partial\\theta}\\) \\(=\\dfrac{x}{\\theta}-\\dfrac{n-x}{1-\\theta}\\) \\(=\\dfrac{x-n\\theta}{\\theta(1-\\theta)}~.\\) \\(~\\) Como \\(E\\left[X|\\theta\\right]=n\\theta\\) e \\(Var(X|\\theta)\\) \\(=E\\left[\\left(X-E\\left[X|\\theta\\right]\\right)^2~\\Big|~\\theta\\right]\\) \\(=E\\left[\\left(X-n\\theta\\right)^2~\\Big|~\\theta\\right]\\) \\(=n\\theta(1-\\theta)\\), a informação de Fisher neste caso é \\(\\mathcal{I}_x(\\theta)\\) \\(=\\text{E}\\left[\\left(\\dfrac{\\partial\\log f(x|\\theta)}{\\partial\\theta}\\right)^2~\\bigg|~\\theta\\right]\\) \\(=\\text{E}\\left[\\left(\\dfrac{X-n\\theta}{\\theta(1-\\theta)}\\right)^2~\\bigg|~\\theta\\right]\\) \\(=\\dfrac{1}{\\theta^2(1-\\theta)^2}~\\text{E}\\left[\\left(X-n\\theta\\right)^2~|~\\theta\\right]\\) \\(=\\dfrac{1}{\\theta^2(1-\\theta)^2}~\\text{Var}\\left(X~|~\\theta\\right)\\) \\(=\\dfrac{n~\\theta(1-\\theta)}{\\theta^2(1-\\theta)^2}\\) \\(=\\dfrac{n}{\\theta(1-\\theta)}\\) \\(=n\\theta^{-1}(1-\\theta)^{-1}~,\\) \\(~\\) de modo que a priori de Jeffreys é \\(f(\\theta)\\) \\(\\propto\\left[\\mathcal{I}_x(\\theta)\\right]^{1/2}\\) \\(\\propto\\theta^{-1/2}(1-\\theta)^{-1/2}\\) \\(~\\Longrightarrow~ \\theta \\sim \\text{Beta}\\left(\\frac{1}{2},\\frac{1}{2}\\right)~.\\) \\(~\\) Exemplo 2. Considere agora que a mesma moeda é lançada e anota-se o número de caras \\(Y\\) até que sejam observadas \\(r\\) coroas, isto é, \\(Y|\\theta \\sim \\text{BinNeg}(r,\\theta)\\). Então, \\(f(y|\\theta)=\\displaystyle\\binom{y+r-1}{y}\\theta^y(1-\\theta)^{r}\\) \\(\\Longrightarrow~ \\log f(y|\\theta)=\\log\\binom{y+k-1}{y}+y\\log\\theta+r\\log(1-\\theta)\\) \\(~\\) \\(\\dfrac{\\partial\\log f(y|\\theta)}{\\partial\\theta}\\) \\(=\\dfrac{y}{\\theta}-\\dfrac{r}{1-\\theta}\\) \\(=\\dfrac{1}{\\theta}\\left[y-\\dfrac{r~\\theta}{1-\\theta}\\right]~.\\) \\(~\\) Como \\(E\\left[X|\\theta\\right]=\\dfrac{r~\\theta}{1-\\theta}\\) e \\(Var(X|\\theta)=\\dfrac{r~\\theta}{(1-\\theta)^2}\\), a informação de Fisher neste caso é \\(\\mathcal{I}_y(\\theta)\\) \\(=\\text{E}\\left[\\dfrac{1}{\\theta^2}\\left(y-\\dfrac{r~\\theta}{1-\\theta}\\right)^2~\\bigg|~\\theta\\right]\\) \\(=\\dfrac{1}{\\theta^2}~\\text{Var}\\left(Y~|~\\theta\\right)\\) \\(=\\dfrac{r}{\\theta(1-\\theta)^2}\\) \\(=r\\theta^{-1}(1-\\theta)^{-2}~,\\) \\(~\\) de modo que a priori de Jeffreys é \\(f(\\theta)\\) \\(\\propto\\left[\\mathcal{I}_y(\\theta)\\right]^{1/2}\\) \\(\\propto\\theta^{-1/2}(1-\\theta)^{-1}~.\\) \\(~\\) Note que nos exemplos apresentados, a priori depende da regra de parada, isto é, a forma como decidimos quando parar de lançar a moeda e que determina se o modelo estatístico é binomial ou binomial negativo. Em outras palavras, a opinião a priori definida dessa forma depende do modelo adotado, mesmo que o parâmetro seja o mesmo nos dois casos. Além disso, a priori de Jeffreys pode ser imprópria, como ocorre no exemplo anterior. \\(~\\) 3.4.4.3 Priori de Máxima Entropia Entropia é um conceito físico que quantifica a desordem ou imprevisibilidade de um sistema, ou da falta de informação sobre ele. O conceito de entropia desempenha um importante papel na teoria da informação. O princípio da máxima entropia afirma que a distribuição de probabilidade que melhor representa a falta de informação é aquela com a maior entropia. Caso Discreto. Considere um espaço paramétrico enumerável \\(\\Theta = \\{\\theta_1,\\theta_2,\\ldots\\}\\). A entropia da distribuição \\(h\\) (Shannon 1948) é dada por \\[\\mathcal{E}(h)=\\text{E}[-\\log h(\\theta)]=\\displaystyle-\\sum_{\\theta\\in\\Theta} \\log\\left[h(\\theta)\\right]~h(\\theta)~.\\] \\(~\\) Definição. Considere um espaço paramétrico \\(\\Theta\\) e \\(h\\) uma f.d.p. para \\(\\theta\\). A distribuição da máxima entropia para \\(\\theta\\) é a função \\(h\\) que maximiza \\(\\mathcal{E}(h)\\) (Jaynes 2003) \\(~\\) Exemplo 1. Considere o espaço paramétrico \\(\\Theta=\\{\\theta_1,\\ldots,\\theta_k\\}\\) e \\(h(\\theta_i)=p_i\\) uma distribuição discreta para \\(\\theta\\). A distribuição da máxima entropia para \\(\\theta\\) é a função \\(h\\) que maximiza \\(\\mathcal{E}(h)=-\\displaystyle\\sum_{i=1}^{k} p_i\\log(p_i)\\) com a restrição \\(\\displaystyle\\sum_{i=1}^k h(\\theta_i)=\\sum_{i=1}^k p_i=1~.\\) Utilizando o método de multiplicadores de Lagrange, deve-se maximizar a função lagrangiana \\(\\mathcal{E}^*(h)=\\displaystyle-\\sum_{i=1}^k p_i\\log(p_i)+\\lambda\\left(\\sum_{i=1}^k p_i-1\\right)\\) \\(\\dfrac{\\partial\\mathcal{E}^*(h)}{\\partial p_i}=-\\left[p_i~\\dfrac{1}{p_i}+\\log(p_i)\\right]+\\lambda=0\\) \\(\\Longleftrightarrow p_i = e^{\\lambda-1}~~,~~~i=1,\\ldots,k~.\\) Assim, como \\(p_i\\) deve ser constante e \\(\\sum p_i=1\\), conclui-se que \\(p_i=1/k\\), para \\(i=1,\\ldots,k~.\\) \\(~\\) Exemplo 2. Considere agora \\(\\Theta = \\{\\theta_1,\\theta_2,\\ldots\\}\\) e suponha que há \\(m\\) informações parciais a respeito do parâmetro \\(\\theta\\) que podem ser escritas como \\(\\text{E}[g_j(\\theta)]=\\mu_j~,~\\) \\(j=1,\\ldots,m~.\\) Usando novamente o método de Lagrange, deve-se maximizar \\(\\mathcal{E}^*(h)\\) \\(=\\displaystyle\\sum_{i=1}^\\infty p_i\\log(p_i)+\\lambda\\left(\\sum_{i=1}^\\infty p_i-1\\right)+ \\sum_{j=1}^m\\lambda_j\\left(\\sum_{i=1}^\\infty p_i~g_j(\\theta_i)-\\mu_j\\right)\\) \\(\\dfrac{\\partial \\mathcal{E}^*(h)}{\\partial p_i}=\\displaystyle-\\log(p_i)-1+\\lambda+\\sum_{j=1}^m\\lambda_j~g_j(\\theta_i)=0\\) \\(\\Longleftrightarrow p_i \\propto e^{\\lambda-1+\\sum_{j=1}^m \\lambda_j~g_j(\\theta_i)}\\) \\(\\propto e^{\\sum_{j=1}^m \\lambda_j~g_j(\\theta_i)}~~,~~~i=1,\\ldots,k~.\\) Como \\(\\sum p_i=1\\), \\(p_i = \\dfrac{e^{\\sum_{j=1}^m \\lambda_j~g_j(\\theta_i)}}{\\sum_{i=1}^\\infty e^{\\sum_{j=1}^m \\lambda_j~g_j(\\theta_i)}}~\\) e \\(\\lambda_j\\) é obtido por meio das restrições. \\(~\\) Exemplo 2a. Seja \\(\\Theta = \\{0,1,2,\\ldots\\}\\) e suponha que \\(\\text{E}[\\theta]=\\mu.\\) Usando o resultado do exemplo anterior com \\(g(\\theta)=\\theta\\) e \\(\\theta_i=i\\), \\(i=0,1,2,\\ldots~,\\) \\(p_i=\\dfrac{e^{\\sum_{j=1}^m \\lambda_j~g_j(\\theta_i)}}{\\sum_{i=0}^\\infty e^{\\sum_{j=1}^m \\lambda_j~g_j(\\theta_i)}}\\) \\(=\\dfrac{e^{\\lambda~i}}{\\sum_{i=0}^\\infty e^{\\lambda~i}}~\\) \\(\\overset{\\left|e^\\lambda\\right|&lt;1}{=}~\\dfrac{e^{\\lambda~i}}{1/\\left(1-e^\\lambda\\right)}\\) \\(=\\left(e^\\lambda\\right)^i\\left(1-e^\\lambda\\right)\\) \\(\\Longrightarrow \\theta \\sim \\text{Geo}\\left(1-e^\\lambda\\right)~.\\) Como \\(\\text{E}\\left[\\theta\\right]=\\dfrac{e^\\lambda}{\\left(1-e^\\lambda\\right)}=\\mu\\), tem-se que \\(\\lambda=\\log\\dfrac{\\mu}{1+\\mu}~.\\) \\(~\\) Exemplo 2b. Considere que \\(\\Theta = \\{1,2,\\ldots,k\\}\\) e suponha que \\(\\text{Med}(\\theta)=m~.\\) Nesse caso, \\(g(\\theta)=\\mathbb{I}\\left(\\theta\\leq m\\right)\\) e \\(\\theta_i=i\\), \\(i=1,2,\\ldots,k~,\\) de modo que \\(\\text{E}\\left[g(\\theta)\\right]\\) \\(=\\text{E}\\left[\\mathbb{I}(\\theta\\leq m)\\right]\\) \\(=\\text{P}\\left(\\theta\\leq m\\right)=1/2\\) e, portanto, \\(\\displaystyle\\sum_{i\\leq m}p_i=\\sum_{j&gt; m}p_j=1/2~.\\) \\(p_i=\\dfrac{e^{\\sum_{j=1}^m \\lambda_j~g_j(\\theta_i)}}{\\sum_{i=1}^k e^{\\sum_{j=1}^m \\lambda_j~g_j(\\theta_i)}}\\) \\(=\\left\\{\\begin{array}{lll} \\dfrac{e^\\lambda}{\\sum_{i\\leq m} e^\\lambda}&amp;,&amp; i\\leq m \\\\ \\dfrac{1}{\\sum_{i\\leq m} 1}&amp;,&amp; i&gt; m\\end{array}\\right.\\) \\(=\\left\\{\\begin{array}{lll} \\dfrac{1}{2m}&amp;,&amp; i\\leq m \\\\ \\dfrac{1}{2(k-m)}&amp;,&amp; i&gt; m\\end{array}\\right.\\) (A distribuição de \\(\\theta\\) é uniforme por blocos.) \\(~\\) \\(~\\) Divergência de Kullbach-Leibler. Considere duas distribuições discretas \\(\\boldsymbol{p} = (p_1,\\ldots,p_k)\\) e \\(\\boldsymbol{q} = (q_1,\\ldots,q_k)~,\\) tal que \\(p_i,q_i&gt;0~,\\) \\(i=1,\\ldots,k~,\\) e \\(\\sum p_i=\\sum q_i=1\\). A divergência de Kullbach-Leibler entre \\(\\boldsymbol{p}\\) e \\(\\boldsymbol{q}\\) (Kullback and Leibler 1951) é dada por \\[D(\\boldsymbol{p}~||~\\boldsymbol{q})=\\sum p_i\\log\\left(\\dfrac{p_i}{q_i}\\right)~.\\] \\(~\\) Suponha que \\(g=(1/k,\\ldots,1/k)\\) \\(D(\\boldsymbol{p}~||~\\boldsymbol{q})\\) \\(=\\displaystyle\\sum_{i=1}^{k} p_i\\log\\left(\\dfrac{p_i}{1/k}\\right)\\) \\(=\\displaystyle\\sum_{i=1}^{k}p_i\\left[ln(p_i)-ln(1/k)\\right]\\) \\(=\\displaystyle\\sum_{i=1}^kp_i ln(pi)+ln(k)\\sum_{i=1}^k p_i\\) \\(=ln(k)-\\mathcal{E}(\\boldsymbol p)\\) Assim, exceto por uma constante, \\(\\mathcal{E}(\\boldsymbol p)\\) está associado com quanto a distribuição \\(\\boldsymbol p\\) diverge da distribuição uniforme (priori de referência na ausência total de informação). \\(~\\) Observação: No caso geral, se \\(H\\) e \\(H_0\\) são duas medidas definidas em \\(\\Theta\\) tais que \\(H\\) é absolutamente contínua com relação à \\(H_0\\) \\((H\\ll H_0)\\), a divergência de Kullbach-Leibler é definida como \\[D(H~||~H_0)=\\displaystyle\\int_\\Theta \\log\\left(\\dfrac{dH}{dH_0}\\right)dH~,\\] em que \\(\\dfrac{dH}{dH_0}\\) é derivada de Radon-Nikodym. Se \\(H\\) e \\(H_0\\) são medidas de probabilidade absolutamente contínuas com relação a medida de Lebesgue \\(\\lambda\\) com f.d.p. \\(\\dfrac{dH}{d\\lambda}=h\\) e \\(\\dfrac{dH_0}{d\\lambda}=h_0\\), temos que, \\(D(H~||~H_0)\\) \\(=\\displaystyle\\int_\\Theta \\log\\left(\\dfrac{dH/d\\lambda}{dH_0/d\\lambda}\\right)\\dfrac{dH}{d\\lambda}d\\lambda\\) \\(=\\displaystyle\\int_\\Theta \\log\\left(\\dfrac{h(\\theta)}{h_0(\\theta)}\\right)h(\\theta)~d\\theta\\) \\(~\\) \\(~\\) Como a definição anterior de entropia vale apenas para o caso discreto, Jaynes (2003) sugere que no caso contínuo seja utilizada a entropia relativa, dada por \\[\\mathcal{E}(h)=-\\displaystyle\\int_\\Theta h(\\theta)\\log\\left(\\dfrac{h(\\theta)}{h_0(\\theta)}\\right)d\\theta=-D(h~||~h_0)~,\\] onde \\(h_0\\) é uma priori de referência na ausência total de informação, preferivelmente invariante. \\(~\\) Assim como no caso discreto, se temos \\(m\\) restrições \\(E[g_i(\\theta)]=\\mu_i,\\) a densidade de máxima entropia é \\(h(\\theta)\\propto h_0(\\theta)\\exp\\left\\{\\displaystyle\\sum_{j=1}^m\\lambda_j~ g_j(\\theta)\\right\\}\\) e os \\(\\lambda_j~,\\) \\(j=1,\\ldots,m~,\\) são obtidos das restrições. Por exemplo, se \\(g(\\theta)=\\theta\\) com \\(E[\\theta]=\\mu\\), basta fazer \\(\\mu = \\displaystyle \\int_\\Theta \\theta~c~ h_0(\\theta)\\exp\\{\\lambda\\theta\\}~d\\theta\\) com \\(c^{-1}=\\displaystyle\\int_\\Theta h_0(\\theta)exp\\{\\lambda \\theta\\}d\\theta\\). \\(~\\) Exemplo 1: \\(\\Theta = \\mathbb{R}_+\\) e \\(\\text{E}[\\theta]=\\mu~.\\) Tomando \\(h_0(\\theta) \\propto \\mathbb{I}_{\\mathbb{R}_+}(\\theta)\\) (f.d.p. imprópria), tem-se \\(h(\\theta )\\propto e^{\\lambda\\theta}~\\mathbb{I}_{\\mathbb{R}_+}(\\theta)\\) \\(\\propto-\\lambda e^{\\lambda\\theta}~\\mathbb{I}_{\\mathbb{R}_+}(\\theta)~\\mathbb{I}_{\\mathbb{R}_-}(\\lambda)~.\\) Como \\(\\text{E}[\\theta]=-1/\\lambda =\\mu~,\\) tem-se que \\(\\lambda= -1/\\mu\\), isto é, \\(\\theta\\sim\\text{Exp}(1/\\mu)~,\\) de modo que \\(h(\\theta)=\\dfrac{1}{\\mu}e^{-\\frac{\\theta}{\\mu}}~,\\) \\(\\mu&gt;0~.\\) \\(~\\) Exemplo 2 \\(\\Theta = \\mathbb{R}\\) e \\(\\text{E}[\\theta]=\\mu\\) e \\(\\text{Var}(\\theta)=\\text{E}[(\\theta-\\mu)^2]=\\sigma^2~.\\) Tomando \\(g_1(\\theta)=\\theta\\) e \\(g_2(\\theta)=(\\theta-\\mu)^2\\), tem-se pelo resultado anterior que \\(h(\\theta) \\propto \\exp\\left\\{\\lambda_1\\theta+\\lambda_2(\\theta-\\mu)^2\\right\\}\\) \\(\\propto \\exp\\left\\{\\lambda_1\\theta+\\lambda_2(\\theta^2-2\\theta\\mu+\\mu^2)\\right\\}\\) \\(\\propto \\exp\\left\\{\\lambda_2\\left[\\theta^2-\\left(2\\mu-\\dfrac{\\lambda_1}{\\lambda_2}\\right)\\theta\\right]\\right\\}\\) \\(\\propto \\exp\\left\\{\\lambda_2\\left[\\theta-\\left(\\mu-\\dfrac{\\lambda_1}{2\\lambda_2}\\right)\\right]^2\\right\\}~.\\) Considere que \\(\\theta\\sim N(\\mu,\\sigma^2)\\), isto é, \\(f(\\theta)=\\dfrac{1}{\\sqrt{2\\pi}~\\sigma}~\\exp\\left\\{-\\dfrac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}\\) \\(\\propto \\exp\\left\\{-\\dfrac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}~.\\) Assim, para concluir que a distribuição de máxima entropia nesse caso é a Normal anterior, basta tomar \\(\\mu-\\dfrac{\\lambda_1}{2\\lambda_2}=\\mu\\) para ver que \\(\\lambda_1=0\\) e \\(\\lambda_2=-\\dfrac{1}{2\\sigma^2}~.\\) \\(~\\) \\(~\\) \\(~\\) 3.5 Alguns Princípios de Inferência Considere um experimento \\(E=(\\boldsymbol{X},\\theta,\\{f(\\boldsymbol x|\\theta)\\})\\) que consiste em observar um particular valor \\(\\boldsymbol{x}\\in\\mathfrak{X}\\) do v.a. \\(\\boldsymbol{X}\\) que, para cada possível valor do parâmetro (desconhecido) \\(\\theta\\in\\Theta\\), tem f.d.p. \\(f(\\boldsymbol x|\\theta)\\). De forma geral, uma inferência sobre \\(\\theta\\) baseada no resultado \\(\\boldsymbol x\\) do experimento \\(E\\) será denotada por \\(\\text{Inf}(E,\\boldsymbol x)~.\\) Princípio de Suficiência. Considere um experimento \\(E=(\\boldsymbol{X},\\theta,\\{f(\\boldsymbol{x}|\\theta)\\})\\) e suponha que \\(T(\\boldsymbol{X})\\) é uma estatística suficiente para \\(\\theta\\). Se \\(\\boldsymbol x_1\\) e \\(\\boldsymbol x_2\\) são dois pontos amostrais tais que \\(T(\\boldsymbol{x}_1)=T(\\boldsymbol{x}_2)\\) então \\(\\text{Inf}(E,\\boldsymbol{x}_1)=\\text{Inf}(E,\\boldsymbol{x}_2)~.\\) \\(~\\) Exemplo 1a. Seja \\(X_1,\\ldots,X_{n}\\) c.i.i.d. tais que \\(X_1\\sim Ber(\\theta)~.\\) Considere \\(n=10\\) e os pontos amostrais \\(\\boldsymbol x_1=(1,1,1,1,1,1,0,0,0,0)\\) e \\(\\boldsymbol x_2=(1,0,1,0,1,0,1,0,1,1)\\) tais que \\(T(\\boldsymbol x_1)=\\sum x_{1i}=6\\) e \\(T(\\boldsymbol x_2)=\\sum x_{2i}=6\\). Um possível estimador para \\(\\theta\\) nesse exemplo é a média amostral, de modo que \\(\\bar{x}_1=\\bar{x}_2=\\dfrac{\\sum x_i}{n}=0,6~.\\) \\(~\\) Exemplo 1b. Ainda no contexto do exemplo anterior, considere que a priori \\(\\theta \\sim \\text{Beta}(a,b)~.\\) Então, se \\(T(\\boldsymbol x_1)=T(\\boldsymbol x_2)=t\\), \\[\\theta|\\boldsymbol x_1\\sim\\theta|\\boldsymbol x_2\\sim\\theta|T(\\boldsymbol x_1)=t~\\sim~ Beta(a+t,b+n-t)~.\\] \\(~\\) Princípio da Condicionalidade. Suponha que \\(E_1=\\left(\\boldsymbol X_1,\\theta,\\{f(\\boldsymbol x_1|\\theta)\\}\\right)\\) e \\(E_2=\\left(\\boldsymbol X_2,\\theta,\\{f(\\boldsymbol x_2|\\theta)\\}\\right)\\) são dois experimentos onde somente o parâmetro \\(\\theta\\) precisa ser comum. Considere um experimento misto em que é observada uma v.a. \\(J\\), com \\(P(J=1)=P(J=2)=1/2\\), independente de \\(\\boldsymbol X_1,~\\boldsymbol X_2\\) e \\(\\theta\\), e então o experimento \\(E_J\\) é realizado. Formalmente, o experimento realizado nesse caso é \\(E^*=(\\boldsymbol X^*,\\theta,\\{f^*(\\boldsymbol x^*|\\theta)\\})\\), onde \\(\\boldsymbol X^*=(J,\\boldsymbol X_J)\\) e \\(f^*(\\boldsymbol x|\\theta)=\\dfrac{1}{2}~f_j(\\boldsymbol x_j|\\theta)~.\\) Então, \\(\\text{Inf}\\left(E^*,(j,x_j)\\right) = \\text{Inf}\\left(E_j,x_j\\right)~.\\) \\(~\\) Princípio da Verossimilhança. Suponha dois experimentos \\(E_1=(\\boldsymbol X_1,\\theta,\\{f_1(\\boldsymbol x_1|\\theta)\\})\\) e \\(E_2=(\\boldsymbol X_2,\\theta,\\{f_2(\\boldsymbol x_2|\\theta)\\})\\), ambos com o mesmo parâmetro \\(\\theta\\). Suponha que \\(\\boldsymbol x_1\\) e \\(\\boldsymbol x_2\\) são pontos amostrais de \\(E_1\\) e \\(E_2\\), respectivamente, tais que \\(f_1(\\boldsymbol x_1|\\theta)\\propto c(\\boldsymbol x_1,\\boldsymbol x_2)f_2(\\boldsymbol x_2|\\theta)~,\\) \\(\\forall \\theta\\in\\Theta~.\\), então, \\(\\text{Inf}(E_1,\\boldsymbol x_1)=\\text{Inf}(E_2,\\boldsymbol x_2)\\). \\(~\\) \\(~\\) Teorema de Birnbaum. (P. Suficiência \\(\\wedge\\) P. Condicionalidade) \\(\\Longleftrightarrow\\) P. Verossimilhança. Demo: \\((\\boldsymbol{\\Longrightarrow})\\) Seja \\(\\boldsymbol x_1^*,\\;\\boldsymbol x_2^*,\\;E_1,\\;E_2\\) como no P. Verossimilhança e \\(E^*\\) como no P. Condicionalidade. Então, \\(f_1(\\boldsymbol x_1|\\theta)\\propto c(\\boldsymbol x_1,\\boldsymbol x_2)f_2(\\boldsymbol x_2|\\theta)~.\\) No espaço do experimento \\(E^*\\), defina \\(T(j,\\boldsymbol{x}_j)=\\left\\{\\begin{array}{ll}(1,\\boldsymbol{x}_1^*), &amp; \\text{se } ~j=1,~\\boldsymbol{x}_1=\\boldsymbol{x}_1^*\\\\ (j,\\boldsymbol{x}_j), &amp; \\text{c. c.} \\end{array}\\right.~.\\) Como \\(f^*(\\boldsymbol x^*|\\theta)=f^*\\left((j,\\boldsymbol x_j)|\\theta\\right)=1/2~f_j(\\boldsymbol x_j|\\theta),\\) pelo o Teorema da Fatoração é possível concluir que \\(T(j,\\boldsymbol x_j)\\) é suficiente para \\(\\theta\\) no experimento \\(E^*\\). Então, pelo P. Suficiência, \\(\\text{Inf}\\left(E^*,(1,\\boldsymbol x_1)\\right)=\\text{Inf}\\left(E^*,(2,\\boldsymbol x_2)\\right)\\) e, pelo P. Condicionalidade, \\(\\text{Inf}\\left(E^*,(1,\\boldsymbol x_1^*)\\right)\\) \\(=\\text{Inf}\\left(E_1,\\boldsymbol x_1^*\\right)\\) \\(=\\text{Inf}\\left(E^*,(2,\\boldsymbol x_2)\\right)\\) \\(=\\text{Inf}\\left(E_2,\\boldsymbol x_2^*\\right)~,\\) de modo que \\(\\text{Inf}\\left(E_1,\\boldsymbol x_1^*\\right)=\\text{Inf}\\left(E_2,\\boldsymbol x_2^*\\right)\\) e, portanto, vale o P. Verossimilhança. \\(~\\) \\((\\boldsymbol{\\Longleftarrow})\\) Como vale o P. Verossimilhança, \\(f_1(x_1^*|\\theta)\\propto f_2(x_2^*|\\theta)\\) e \\(\\text{Inf}(E_1,x_1^*)=\\text{Inf}(E_2,x_2^*)~.\\) Além disso, se \\(x^*=(1,x_1^*)\\), \\(f^*(x^*|\\theta)\\) \\(=f^*\\left((1,x_1^*)|\\theta\\right)\\) \\(=1/2~f_1(x_1^*|\\theta)\\) \\(\\propto f_1(x_1^*|\\theta)\\) \\(\\propto 1/2~f_2(x_2^*|\\theta)\\) \\(=f^*\\left((2,x_2^*)|\\theta\\right)~,\\) e, como vale P. Verossimilhança, então \\(\\text{Inf}\\left(E^*,(1,x_1^*)\\right)=\\text{Inf}(E_1,x_1^*)~.\\) Usando o mesmo argumento, se \\(x^*=(2,x_2^*)\\), conclui-se que \\(\\text{Inf}\\left(E^*,(2,x_2^*)\\right)=Inf(E_2,x_2^*)~.\\) Portando, vale o P. Condicionalidade. \\(~\\) Pelo Teorema de Fatoração, \\(f(\\boldsymbol x|\\theta)\\) \\(=g\\left(T(\\boldsymbol x),\\theta\\right)h(\\boldsymbol x)\\) \\(\\propto g\\left(T(\\boldsymbol x),\\theta\\right)~.\\) Se \\(\\boldsymbol x_1\\) e \\(\\boldsymbol x_2\\) são pontos amostrais tais que \\(T(\\boldsymbol x_1)=T(\\boldsymbol x_2),\\) \\(f_1(\\boldsymbol x_1|\\theta)\\propto g\\left(T(\\boldsymbol x_1),\\theta\\right)\\) \\(\\propto g(T(\\boldsymbol x_2),\\theta)\\) \\(\\propto f_2(\\boldsymbol x_2|\\theta)~,\\) tem-se, pelo P. Verossimilhança, que \\(\\text{Inf}(E_1,\\boldsymbol x_1)=\\text{Inf}(E_2,\\boldsymbol x_2)\\) e, portanto vale o P. Suficiência. \\(~\\) \\(~\\) Exemplo. Seja \\(X_1|\\theta \\sim Bin(n,\\theta)\\) e \\(X_2|\\theta \\sim BinNeg(r,\\theta)\\), onde \\(n\\) é número total de lançamentos (fixado) e \\(r\\) é número de fracassos (fixado). Então, \\(E_1=\\left(\\boldsymbol X_1,\\theta,\\left\\{\\binom{n}{x_1}\\theta^{x_1}(1-\\theta)^{n-x_1}:\\theta\\in[0,1]\\right\\}\\right)\\) e \\(E_2=\\left(\\boldsymbol X_2,\\theta,\\left\\{\\binom{r+x_2-1}{x_2}\\theta^{x_2}(1-\\theta)^{r}:\\theta\\in[0,1]\\right\\}\\right)~.\\) Note que em ambos os experimentos, o parâmetro \\(\\theta\\) é o mesmo! \\(~\\) \\(~\\) (I) Estimação pontual usando Estimador Não-Viesado (ENV) para \\(\\theta\\), isto é, \\(\\hat{\\theta}_i(X_i)\\) tal que \\(E\\left[\\hat{\\theta}_i(X_i)|\\theta\\right]=\\theta\\). Nesse caso, \\(\\text{Inf}(E_i,x_i)=\\hat{\\theta}_i(x_i)\\) para \\(i=1,2\\). Então, \\(\\hat{\\theta}_1(X_i)=\\dfrac{X_1}{n}\\) e \\(\\hat{\\theta_2}(X_2)=\\dfrac{X_2-1}{X_2+r-1}\\) são ENV para \\(\\theta\\) em \\(E_1\\) e \\(E_2\\), respectivamente. Suponha que \\(n=12,r=3\\) e \\(x_1=x_2=9\\). Então, as funções de verossimilhança são \\(f_1(x_1|\\theta)=\\binom{12}{9}\\theta^9(1-\\theta)^3\\) \\(\\propto \\binom{11}{9}\\theta^9(1-\\theta)^3=f_2(x_2|\\theta)\\). Contudo, \\(\\hat{\\theta}_1(x_1)=\\dfrac{9}{12}=0,75\\) \\(\\neq\\hat{\\theta}_2(x_2)=\\dfrac{8}{11}\\approx0,72\\bar{72}\\), e portanto, o ENV viola o P. Verossimilhança. \\(~\\) \\(~\\) (II) Estimador de Máxima Verossimilhança (EMV) \\(\\delta_{MV}\\) é um estimador tal que \\(\\delta_{MV}(\\boldsymbol x)=\\displaystyle\\arg\\sup_{\\theta\\in\\Theta} f(\\boldsymbol x|\\theta)~.\\) \\(\\delta_{MV}^1(x_1)=\\dfrac{x_1}{n}\\) \\(=\\delta^2_{MV}(x_2)=\\dfrac{x_2}{x_2+r}\\) \\(=\\dfrac{9}{12}=0,75~.\\) Portanto, o EMV não viola o P. Verossimilhança. \\(~\\) \\(~\\) (III) Suponha que deseja-se testar \\(H_0:\\theta\\leq 1/2 \\; (\\Theta_0)\\) contra \\(H_1:\\theta &gt; 1/2 \\; (\\Theta_1)~,\\) com \\(\\Theta=\\Theta_0 \\cup \\Theta_1~.\\) \\(\\phi(x)=\\left\\{\\begin{array}{ll} 1,&amp; T(x)\\leq c(\\alpha)\\\\ 0,&amp; T(x)&gt; c(\\alpha)\\end{array}\\right.\\) em que \\(T\\) é uma estatística de teste (isto é, valores grandes de \\(T(x)\\) indicam que \\(x\\) é favorável a \\(H_0\\)) e \\(c(\\alpha)\\) é tal que \\(\\alpha=\\displaystyle\\sup_{\\theta_0\\in\\Theta_0}\\text{P}(\\text{Rejeitar } H_0~|~\\theta_0)\\) \\(\\displaystyle\\sup_{\\theta_0\\in\\Theta_0}\\text{P}\\left(\\left\\{x\\in\\mathfrak{X} : T(x)\\leq c(\\alpha)\\right\\}~|~\\theta_0\\right)~.\\) Considere \\(T(x)=RV(x)=\\dfrac{\\underset{\\Theta_0}{sup}f(x|\\theta)}{\\underset{\\Theta}{sup}f(x|\\theta)}\\), de modo que um \\(p\\)-value pode ser calculado por \\(p(x)=\\displaystyle\\sup_{\\Theta_0} \\text{P}\\left(T(X)\\geq T(x)|\\theta\\right)\\). Assim, um teste que conduz a uma decisão equivalente ao descrito anteriormente é rejeitar \\(H_0\\) se, e somente se, \\(p(x)\\leq \\alpha\\). Considere a escolha usual \\(\\alpha=0.05\\). Então, \\(p_1(x_1)=P(X_1\\geq 9|\\theta=1/2)=0.073&gt;0.05\\Rightarrow\\) Não rejeita \\(H_0~.\\) \\(p_2(x_2)=P(X_2\\geq 9|\\theta=1/2)=0.0327&lt;0.05\\Rightarrow\\) Rejeita \\(H_0~.\\) Portanto, o Teste da Razão de Verossimilhanças viola o P. Verossimilhança. \\(~\\) \\(~\\) (IV) Aboragem Bayesiana \\(\\Rightarrow Inf(E_i,x_i)=f_i(\\theta|x_i)\\) a) Bayesiano Subjetivista Como o parâmetro \\(\\theta\\) é o mesmo nos dois experimentos, a priori deve ser a mesma. \\(f(\\theta)\\) não depende de \\(\\{f_i(\\boldsymbol x|\\theta):\\theta\\in\\Theta\\}\\) \\(f(\\theta|\\boldsymbol x)\\propto f(\\theta)f(x_1|\\theta)\\propto f(\\theta)f(x_2|\\theta)\\) e, portanto, satisfaz o P. Verossimilhança. \\(~\\) b) Bayesiano Objetivista (p.e., usando priori de Jeffreys) Para \\(E_1\\), \\(f_1(\\theta)\\propto |I_F(\\theta)|^{1/2}\\propto\\) \\(\\theta^{-1/2}(1-\\theta)^{-1/2}\\sim Beta (1/2,1/2)\\) Para \\(E_2\\), \\(f_2(\\theta)\\propto\\) \\(\\theta^{-1}(1-\\theta)^{-1/2}\\sim Beta (0,1/2)\\) (distribuição imprópria). Se o número de sucessos é \\(x=x_1=x_2\\) e número de fracassos é \\(y=n-x_1=r\\), temos que \\(\\theta|X_1=x_1\\sim Beta(x+1/2,y+1/2)~\\) e \\(~\\theta|X_2=x_2\\sim Beta(x,y+1/2)~.\\) Como \\(f_1(x_1|\\theta)\\propto f_2(x_2|\\theta)\\) mas \\(f_1(\\theta)\\neq f_2(\\theta)\\), tem-se que \\(f_1(\\theta|x_1)\\neq f_2(\\theta|x_2)\\) e, portanto, esse procedimento viola o P. Verossimilhança. \\(~\\) Referências "],["TeoDec.html", "4 Introdução à Teoria da Decisão 4.1 Conceitos Básicos 4.2 Aleatorização e Decisões Mistas 4.3 Problemas com Dados", " 4 Introdução à Teoria da Decisão A teoria da decisão é uma das possíveis formas de embasar a inferência bayesiana. Sob essa abordagem, considera-se uma função de perda (ou função de utilidade) que quantifica numericamente as consequências de sua decisão para um dado valor do parâmetro. Essa quantificação de preferência é novamente subjetiva e é possível fazer uma construção de função de perda similar ao que fizemos com probabilidade. Ou seja, dado um conjunto de suposições, existe uma função de perda que representa numericamente suas preferências para cada decisão e cada possível valor do parâmetro. Essa construção não será feita aqui mas pode ser encontrada no livro Optimal Statistical Decisions (DeGroot 1970). \\(~\\) 4.1 Conceitos Básicos \\(d \\in \\mathcal{D}:\\) decisão - uma particular afirmação, por exemplo, sobre \\(\\theta\\). No contexto inferencial, uma decisão pode ser uma estimativa (pontual ou intervalar) para \\(\\theta\\) ou a escolha de uma hipótese específica em um teste de hipóteses. \\(\\mathcal{D}:\\) espaço de decisões - conjunto de todas as possíveis decisões (afirmações). \\(\\theta\\): estado da natureza - quantidade desconhecida ou parâmetro, no contexto de inferência estatística. \\(\\Theta\\): espaço dos estados da natureza - espaço paramétrico. \\(L:\\mathcal{D}\\times\\Theta\\longrightarrow\\mathbb{R}\\): função de perda - \\(L(d,\\theta)\\) que representa o prejuízo de uma decisão \\(d\\) quando o estado da natureza é \\(\\theta\\). \\(~\\) Exemplo 1. Suponha que você está saindo de casa pela manhã e precisa tomar uma importante decisão: levar ou não seu guarda-chuva. \\(\\mathcal{D}=\\{G,G^c\\}\\) , onde \\(G:\\) levar guarda-chuva. \\(\\Theta=\\{C,C^c\\}\\) , onde \\(C:\\) chuva. Suponha que carregar o guarda-chuva é algo que não lhe agrada mas, por outro lado, você odeia ficar molhado e acredita que a pior situação seria não levá-lo e tomar chuva. Você ficará incomodado se levar o guarda-chuva e chover pois, além de tê-lo carregado, voltou para casa com os sapatos molhados. Note que, nessas circunstâncias, o cenário preferido por você seria não levar o guarda-chuva e não chover. Para quantificar suas preferências, considere uma função de perda \\(L:\\mathcal{D}\\times\\Theta\\longrightarrow\\mathbb{R}\\), de modo que, quanto mais algum cenário lhe gera incômodo, maior sua perda. Um exemplo é apresentado a seguir. Estados da Natureza Decisão \\(C\\) \\(C^c\\) \\(G\\) 2 (ruim) 1 (bom) \\(G^c\\) 3 (pior) 0 (melhor) \\(P(\\theta)\\) p 1-p Uma possível maneira de tomar uma decisão é escolher a decisão menos prejudicial. Se levar o Guarda chuva, no pior caso, sua perda é \\(\\displaystyle \\max_\\theta L(G,\\theta)=2\\) e, se não levá-lo, a maior perda possível é \\(\\displaystyle \\max_\\theta L(G^c,\\theta)=3\\). Assim, a decisão que tem a menor dentre as maiores perdas é levar o guarda-chuva. Esse procedimento para tomada de decisões é chamado min-max e consiste em escolher a decisão \\(d&#39;\\) tal que \\(d&#39; = \\displaystyle \\underset{d}{\\text{argmin}} \\max_\\theta L(d,\\theta)\\). Sendo um pouco mais otimista, você pode escolher a decisão que tenha a maior dentre as menores perdas. Esse procedimento é chamado max-min e consiste em escolher a decisão \\(d&#39; = \\displaystyle \\underset{d}{\\text{argmax}} \\min_\\theta L(d,\\theta)\\). No nosso exemplo, esse procedimento também sugere que você sempre carregue o guarda-chuvas. Note que a decisão escolhida pelos dois procedimentos descritos anteriormente sugere que você sempre deve carregar o guarda-chuvas. Contudo, isso pode não ser razoável. Imagine que você estava lendo notícias antes de sair de casa e viu que a probabilidade de chuva era \\(0.01\\). Nesse caso, não parece fazer sentido você levar o guarda-chuva, já que isso vai te trazer um desconforto e a chance de chover é muito baixa. Assim, a probabilidade de chover deveria ser levada em consideração em sua tomada de decisão. Uma maneira de fazer isso é utilizar a perda esperada. Note que \\(\\theta\\) é uma quantidade desconhecida e, pelo que já foi discutido anteriormente, você deve descrever sua incerteza em relação a essa quantidade em termos de probabilidade. Suponha que no exemplo \\(P(C)=p\\), \\(0\\leq p\\leq 1\\). \\(~\\) Para cada decisão \\(d \\in \\mathcal{D}\\), é possível calcular o valor esperado da função de perda (perda esperada ou risco da decisão \\(d\\) contra a priori \\(P\\)) \\[\\rho(d,P) = E\\left[L(d,\\theta) ~|~ P\\right] = \\int_{\\Theta} L(\\theta) dP(\\theta).\\] \\(~\\) No exemplo, temos \\(E\\left[L(G^{},\\theta)\\right]\\) \\(=L(G,C)P(C) + L(G,C^c)P(C^c)\\) \\(=2p+1(1-p)\\) \\(=p+1\\); \\(E\\left[L(G^c,\\theta)\\right]\\) \\(=L(G^c,C)P(C) + L(G^c,C^c)P(C^c)\\) \\(=3p+0(1-p)\\) \\(=3p\\). Deste modo, as perdas esperadas associadas a cada decisão dependem da probabilidade de chuva \\(p\\). Assim, para cada possível valor de \\(p\\), deve-se tomar a decisão que tem menor perda esperada. Por exemplo, se a probabilidade de chuva é \\(p=0.1\\), temos que as perdas esperadas para as decisões de levar ou não o guarda-chuva são, respectivamente, \\(E\\left[L(G,\\theta)\\right]=1.1\\) e \\(E\\left[L(G^c,\\theta)\\right]=0.3\\). Assim, sob essa abordagem, sua decisão seria de não levar o guarda-chuva nesse caso. Por outro lado, se a probabilidade de chuva for \\(p=0.9\\), suas perdas esperadas seriam respectivamente \\(E\\left[L(G,\\theta)\\right]=1.9\\) e \\(E\\left[L(G^c,\\theta)\\right]=2.7\\), de modo que a decisão ótima seria levar o guarda-chuva. O gráfico a seguir apresenta as perdas para cada decisão \\(d\\) e para cada valor de \\(p\\). É possível notar que a decisão ótima é levar o guarda-chuva quando \\(p&gt;0.5\\) e não levá-lo caso contrário. \\(~\\) Vamos denotar por \\(\\rho^*\\) o risco de bayes, isto é, a perda esperada da decisão de Bayes (ou decisão ótima) \\(d^*\\in \\mathcal{D}\\) tal que \\(\\rho^*(P)\\) \\(=\\rho(d^*,P)\\) \\(=\\underset{d\\in\\mathcal{D}}{min}~\\rho(d,P)\\). \\(~\\) Para uma argumentação mais formal sobre a escolha pela decisão que minimiza a perda esperada, ver Optimal Statistical Decisions (DeGroot, M.H.). \\(~\\) \\(~\\) Vamos denotar um problema de decisão por \\(\\left(\\Theta, \\mathcal{D}, L, P\\right)\\), onde \\(\\Theta\\) é o espaço paramétrico, \\(\\mathcal{D}\\) é o espaço de decisões, \\(L: \\mathcal{D} \\times \\Theta \\longrightarrow \\mathbb{R}\\) é uma função de perda e \\(P\\) é a distribuição de probabilidade que representa sua crença sobre a quantidade desconhecida \\(\\theta\\). Equivalentemente, a função de perda \\(L\\) pode ser substituída por uma função de utilidade \\(U\\) (por exemplo, tome \\(U=-L\\)). \\(~\\) A solução para um problema de decisão \\(\\left(\\Theta, \\mathcal{D}, L, P\\right)\\) é a decisão de Bayes, \\({d}^* \\in \\mathcal{D}\\), tal que \\(\\rho^*(P) = \\rho({d}^*,P) = \\displaystyle \\inf_{d \\in \\mathcal{D}} \\rho(d,\\theta)\\), com \\(\\rho(d,P) = \\displaystyle \\int_\\Theta L(d,\\theta)dP(\\theta)\\). \\(~\\) 4.2 Aleatorização e Decisões Mistas Seja \\(D=\\left\\{d_1,d_2,\\ldots\\right\\}\\) um espaço de decisões e considere \\(\\mathcal{M}\\) o conjunto de todas as decisões mistas (ou aleatorizadas), isto é, para toda distribuição de probabilidades \\(Q=\\left\\{q_1,q_2,\\ldots\\right\\}\\), uma decisão \\(d\\in\\mathcal{M}\\) se \\(d\\) consiste em escolher a decisão \\(d_i\\) com probabilidade \\(q_i\\). Assim, a perda associada à uma decisão \\(d\\in\\mathcal{M}\\) é \\(L(d,\\theta) = \\sum q_i L(d_i,\\theta)\\) e o risco dessa decisão é \\(\\rho(d,P)\\) \\(= \\displaystyle \\int_\\Theta L(d,\\theta) dP(\\theta)\\) \\(=\\displaystyle \\int_\\Theta \\sum q_i L(d_i,\\theta) dP(\\theta)\\) \\(=\\displaystyle \\sum q_i \\int_\\Theta L(d_i,\\theta) dP(\\theta)\\) \\(=\\displaystyle \\sum q_i~ \\rho(d_i,\\theta)\\). Considere a decisão \\({d}^* \\in \\mathcal{D}\\) tal que \\(\\rho({d}^*,P) = \\displaystyle \\inf_{d \\in \\mathcal{D}} \\rho(d,\\theta)\\). Então, \\(\\forall ~d \\in \\mathcal{M}\\), \\(\\rho(d,P)\\) \\(=\\displaystyle \\sum q_i~ \\rho(d_i,\\theta)\\) \\(\\geq \\displaystyle \\sum q_i~ \\rho({d}^*,\\theta)\\). Em palavras, para toda decisão aleatorizada \\(d\\in\\mathcal{M}\\), existe uma decisão não aleatorizada \\({d}^*\\in\\mathcal{D} \\subset\\mathcal{M}\\), tal que \\(\\rho({d}^*,P) \\leq \\rho(d,P)\\). \\(~\\) 4.3 Problemas com Dados Suponha que antes de escolher uma decisão \\(d \\in \\mathcal{D}\\), é possível observar um v.a. \\(\\boldsymbol X\\) que (supostamente) está relacionado com \\(\\theta\\) (isto é, \\(\\boldsymbol X\\) traz alguma informação sobre \\(\\theta\\)). Desde modo, considere a família \\(\\mathcal{P}=\\left\\{ f(\\cdot|\\theta) : \\theta \\in \\Theta\\right\\}\\) de funções de distribuição condicionais para \\(\\boldsymbol X\\), isto é, para cada \\(\\theta \\in \\Theta\\) é possível determinar a distribuição condicional de \\(\\boldsymbol X|\\theta\\). Essa distribuição, juntamente com a distribuição a priori \\(f(\\theta)\\), determina totalmente uma distribuição conjunta \\(f(\\boldsymbol x,\\theta) = f(\\boldsymbol x|\\theta) f(\\theta)\\). Pode-se definir uma função de decisão \\(\\delta: \\mathfrak{X} \\longrightarrow \\mathcal{D}\\) que associa a cada resultado experimental \\(\\boldsymbol x \\in \\mathfrak{X}\\) uma decisão \\(d \\in \\mathcal{D}\\). Denote o conjunto de todas as possíveis funções de decisão por \\(\\Delta\\). O risco \\(r(\\delta,P)\\) da função de decisão \\(\\delta \\in \\Delta\\) é dado por \\(r(\\delta,P)\\) \\(=E\\left[L\\left(\\delta,\\theta\\right)\\right]\\) \\(=\\displaystyle \\int_\\Theta \\int_{\\mathfrak{X}} L\\left(\\delta(\\boldsymbol x),\\theta\\right) dP(\\boldsymbol x,\\theta)\\). A função de decisão de Bayes, \\({\\delta}^* \\in \\Delta\\), é tal que \\({\\rho}^*(P)\\) \\(=\\rho({\\delta}^*,P)\\) \\(=\\displaystyle \\inf_{\\delta\\in \\Delta} \\rho(\\delta,P)\\). \\(~\\) Exemplo 1. Seja \\(\\Theta=\\{\\theta_1,\\theta_2\\}\\), \\(\\mathcal{D}=\\{d_1,d_2\\}\\), \\(X|\\theta_1\\sim Ber(3/4)\\), \\(X|\\theta_2 \\sim Ber(1/3)\\), \\(\\mathfrak{X}=\\{0,1\\}\\) e, a priori, \\(P(\\theta=3/4)=P(\\theta=1/3)=1/2\\). Considere a função de perda a seguir. L \\(\\theta_1\\) \\(\\theta_2\\) \\(d_1\\) 0 5 \\(d_2\\) 10 0 Temos que \\(|\\Delta| = 2^2=4\\), de modo que as possíveis funções de decisão são \\(\\delta_1(x)=\\left\\{\\begin{array}{lr} d_1, &amp; x=1\\\\ d_2, &amp;x=0\\end{array}\\right.\\) \\(\\delta_2(x)=\\left\\{\\begin{array}{lr} d_1, &amp; x=0\\\\ d_2, &amp;x=1\\end{array}\\right.\\) \\(\\delta_3(x)=d_1\\) e \\(\\delta_4(x)=d_2\\). Para a função \\(\\delta_1\\), temos x \\(\\theta\\) \\(L(\\delta_1(x),\\theta)\\) \\(P(x|\\theta)\\) \\(P(\\theta)\\) \\(P(x,\\theta)\\) 0 \\(\\theta_1\\) 10 1/4 1/2 1/8 0 \\(\\theta_2\\) 0 2/3 1/2 2/6 1 \\(\\theta_1\\) 0 3/4 1/2 3/8 1 \\(\\theta_2\\) 5 1/3 1/2 1/6 \\(\\rho(\\delta_1)\\) \\(=\\displaystyle \\sum_{x=0}^1\\sum_{i=1}^2L(\\delta_1(x),\\theta_i)\\underbrace{P(X=x|\\theta_i)P(\\theta_i)}_{P(x,\\theta)}\\) \\(=10~\\dfrac{1}{8}+5~\\dfrac{1}{6}\\) \\(=\\dfrac{50}{24}\\) De forma análoga, \\(~\\rho(\\delta_2,P)=130/24\\) , \\(~\\rho(\\delta_3,P)=60/24\\) , \\(~\\rho(\\delta_4,P)=120/24\\) , e, assim. \\({\\delta}^*(x)={\\delta}^*_1(x)=\\left\\{\\begin{array}{rl} d_1, &amp; x=1\\\\ d_2, &amp; x=0\\end{array}\\right.\\) Risco de Bayes: \\(\\rho^*(P)=\\rho({\\delta}^*,P)=50/24\\). \\(~\\) Em problemas mais complicados, pode ser muito trabalhoso (ou impossível) obter a função de decisão dessa forma, chamada forma normal. Sob essa abordagem, é necessário encontrar a função de decisão de bayes \\({\\delta}^*\\) dentre todas as possíveis funções de decisão. Nesses casos, pode ser mais fácil resolver o problema usando a forma extensiva em que, para cada \\(\\boldsymbol x \\in \\mathfrak{X}\\), obtem-se a decisão de Bayes \\({d}_{x}^*\\) que minimiza o risco posterior, definido por \\(r_x(d)\\) \\(= \\displaystyle \\int_\\Theta L(d,\\theta) dP(\\theta|\\boldsymbol x)\\). Assim, é posível obter uma decisão de Bayes \\({d}_x^*\\) para um específico ponto \\(x\\) observado ou, ainda, construir uma função de decisão de Bayes, fazendo \\(~{\\delta}^*(\\boldsymbol x) = {d}_x^*~\\) para cada \\(\\boldsymbol x \\in \\mathfrak{X}\\). A seguir, é mostrado que essa duas formas produzem resultados que minimizam o risco. Note que \\(r(\\delta,P)\\) \\(=E\\left[L\\left(\\delta,\\theta\\right)\\right]\\) \\(=\\displaystyle \\int_\\Theta \\int_{\\mathfrak{X}} L\\left(\\delta(\\boldsymbol x),\\theta\\right) dP(\\boldsymbol x,\\theta)\\) \\(=\\displaystyle \\int_\\Theta \\int_{\\mathfrak{X}} L\\left(\\delta(\\boldsymbol x),\\theta\\right) dP(\\boldsymbol x|\\theta)dP(\\theta)\\) \\(=\\displaystyle \\int_{\\mathfrak{X}} \\left[ \\underbrace{\\int_\\Theta L\\left(\\delta(\\boldsymbol x),\\theta\\right) dP(\\theta|\\boldsymbol x)}_{r_x\\left(\\delta(\\boldsymbol x)\\right)} \\right]dP(\\boldsymbol x)\\). Note que a integral interna (em \\(\\theta\\)) pode ser resolvida para cada \\(\\boldsymbol x\\) fixado. Para cada \\(\\boldsymbol x\\), considere a decisão \\({d}_x^*\\) tal que \\(r_x\\left({d}_x^*\\right)\\) \\(=\\displaystyle \\inf_{d \\in \\mathcal{D}} r_x(d)\\). Assim \\(r(\\delta,P)\\) \\(=\\displaystyle \\int_{\\mathfrak{X}} \\left[ \\underbrace{\\int_\\Theta L\\left(\\delta(\\boldsymbol x),\\theta\\right) dP(\\theta|\\boldsymbol x)}_{r_x\\left(\\delta(\\boldsymbol x)\\right)} \\right]dP(\\boldsymbol x)\\) \\(=\\displaystyle \\int_{\\mathfrak{X}} \\left[ {r_x\\left(\\delta(\\boldsymbol x)\\right)} \\right]dP(\\boldsymbol x)\\) \\(\\geq \\displaystyle \\int_{\\mathfrak{X}} \\left[ {r_x\\left({d}_x^*\\right)} \\right]dP(\\boldsymbol x)\\) \\(= \\displaystyle \\int_{\\mathfrak{X}} \\left[ {r_x\\left({d}_x^*\\right)} \\right]dP(\\boldsymbol x)\\). Assim, a função \\({\\delta}^*(x)={d}^*_{x}\\) é uma função de decisão de Bayes. \\(~\\) No Exemplo 1 \\(~X|\\theta_1 \\sim Ber(3/4)\\) , \\(~X|\\theta_2 \\sim Ber(1/3)\\) e \\(~P(\\theta_1)=P(\\theta_2)=1/2\\). \\(P(\\theta_1|x=0)\\) \\(=\\dfrac{P(X=0|\\theta_1)P(\\theta_1)}{P(X=0|\\theta_1)P(\\theta_1)+P(X=0|\\theta_2)P(\\theta_2)}\\) \\(=\\dfrac{\\frac{1}{4}~\\frac{1}{2}}{\\frac{1}{4}~\\frac{1}{2}+\\frac{2}{3}~\\frac{1}{2}}\\) \\(=\\dfrac{3}{11}\\) \\(P(\\theta_2|X=0)\\) \\(=\\dfrac{8}{11}\\) \\(r_x(d_1,P)\\) \\(=\\displaystyle \\sum_{i=1}^2L(d_1,\\theta_i)P(\\theta_i|X=0)\\) \\(=0~P(\\theta_1|X=0)+10~P(\\theta_2|X=0)\\) \\(=\\dfrac{80}{11}\\) \\(r_x(d_2,P)\\) \\(=5~P(\\theta_1|X=0)+0~P(\\theta_2|X=0)\\) \\(=\\dfrac{15}{11}\\) Logo, para \\(x=0\\), \\({d}_0^*={d}_2\\). De forma análoga, para \\(x=1\\), \\({d}_1^*={d}_2\\) e, assim, \\({\\delta}^*(x)=\\left\\{\\begin{array}{rl} {d}_2, &amp; x=0\\\\ {d}_1, &amp; x=1~.\\end{array}\\right.\\) \\(~\\) Referências "],["Estimacao.html", "5 Estimação 5.1 Estimação Pontual 5.2 Estimação por Regiões 5.3 Custo das Observações", " 5 Estimação 5.1 Estimação Pontual Todos os problemas de inferência estatística podem ser vistos como um caso particular de Teoria da Decisão. Um problema de estimação pontual consiste em encontrar um chute para o valor do parâmetro \\(\\theta\\), de modo que o espaço de decisões é \\(\\mathcal{D}=\\Theta\\). Além disso, nesse tipo de problema é usual considerar funções de perda da forma \\(L(d,\\theta)=s(\\theta)\\Delta(d,\\theta)\\), onde \\(\\Delta\\) é alguma distância (ou uma medida de discrepância) relacionada ao erro por tomar a decisão \\(d\\) quando o valor do parâmetro é \\(\\theta\\) e \\(s\\) é uma função não-negativa relacionada à gravidade do erro para cada \\(\\theta\\) (pode ser constante). \\(~\\) Exemplo Considere um problema de estimação pontual, isto é, \\(\\mathcal{D}=\\Theta\\), onde a função de perda é dada por \\(L(d,\\theta)=(d-\\theta)^2\\), conhecida como perda quadrática. \\(r_x(d)\\) \\(=\\displaystyle \\int_\\Theta(d-\\theta)^2~dP(\\theta|x)\\) \\(=\\displaystyle \\int_\\Theta \\left(d^2 - 2d\\theta + \\theta^2\\right) dP(\\theta|x)\\) \\(=d^2\\displaystyle\\int_\\Theta dP(\\theta|x) - 2d\\int_\\Theta\\theta ~dP(\\theta|x) + \\int_\\Theta \\theta^2 ~dP(\\theta|x)\\) \\(=d^2-2d~E[\\theta|x]+E[\\theta^2|x]=g(d)\\). \\(\\dfrac{\\partial g(d)}{\\partial d}\\) \\(=2d-2E[\\theta|x]=0\\) \\(\\Rightarrow {d}_x^*=E[\\theta|x]\\). Logo, um estimador para \\(\\theta\\) contra a perda quadrática é \\({\\delta}^*(X)=E[\\theta|X]\\). \\(~\\) \\(~\\) Estimador de Bayes para \\(\\theta\\) contra diferentes funções de perda: Perda Quadrática: \\(L_2(d,\\theta)=(d-\\theta)^2\\) \\(~\\Longrightarrow~\\) \\({\\delta}_2^*(X)=E[\\theta|X]\\); Perda Absoluta: \\(L_1(d,\\theta)=|d-\\theta|\\) \\(\\Longrightarrow\\) \\({\\delta}_1^*(X)=Med(\\theta|X)\\); Perda 0-1: \\(L_0(d,\\theta)=c~\\mathbb{I}(d\\neq\\theta)\\) \\(\\Longrightarrow\\) \\({\\delta}_0^*(X)=Moda(\\theta|X)\\). \\(~\\) \\(~\\) Exemplo 1. Voltando à Perda Quadrática: \\(L(d,\\theta)=a(d-\\theta)^2\\), \\(a&gt;0\\). Já vimos que \\(\\delta^*(\\boldsymbol X)=E[\\theta|\\boldsymbol X]\\). É importante notar que esse estimador só faz sentido se \\(E[\\theta|\\boldsymbol X]\\in \\mathcal{D}\\). Nos casos em que isso não ocorre, tomamos um valor \\({d}_x^* \\in \\mathcal{D}\\) próximo a \\(E[\\theta|\\boldsymbol X]\\) tal que \\(r_x\\left({d}_x^*\\right)\\) é mínimo. \\(~\\) O risco a posteriori para esse estimador é \\(r_x\\left(\\delta^*(\\boldsymbol x)\\right)\\) \\(=r_x\\left(E[\\theta|\\boldsymbol x]\\right)\\) \\(=\\displaystyle \\int_\\Theta L\\left(\\delta^*(\\boldsymbol x),\\theta\\right)dP(\\theta|\\boldsymbol x)\\) \\(=\\displaystyle \\int_\\Theta \\left(\\theta-E[\\theta|\\boldsymbol x]\\right)^2 dP(\\theta|\\boldsymbol x)\\) \\(=Var(\\theta|\\boldsymbol x)\\), de modo que o risco de Bayes é dado por \\({\\rho}^*\\left(P\\right)\\) \\(=\\rho\\left(\\delta^*(\\boldsymbol X),P\\right)\\) \\(=\\displaystyle \\int_{\\mathfrak{X}} \\underbrace{ \\int_{\\Theta}(\\theta-E[\\theta|\\boldsymbol{x}])^2dP(\\theta|\\boldsymbol{x})}_{Var[\\theta|\\boldsymbol{x}]}dP(\\boldsymbol{x})\\) \\(=E\\left[Var(\\theta|\\boldsymbol X)\\right]\\). A variância da posteriori \\(Var(\\theta|\\boldsymbol x)\\) pode ser vista como uma medida de informação, no sentido que quanto menor essa variância, mais concentrada é a distribuição e há menos incerteza sobre \\(\\theta\\). Nesse sentido, espera-se que ao observar \\(\\boldsymbol X=\\boldsymbol x\\), a variância \\(Var(\\theta|\\boldsymbol x)\\) diminua em relação a variância da priori \\(Var(\\theta)\\). \\[\\underbrace{Var(\\theta)}_{constante}=\\underbrace{E\\left[Var(\\theta|\\boldsymbol X)\\right]}_{\\boldsymbol \\Downarrow}+\\underbrace{Var\\left[E(\\theta|\\boldsymbol X)\\right]}_{\\boldsymbol \\Uparrow}\\] Aparentemente, quando espera-se que a variância da posteriori diminua, a variância do estimador deveria aumentar. Muitas vezes isso é colocado como se o objetivo fosse encontrar o estimador de maior variância, em contradição com a abordagem frequentista. Contudo, há outra interpretação desse resultado: deseja-se obter um estimador que varie bastante de acordo com o valor observado de \\(\\boldsymbol X\\), isto é, a informação trazida pela amostra muda sua opinião sobre \\(\\theta\\). \\(~\\) Exemplo 2. Considere \\(X_1,...X_n\\) c.i.i.d. tais que \\(X_i|\\theta\\sim Poisson(\\theta)\\) e, a priori, \\(\\theta \\sim Gama(a,b)\\). A função de verossimilhança é \\(f(\\boldsymbol x | \\theta)\\propto \\prod {\\theta}^{x_i}~{e}^{-\\theta}\\) e a priori é \\(~f(\\theta)\\propto{\\theta}^{a-1}~{e}^{-b\\theta}\\). Assim, \\(f(\\theta | \\boldsymbol x)\\) \\(\\propto {\\theta}^{\\sum x_i}~{e}^{-n\\theta} ~\\cdot~ {\\theta}^{a-1}~{e}^{-b\\theta}\\) \\(\\propto {\\theta}^{a+\\sum x_i-1}~{e}^{-(b+n)\\theta}\\), de modo que \\(\\theta|\\boldsymbol X=\\boldsymbol x\\sim Gama\\left(a+\\sum x_i,b+n\\right)\\). Como visto anteriormente, o estimador de Bayes contra a perda quadrática é \\(\\delta^*(\\boldsymbol X)=E[\\theta|\\boldsymbol X]=\\dfrac{a+\\sum_iX_i}{b+n}\\). Para calcular o risco de Bayes, note que \\(E[X_i|\\theta]=\\theta\\), de modo que \\(E[X_i]=E\\left[E(X_i|\\theta)\\right]=E[\\theta]=a/b\\). Além disso, \\(Var(\\theta)=\\dfrac{a}{b^2}\\) e \\(Var(\\theta|\\boldsymbol X)=\\dfrac{a+\\sum_iX_i}{(b+n)^2}\\). Então, \\(\\rho^*(P)\\) \\(=E\\left[Var(\\theta|\\boldsymbol X)\\right]\\) \\(=E\\left[\\dfrac{a+\\sum_iX_i}{(b+n)^2}\\right]\\) \\(= \\dfrac{\\left(a+\\sum E[X_i]\\right)}{(b+n)^2}\\) \\(=\\dfrac{\\dfrac{a}{b}\\left(b+n\\right)}{(b+n)^2}\\) \\(=\\dfrac{a}{b(b+n)}\\). Por fim, note que a decisão de Bayes pode ser escrita como uma combinação linear convexa da média da distribuição a priori e do estimador de máxima verossimilhança \\(E[\\theta|\\boldsymbol X]\\) \\(=\\dfrac{a+\\sum_iX_i}{b+n}\\) \\(=\\dfrac{b}{b+n}\\left(\\dfrac{a}{b}\\right)+\\dfrac{n}{b+n}\\bar{X}\\). \\(~\\) Resultado: Seja \\(L(d,\\theta)=\\mathbb{I}(|\\theta-d|&gt; \\varepsilon)\\) \\(=1-\\mathbb{I}(d-\\varepsilon\\leq \\theta \\leq d+\\varepsilon)\\) , \\(\\varepsilon &gt; 0\\). Então, \\(\\delta^*(\\boldsymbol X)\\) é centro do intervalo modal, isto é, o intervalo de tamanho \\(2\\varepsilon\\) de maior densidade a posteriori. Em particular, quando \\(\\varepsilon \\downarrow 0\\), temos que \\({\\delta}^*(\\boldsymbol X)=Moda(\\theta|\\boldsymbol X)\\). Demo: O risco posterior de uma decisão \\(d\\) é \\(r_{\\boldsymbol{x}}(d)\\) \\(=E\\left[L(d,\\theta)|\\boldsymbol{x}\\right]\\) \\(=E\\left[\\mathbb{I}(|\\theta-d|&gt; \\varepsilon)\\right]\\) \\(=E\\left[1-\\mathbb{I}(d-\\varepsilon\\leq \\theta \\leq d+\\varepsilon)|\\boldsymbol x\\right]\\) \\(=1-P(d-\\varepsilon\\leq \\theta \\leq d+\\varepsilon|\\boldsymbol x)\\). O risco \\(r_{\\boldsymbol{x}}(d)\\) é mínimo quando a probabilidade \\(P(d-\\varepsilon\\leq \\theta \\leq d+\\varepsilon|\\boldsymbol x)\\) é máxima. Assim, basta tomar o intervalo \\(\\left[{d}_x^*-\\varepsilon ~;~ {d}_x^*+\\varepsilon\\right]\\) com maior probabilidade a posteriori e o estimador de Bayes nesse caso será o valor central desse intervalo, \\({d}_x^*\\). \\(~\\) Exemplo 3. Considere o exemplo anterior onde \\(\\theta|\\boldsymbol x\\sim Gama(a+\\sum x_i,b+n)\\) e a função de perda do resultado anterior, \\(L(d,\\theta)=\\mathbb{I}(|\\theta-d|&gt; \\varepsilon)\\). Temos que \\(f(\\theta|\\boldsymbol x)\\) \\(\\propto \\theta^{\\overbrace{a+\\sum x_i-1}^{A_x}}e^{-\\overbrace{(b+n)}^{B_x}\\theta}\\) \\(=\\theta^{A_x}e^{-{B_x}\\theta}\\) theta = seq(0,1.2,0.01) #Parâmetros da dist a posteriori gama a1 = 3 b1 = 10 posterior = dgamma(theta, a1, b1 ) # Escolhendo valores de epsilon e calculando a perda mínima associada e=c(0.5,0.4,0.3,0.25,0.2,0.15,0.1,0.05,0) loss &lt;- NULL for(i in 1:length(e)){ loss[i] &lt;- theta[which.min(as.vector(apply(matrix(theta), 1, function(d) sum(posterior*(abs(theta-d)&gt;e[i])))))]} # Criando o gráfico n &lt;- length(e) tibble(x=rep(loss,each=length(theta)), e=rep(round(e,2),each=length(theta)), theta=rep(theta,(n)), post = rep(posterior,n)) %&gt;% ggplot() + geom_segment(aes(x=x, xend=x, y=0,yend=dgamma(x, a1, b1 ),colour=as.factor(e))) + geom_point(aes(x=x, y=0,colour=as.factor(e))) + geom_segment(aes(x=x-e, xend=x+e, y=0,yend=0,colour=as.factor(e))) + geom_line(aes(x=theta,y=post, colour=&quot;Dist. a posteriori&quot;)) + geom_segment(aes(x=(a1-1)/b1, xend=(a1-1)/b1, y=0,yend=dgamma((a1-1)/b1, a1, b1 ), colour = &quot;Moda a posteriori&quot;),lty=2) + xlab(expression(theta)) + ylab(expression(paste(&quot;f(&quot;,theta,&quot;|x)&quot;))) + theme_bw() + labs(colour = &quot;epsilon&quot;) + gganimate::transition_states(rev(e)) Como comentado no resultado anterior, quando \\(\\varepsilon \\downarrow 0\\), temos que \\({\\delta}^*(\\boldsymbol x)=Moda(\\theta|\\boldsymbol x)\\) \\(= \\displaystyle\\sup_{\\theta} f(\\theta|\\boldsymbol x)\\). \\(\\dfrac{\\partial f(\\theta|\\boldsymbol x)}{\\partial \\theta}\\) \\(=(A_x-B_x\\theta)\\theta^{A_x-1}e^{-B_x\\theta}=0\\) \\(\\Leftrightarrow \\theta =\\dfrac{A_x}{B_x}\\) \\({\\delta}^*(\\boldsymbol X) = Moda(\\theta|\\boldsymbol X)\\) \\(=\\dfrac{a+\\sum X_i-1}{b+n}\\) \\(=\\dfrac{b}{b+n}\\left(\\dfrac{a-1}{b}\\right)+\\dfrac{n}{b+n}~\\bar{X}\\). \\(~\\) Resultado. Seja \\(L(d,\\theta)=c_1(d-\\theta)~\\mathbb{I}(d\\geq \\theta)+c_2(\\theta-d)~\\mathbb{I}(d&lt;\\theta)\\) com \\(c_1&gt;0\\), \\(c_2 &gt;0\\). Então, \\({\\delta}^*(\\boldsymbol{x})\\) é tal que \\(P\\left(\\theta\\leq {\\delta}^*(\\boldsymbol{x})\\big|\\boldsymbol x\\right)=\\dfrac{c_1}{c_1+c_2}\\). Em particular, se \\(c_1=c_2=c\\), temos a perda absoluta \\(L(d,\\theta)=c~|d-\\theta|\\) e \\({\\delta}^*(\\boldsymbol{X})=Med(\\theta|\\boldsymbol X)\\). Demo: exercício. \\(~\\) \\(~\\) 5.2 Estimação por Regiões Em um problema de estimação por regiões (ou estimação intervalar, no caso univariado), o objetivo é obter um conjunto de valores razoáveis para \\(\\theta\\). Mais formalmente, temos um problema aonde a decisão consiste em escolher um subconjunto do espaço paramétrico, de modo que \\(\\mathcal{D}=\\mathcal{A}\\), onde \\(\\mathcal{A}\\) é \\(\\sigma\\)-algebra de subconjuntos de \\(\\Theta\\). Uma estimativa por região é comumente chamada na literatura Bayesiana de região de credibilidade (intervalo de credibilidade, no caso univariado) ou região de probabilidade \\(~\\gamma=1-\\alpha\\). \\(~\\) Exemplo 1. Suponha que a distribuição a posteriori é \\(f(\\theta|\\boldsymbol{x})=4\\theta~\\mathbb{I}_{[0,1/2)}(\\theta)+4(1-\\theta)~\\mathbb{I}_{[1/2,1]}(\\theta)\\). Uma possível estimativa intervalar é um intervalo central ou um intervalo simétrico em torno da média (ou da moda) com uma probabilidade \\(\\gamma = 1-\\alpha\\). Nesse caso, vamos considerar um intervalo central no sentido que deixa de fora conjuntos caudais de probabilidade \\(\\alpha/2\\). Note que é possível obter o intervalo de forma analítica nesse exemplo. A seguir, são apresentadas as funções de distribuição \\(F\\) e quantílica \\(Q\\) a posteriori e o intervalo de credibilidade \\(\\alpha\\). \\(F(\\theta|\\boldsymbol x)\\) \\(=\\displaystyle \\int_0^\\theta f(t|x)dt\\) \\(=\\displaystyle \\int_0^\\theta\\left[4t~\\mathbb I_{[0,1/2]}(t)+(4-4t)~\\mathbb I_{(1/2,1]}(t)\\right]dt\\) \\(=\\left\\{\\begin{array}{lcc} 2\\theta^2 &amp;,&amp; \\theta\\leq 1/2\\\\ -2\\theta^2+4\\theta-1 &amp;,&amp; \\theta&gt;1/2\\end{array}\\right.\\) \\(Q(p)=\\left\\{\\begin{array}{lcc} \\sqrt{\\dfrac{p}{2}} &amp;,&amp; p\\leq 1/2\\\\ 1-\\sqrt{\\dfrac{1-p}{2}} &amp;,&amp; p&gt;1/2\\end{array}\\right.\\) \\(I.C.(1-\\alpha)=\\left[\\sqrt{\\dfrac{\\alpha/2}{2}};1-\\sqrt{\\dfrac{\\alpha/2}{2}}\\right]\\). alpha=0.1 theta = seq(0,1,0.01) # Densidade a posteriori dpost = Vectorize(function(t){ 4*t*I(t&gt;=0)*I(t&lt;=0.5)+ 4*(1-t)*I(t&gt;0.5)*I(t&lt;=1) }) # F. Distribuição a posteriori ppost = Vectorize(function(t){ 2*(t^2)*I(t&gt;=0)*I(t&lt;=0.5)+ 4*(1-t)*I(t&gt;0.5)*I(t&lt;=1)+I(t&gt;1) }) # F. Quantílica a posteriori qpost = Vectorize(function(t){ ifelse(t&lt;=0.5, sqrt(t/2)*I(t&gt;0), (1-sqrt((1-t)/2))*I(t&lt;1)+I(t&gt;=1)) }) post = dpost(theta) l = c(qpost((alpha/2)),qpost((1-alpha/2))) X = tibble(theta=theta,Posterior=post) ggplot(data=X,mapping = aes(x=theta,y=Posterior)) + geom_line(lwd=1.5) + geom_area(data=subset(X, theta &gt;= l[1] &amp; theta &lt;= l[2]),fill = &quot;blue&quot;, alpha=0.5)+ theme_bw() \\(~\\) Exemplo 2. Por fim, suponha que a distribuição a posteriori é \\(f(\\theta|\\boldsymbol{x})=(2-4\\theta)~\\mathbb{I}_{[0,1/2)}(\\theta)+(4\\theta-2)~\\mathbb{I}_{[1/2,1]}(\\theta)\\). Vamos construir um intervalo como no Exemplo anterior. \\(F(\\theta|\\boldsymbol x)\\) \\(=\\left\\{\\begin{array}{lcc} 2\\theta(1-\\theta) &amp;,&amp; \\theta\\leq 1/2\\\\ 2\\theta(\\theta-1)+1 &amp;,&amp; \\theta&gt;1/2\\end{array}\\right.\\) \\(Q(p)=\\left\\{\\begin{array}{lcc} \\dfrac{1}{2}-\\dfrac{\\sqrt{1-2p}}{2} &amp;,&amp; p\\leq 1/2\\\\ \\dfrac{1}{2}+\\dfrac{\\sqrt{2p-1}}{2} &amp;,&amp; p&gt;1/2\\end{array}\\right.\\) \\(I.C.(1-\\alpha)=\\left[\\dfrac{1}{2}-\\dfrac{\\sqrt{1-\\alpha}}{2};\\dfrac{1}{2}+\\dfrac{\\sqrt{1-\\alpha}}{2}\\right]\\). alpha=0.1 theta = seq(0,1,0.01) # Densidade a posteriori dpost = Vectorize(function(t){ (2-4*t)*I(t&gt;=0)*I(t&lt;=0.5)+ (4*t-2)*I(t&gt;0.5)*I(t&lt;=1) }) # F. Distribuição a posteriori ppost = Vectorize(function(t){ 2*t*(1-t)*I(t&gt;=0)*I(t&lt;=0.5)+ (2*(t^2)-2*t+1)*I(t&gt;0.5)*I(t&lt;=1)+I(t&gt;1) }) # F. Quantílica a posteriori qpost = Vectorize(function(t){ ifelse(t&lt;=0.5, (0.5-sqrt(1-2*t)/2)*I(t&gt;0), (0.5+sqrt(2*t-1)/2)*I(t&lt;1)+I(t&gt;=1)) }) post = dpost(theta) l = c(qpost((alpha/2)),qpost((1-alpha/2))) X = tibble(theta=theta,Posterior=post) ggplot(data=X,mapping = aes(x=theta,y=Posterior)) + geom_line(lwd=1.5) + geom_area(data=subset(X, theta &gt;= l[1] &amp; theta &lt;= l[2]),fill = &quot;blue&quot;, alpha=0.5)+ theme_bw() Note que, nesse exemplo, as regiões que tem mais densidade a posteriori foram excluídas do intervalo. Isso não faz muito sentido pois essas regiões têm maior chance de conter o \\(\\theta\\) que quaquer outra região de mesmo tamanho. \\(~\\) \\(~\\) Uma função de perda razoável para um problema de estimação por região deve levar em consideração dois fatores: O tamanho da região \\(d \\in \\mathcal{A}\\) (deseja-se uma região que seja menor que o espaço paramétrico); Pertinência de \\(\\theta\\) na região \\(d\\). Assim, considere uma função de perda da forma \\(L(d,\\theta)=\\lambda(d)-k~\\mathbb I_d(\\theta)\\), onde \\(\\lambda(d)\\) é o tamanho da região \\(d\\). Por exemplo, a medida de Lebesgue, no caso contínuo, ou a medida de contagem, no caso discreto (no caso geral, considere uma medida que domina a distribuição a posteriori, \\(P(\\theta|\\boldsymbol x) \\ll \\lambda\\)). No caso absolutamente contínuo, o risco a posteriori de uma decisão \\(d \\in \\mathcal{A}\\) é \\({r}_{x}(d)\\) \\(=\\displaystyle\\int_\\Theta \\left[\\lambda(d)-k~\\mathbb I_d(\\theta)\\right]dP(\\theta|\\boldsymbol x)\\) \\(=\\displaystyle \\int_\\Theta\\mathbb I_d(\\theta)d\\theta-\\int_\\Theta k~\\mathbb I_d(\\theta)f(\\theta|\\boldsymbol x)d\\theta\\) \\(=\\displaystyle \\int_d\\left(1-kf(\\theta|\\boldsymbol x)\\right)d\\theta\\). Esse risco é mínimo quando \\(d=\\left\\{\\theta\\in\\Theta:1-kf(\\theta|\\boldsymbol x)\\leq 0\\right\\}\\) \\(\\Leftrightarrow d=\\{\\theta\\in\\Theta:f(\\theta|\\boldsymbol x)\\geq 1/k\\}\\). Assim, a decisão de Bayes contra essa função de perda consiste em escolher uma região \\(d \\in \\mathcal{A}\\) que contêm os pontos do espaço paramétrico com maior densidade a posteriori. \\(~\\) Definição: A região \\(R\\subseteq \\Theta\\) é dita ser uma região HPD (Highest Posterior Density) de probabilidade \\(\\gamma\\) se \\(P(\\theta\\in R|\\boldsymbol x)=\\gamma\\); \\(\\forall \\theta \\in R\\) e \\(\\forall \\theta^\\prime\\notin R\\), \\(f(\\theta|\\boldsymbol x)\\geq f(\\theta^\\prime|\\boldsymbol x)\\). \\(~\\) Voltando ao Exemplo 2. As regiões centrais nesse exemplo tem menor densidade a posteriori. Assim, uma região HPD de probabilidade \\(\\gamma=1-\\alpha\\) é dada por \\(I.C.(1-\\alpha)=\\left[0~;~\\dfrac{1}{2}-\\dfrac{\\sqrt{\\alpha}}{2}\\right]\\bigcup \\left[\\dfrac{1}{2}+\\dfrac{\\sqrt{\\alpha}}{2}~;~1\\right]\\) alpha=0.1 theta = seq(0,1,0.01) # Densidade a posteriori dpost = Vectorize(function(t){ (2-4*t)*I(t&gt;=0)*I(t&lt;=0.5)+ (4*t-2)*I(t&gt;0.5)*I(t&lt;=1) }) # F. Distribuição a posteriori ppost = Vectorize(function(t){ 2*t*(1-t)*I(t&gt;=0)*I(t&lt;=0.5)+ (2*(t^2)-2*t+1)*I(t&gt;0.5)*I(t&lt;=1)+I(t&gt;1) }) # F. Quantílica a posteriori qpost = Vectorize(function(t){ ifelse(t&lt;=0.5, (0.5-sqrt(1-2*t)/2)*I(t&gt;0), (0.5+sqrt(2*t-1)/2)*I(t&lt;1)+I(t&gt;=1)) }) post = dpost(theta) l = c(qpost((1-alpha)/2),qpost((alpha+1)/2)) X = tibble(theta=theta,Posterior=post) ggplot(data=X,mapping = aes(x=theta,y=Posterior)) + geom_line(lwd=1.5) + geom_hline(yintercept=dpost(l[1]), lty=2, col=&quot;darkgray&quot;) + geom_area(data=subset(X, theta &lt;= l[1]),fill = &quot;blue&quot;, alpha=0.5) + geom_area(data=subset(X, theta &gt;= l[2]),fill = &quot;blue&quot;, alpha=0.5)+ theme_bw() Note que na solução anterior, o comprimento do intervalo era \\(\\sqrt{1-\\alpha}\\), enquanto que o comprimento do HPD é \\(1-\\sqrt{\\alpha}\\). Tomando, por exemplo, \\(\\alpha=0.1\\) temos que \\(\\sqrt{1-\\alpha} \\approx 0.95\\) enquanto \\(1-\\sqrt{\\alpha}\\approx 0.68\\). Por conter apenas os pontos com maior densidade, o HPD sempre terá comprimento menor ou igual a qualquer intervalo com mesma probabilidade. \\(~\\) Exemplo 3. Considere \\(X_1,...,X_n\\) c.i.i.d. tais que \\(X_i|\\theta\\sim N(\\theta,1/\\tau)\\) , \\(\\tau=1/\\sigma^2\\) , com \\(\\tau\\) conhecido (fixado). Vimos que se, a priori, \\(\\theta\\sim N(m,1/v)\\) então \\(\\theta|\\boldsymbol x\\sim N\\left(\\underbrace{\\dfrac{vm+n\\tau\\bar x}{v+n\\tau}}_{M_x},\\underbrace{\\dfrac{1}{v+n\\tau}}_{V_x}\\right)\\) e, assim, \\(Z=\\dfrac{\\theta-M_x}{\\sqrt V_x}~\\Bigg|~\\boldsymbol x\\sim N(0,1)\\). O intervalo HPD de probabilidade \\(\\gamma=1-\\alpha=0.95\\) é \\(I.C.(1-\\alpha)=\\) \\(\\left[M-z_{\\alpha/2}\\sqrt V;M+z_{\\alpha/2}\\sqrt V\\right]\\) \\(=\\left[\\dfrac{vm+n\\tau\\bar x}{v+n\\tau}\\pm1.96\\sqrt{\\dfrac{1}{v+n\\tau}}\\right]\\). Uma possível forma de representar falta de informação a priori é tomar o limite \\(~v\\downarrow 0~~\\) (\\(1/v\\uparrow \\infty\\)). Dessa forma, tem-se \\(\\theta|\\boldsymbol x\\sim N(\\bar x,1/(n\\tau))\\sim N(\\bar x,\\sigma^2/n)\\), e o intervalo HPD coincide com o I.C. frequentista \\(I.C.(1-\\alpha)=\\left[\\bar x\\pm 1.96\\dfrac{\\sigma}{\\sqrt{n}}\\right]\\). mx=2; vx=sqrt(3) alpha=0.05 theta = seq(qnorm(0.001,mx,vx),qnorm(0.999,mx,vx),length.out=100) post = dnorm(theta,mx,vx) l = c(qnorm(alpha/2,mx,vx),qnorm(1-alpha/2,mx,vx)) X = tibble(theta=theta,Posterior=post) ggplot(data=X,mapping = aes(x=theta,y=Posterior)) + geom_line(lwd=1.5) + geom_hline(yintercept=dnorm(l[1],mx,vx), lty=2, col=&quot;darkgray&quot;) + geom_area(data=subset(X, theta &gt;= l[1] &amp; theta &lt;= l[2]),fill = &quot;blue&quot;, alpha=0.5)+ theme_bw() \\(~\\) Exemplo 4: Considere \\(X_1,...,X_n\\) c.i.i.d. tais que \\(X_i|\\theta\\sim Unif(0,\\theta)\\). Vimos que se \\(\\theta\\sim Pareto(a,b)\\), então \\(\\theta|\\boldsymbol x\\sim Pareto (a+n,\\max\\{x_{(n)},b\\})\\) \\(f(\\theta|\\boldsymbol x) =\\dfrac{(a+n)[\\max\\{x_{(n)},b\\}]^{a+n}}{\\theta^{a+n+1}}~\\mathbb I_{[\\max\\{x_{(n)},b\\},\\infty)}\\) Note que a função de densidade a posteriori é estritamente decrescente de modo que o extremo inferior da região HPD é \\(\\max\\{x_{(n)},b\\}\\). A função de distribuição a posteriori é \\(F(\\theta|\\boldsymbol x)=1-\\left(\\dfrac{\\max\\{x_{(n)},b\\}}{\\theta}\\right)^{a+n}\\), de modo que o extremo superior do intervalo pode ser obtido fazendo \\(1-\\left(\\dfrac{\\max\\{x_{(n)},b\\}}{\\theta}\\right)^{a+n}=\\gamma\\) \\(\\Leftrightarrow\\dfrac{\\max\\{x_{(n)},b\\}}{\\theta^*}=(1-\\gamma)^{1/(a+n)}\\) \\(\\Leftrightarrow \\theta^*=\\dfrac{\\max\\{x_{(n)},b\\}}{(1-\\gamma)^{1/(a+n)}}\\). \\(I.C.(1-\\alpha)=\\left[\\max\\{x_{(n)},b\\},\\dfrac{\\max\\{x_{(n)},b\\}}{\\alpha^{1/(a+n)}}\\right]\\) ax=2; bx=1 maxt=bx/((alpha/3)^(1/ax)) alpha=0.1 limsup=bx/(alpha^(1/ax)) theta = seq(bx,maxt,0.1) dpareto=Vectorize(function(t){ ax*(bx^ax)*I(t&gt;=bx) / (t^(ax+1)) }) post = dpareto(theta) X = tibble(theta=theta,Posterior=post) ggplot(data=X,mapping = aes(x=theta,y=Posterior)) + geom_line(lwd=1.5) + xlim(c(0,maxt))+ geom_hline(yintercept=dpareto(limsup), lty=2, col=&quot;darkgray&quot;) + geom_area(data=subset(X, theta &lt;= limsup),fill = &quot;blue&quot;, alpha=0.5)+ theme_bw() \\(~\\) \\(~\\) 5.3 Custo das Observações Suponha agora que o custo para observar uma amostra de tamanho \\(n\\) é dado por uma função custo \\(c(n)\\) e, antes de observar \\(X_1,\\ldots,X_n\\), você precisa decidir qual o tamanho amostral ótimo, \\(n^*\\). Desta forma, considere a função de perda \\(L(d,\\theta,n) = L(d,\\theta) + c(n)\\) com risco \\(\\rho_n(\\delta,P)\\) \\(= E\\left[L(d,\\theta,n)\\right]\\) \\(= E\\left[L(d,\\theta) + c(n)\\right]\\) \\(= \\rho_n(\\delta,P) + c(n)\\). Note que (supostamente, por simplicidade) o custo \\(c(n)\\) não depende de \\(\\theta\\). Se \\(\\delta^*\\) é função de decisão de Bayes contra \\(L(d,\\theta)\\) e a priori \\(P\\), o tamanho amostral ótimo \\(n^*\\) é o valor que minimiza \\(\\rho_n(P)\\) \\(= \\rho(\\delta,P) + c(n)\\) \\(= \\rho^*(P) + c(n)\\). \\(~\\) Exemplo. Considere o exemplo visto anteriorimente em que \\(\\mathcal{D}=\\{d_1,d_2\\}\\), \\(\\Theta=\\{\\theta_1,\\theta_2\\}=\\{3/4,1/3\\}\\), \\(P(\\theta_1)=1/2\\) e a função de perda é \\(L(d,\\theta)=10~\\mathbb{I}(d_1,\\theta_2) + 5~\\mathbb{I}(d_2,\\theta_1)\\). Se \\(X|\\theta \\sim Ber(\\theta)\\), a função de decisão de Bayes é \\(\\delta^*(x)=d_1~\\mathbb{I}(x=1)+d_2~\\mathbb{I}(x=0)\\). \\(~\\) Suponha agora que é possível observar \\(X_1,\\ldots,X_n\\) v.a. c.i.i.d. tais que \\(X_i|\\theta \\sim Ber(\\theta)\\). Note que \\(T(\\boldsymbol X)=\\sum X_i\\) é suficiente para \\(\\theta\\) com \\(T(\\boldsymbol X)|\\theta \\sim Bin(n,\\theta)\\) e \\(f(\\theta_1|T(\\boldsymbol X)=t)\\) \\(=\\dfrac{f(t|\\theta_1)P(\\theta_1)}{\\displaystyle \\sum_{i\\in\\{1,2\\}} f(t|\\theta_i)P(\\theta_i)}\\) \\(=\\dfrac{f(t|\\theta_1)}{\\displaystyle \\sum_{i\\in\\{1,2\\}} f(t|\\theta_i)}\\) \\(=\\dfrac{\\binom{n}{t}\\left(\\frac{3}{4}\\right)^t\\left(\\frac{1}{4}\\right)^{n-t}}{\\binom{n}{t}\\left(\\frac{3}{4}\\right)^t\\left(\\frac{1}{4}\\right)^{n-t}+\\binom{n}{t}\\left(\\frac{1}{3}\\right)^t\\left(\\frac{2}{3}\\right)^{n-t}}\\) \\(=\\dfrac{1}{1+\\left(\\frac{1}{6}\\right)^t\\left(\\frac{8}{3}\\right)^{n}}\\) \\(=p_x\\). \\(~\\) O risco posterior das decisões \\(d_1\\) e \\(d_2\\) são, respectivamente, \\(r_x(d_1)=10(1-p_x)\\) e \\(r_x(d_2)=5p_x\\), de modo que decide-se por \\(d_1\\) se \\(r_x(d_1)\\leq r_x(d_2)\\) \\(~\\Longleftrightarrow~ 10(1-p_x) \\leq 5p_x\\) \\(~\\Longleftrightarrow~ p_x \\geq \\frac{10}{15}= \\frac{2}{3}\\) \\(~\\Longleftrightarrow~ \\dfrac{1}{1+\\left(\\frac{1}{6}\\right)^t\\left(\\frac{8}{3}\\right)^{n}} \\geq \\frac{2}{3}\\) \\(~\\Longleftrightarrow~ \\left(\\frac{1}{6}\\right)^t\\left(\\frac{8}{3}\\right)^{n} \\leq \\frac{1}{2}\\) \\(~\\Longleftrightarrow~ \\left(\\frac{1}{6}\\right)^t\\left(\\frac{8}{3}\\right)^{n} \\leq \\frac{1}{2}\\) \\(~\\Longleftrightarrow~ -t\\log(6) \\leq n\\log\\left(\\frac{3}{8}\\right)-\\log\\left(2\\right)\\) \\(~\\Longleftrightarrow~ t\\geq -n\\log_6\\left(\\frac{3}{8}\\right)+\\log_6\\left(2\\right) = k_n\\), e a função de decisão de Bayes é \\({\\delta}^*(\\boldsymbol X) = \\left\\{ \\begin{array}{ccl} d_1 &amp;,&amp; \\sum X_i ~\\geq~ k_n ~~\\approx~~ 0.55 n + 0.39 \\\\ d_2 &amp;,&amp; \\text{caso contrário} \\end{array}\\right.\\) \\(~\\) O risco de Bayes neste caso é \\({\\rho}^*(P)\\) \\(= E\\left[L\\left(\\delta^*(x),\\theta\\right)\\right]\\) \\(=10~P\\left(\\delta^*(x)=d_1,\\theta=\\theta_2\\right)+5~P\\left(\\delta^*(x)=d_2,\\theta=\\theta_1\\right)\\) \\(=10\\dfrac{1}{2}~P\\left(\\sum X_i ~\\geq~ k_n ~\\Big|~ \\theta=\\dfrac{1}{3}\\right)+5~\\dfrac{1}{2}~P\\left(\\sum X_i ~&lt;~ k_n ~\\Big|~ \\theta=\\dfrac{3}{4}\\right)\\) \\(=5~P\\left(Bin\\left(n,\\frac{1}{3}\\right) ~\\geq~ k_n \\right)+2.5~P\\left(Bin\\left(n,\\frac{3}{4}\\right) ~&lt;~ k_n \\right)\\). \\(~\\) Suponha agora que há um custo \\(c(n)\\) por essas \\(n\\) observações e que a função de perda é dada por \\(L(d,\\theta,n) = L(d,\\theta) + c(n)\\). Essa função custo \\(c: \\mathbb{N} \\rightarrow \\mathbb{R}\\) pode depender de questões além das financeiras, como, por exemplo, o tempo de coleta da amostra ou algum risco aos envolvidos no experimento. Considere, por simplicidade, uma função de custo linear \\(c(n) = 0.02n\\), de modo que o risco é \\({\\rho}_n(P)\\) \\(= {\\rho}^*(P) + c(n)\\) \\(=5~P\\left(Bin\\left(n,\\frac{1}{3}\\right) ~\\geq~ k_n \\right)+2.5~P\\left(Bin\\left(n,\\frac{3}{4}\\right) ~&lt;~ k_n \\right) + 0.02n\\). A seguir é apresentado um gráfico desse risco para alguns valores de \\(n\\) e é possível notar que o tamanho amostral ótimo é \\({n}^*=20\\). tibble(n=seq(0,80), kn=-n*log(3/8,6)+log(2,6), risco=5*(1-pbinom(kn,n,1/3))+2.5*pbinom(kn,n,3/4)+0.02*n) %&gt;% ggplot() + geom_point(aes(x=n,y=risco)) + geom_point(aes(x=n[which.min(risco)],y=min(risco)),col=&quot;red&quot;,cex=2.5) \\(~\\) No exemplo anterior, foi apresentado uma maneira de considerar custos das observações e obter um tamanho amostral ótimo para determinado problema de decisão. Quando o custo está relacionado somente a quantidades monetárias, funções de custo lineares não são as mais adequadas. Para uma discussão bastante didática sobre esse problema, veja o artigo O Paradoxo de São Petersburgo (Peixoto, C. M. e Wechsler, S.) no Boletim da ISBrA, 6(2). \\(~\\) "],["Test.html", "6 Testes de Hipóteses 6.1 Conceitos Básicos 6.2 Revisão: Abordagem Frequentista 6.3 Abordagem Bayesiana (via Teoria da Decisão) 6.4 Probabilidade Posterior de \\(H_0\\) 6.5 Fator de Bayes 6.6 Teste de Jeffreys 6.7 Hipóteses Precisas 6.8 FBST - Full Bayesian Significance Test 6.9 P-value - Nivel de Significância Adaptativo", " 6 Testes de Hipóteses 6.1 Conceitos Básicos Uma hipótese estatística é uma afirmação sobre o parâmetro \\(\\theta\\) (ou a família \\(\\mathcal{P}\\)). No caso usual, tem-se duas hipóteses: \\(H_0: ~\\theta\\in\\Theta_0~\\), chamada de hipótese nula, e \\(H_1: ~\\theta\\in\\Theta_1={\\Theta}_0^c~\\), chamada hipótese alternativa. \\(~\\) Um teste de hipótese é uma regra de decisão \\(\\varphi: \\mathfrak{X} \\longrightarrow \\{0,1\\}\\), onde \\(\\varphi(\\boldsymbol{x})=1\\) significa rejeitar \\(H_0\\) (aceitar \\(H_1\\)) e \\(\\varphi(\\boldsymbol x)=0\\), não rejeitar (aceitar) \\(H_0\\). \\(~\\) Se rejeita-se \\(H_0\\) (aceita-se \\(H_1\\)) quando \\(H_0\\) é verdadeira, comete-se um erro do tipo I. Por outro lado, se não rejeita-se \\(H_0\\) (aceita \\(H_0\\)) quando \\(H_0\\) é falso, ocorre um erro do tipo II. \\(~\\) O conjunto \\(\\varphi^{-1}(1)=\\{\\boldsymbol{x} \\in \\mathfrak{X} :~ \\varphi(\\boldsymbol{x})=1\\}\\) recebe o nome de região de rejeição (ou região crítica). A função de poder do teste \\(\\varphi\\) é \\({\\pi}_\\varphi(\\theta)\\) \\(=P\\left(\\varphi^{-1}(1)|\\theta\\right)\\) \\(=P\\big(\\text{&#39;Rejeitar $H_0$&#39;} | \\theta\\big)\\). \\(~\\) Dizemos que um teste \\(\\varphi\\) tem nível de significância \\(\\alpha\\) se \\(\\displaystyle\\sup_{\\theta\\in\\Theta_0}\\pi_\\varphi(\\theta)\\leq \\alpha\\). Se \\(\\alpha=\\displaystyle\\sup_{\\theta\\in\\Theta_0}\\pi_\\varphi(\\theta)\\) dizemos que o teste é de tamanho \\(\\alpha\\). \\(~\\) Uma hipótese é dita simples se contém apenas um ponto, \\(H:\\theta=\\theta_0\\). Caso contrário é chamada de hipótese composta. No caso em que \\(H:\\theta\\in\\Theta_0\\) é tal que \\(\\dim(\\Theta_0)&lt;\\dim(\\Theta)\\), diz-se que \\(H\\) é uma hipótese precisa (sharp). \\(~\\) 6.2 Revisão: Abordagem Frequentista Um teste de hipótese ideal seria aquele que as probabilidades de erros tipo I e tipo II são iguais a zero, isto é, \\(\\pi_\\varphi(\\theta)=0\\), \\(\\forall \\theta \\in \\Theta_0\\), e \\(\\pi_\\varphi(\\theta)=1\\), \\(\\forall \\theta \\in \\Theta_1\\). Contudo, não é possível obter tais testes em geral. A solução usual é fixar um nível de significância \\(\\alpha\\) e considerar apenas a classe de teste de nível \\(\\alpha\\), isto é, testes tais que \\(\\displaystyle\\sup_{\\theta\\in\\Theta_0}\\pi_\\varphi(\\theta) \\leq \\alpha\\). Os testes ótimos sob o ponto de vista frequentista são aqueles na classe de testes de nível alpha que tenha maior função poder \\({\\pi}_\\varphi(\\theta)\\) para \\(\\theta \\in \\Theta_1\\). Um teste que satisfaz isso é chamado de Teste Uniformemente Mais Poderoso (UMP) mas testes com essa propriedade também só podem ser obtidos em casos específicos. \\(~\\) Exemplo. Considere que \\(\\Theta=[0,1]\\) e deseja-se testar \\(H_0: \\theta \\leq 0.5\\) contra \\(H_1: \\theta &gt; 0.5\\). O gráfico a seguir ilustra as funções poder dos quatro testes disponíveis para esse problema. Supondo (apenas para fins didáticos) que \\(\\alpha=0.5\\), temos que o teste UMP de nível \\(\\alpha\\) é o teste 2. O teste 4, apesar de ser mais poderoso, não é um teste de nível \\(\\alpha\\). \\(~\\) Uma das situações onde é possível obter o teste mais poderoso é o caso que as hipóteses nula e alternativa são simples, isto é, \\(H_0:\\theta=\\theta_0\\) contra \\(H_1:\\theta=\\theta_1\\). Nesses casos, pode-se considerar o Lema de Neyman-Pearson, que afirma que o teste mais poderoso é dado por \\({\\varphi}^*(\\boldsymbol x)=\\left\\{\\begin{array}{rl} 1,&amp; \\dfrac{f(x|\\theta_0)}{f(x|\\theta_1)}\\leq k\\\\ 0,&amp; c.c.\\end{array}\\right.~.\\) Além disso, o teste \\({\\varphi}^*\\) minimiza a combinação linear das probabilidades de erro \\(a\\alpha+b\\beta\\) com \\(k=b/a\\), \\(\\alpha={\\pi}_{{\\varphi}^*}(\\theta_0)\\) \\(=P(\\textrm{&#39;erro tipo I&#39;})\\) e \\(\\beta=1-{\\pi}_{{\\varphi}^*}(\\theta_1)\\) \\(=P(\\textrm{&#39;erro tipo II&#39;})\\). \\(~\\) Como dito anteriorimente, usualmente é fixado um nível \\(\\alpha\\) e isso permite encontrar o valor de \\(k\\) de modo que o teste construído a partir do lema é o teste mais poderoso de nível \\(\\alpha\\). Assim, \\(\\alpha\\) \\(={\\pi}_{\\varphi^*}(\\theta_0)\\) \\(=P\\left(\\boldsymbol X \\in {\\varphi}^{-1}(\\{1\\})|\\theta_0\\right)\\) \\(=P\\left(\\left\\{\\boldsymbol x: \\dfrac{f(\\boldsymbol x|\\theta_0)}{f(\\boldsymbol x|\\theta_1)} \\leq k\\right\\}\\Big|\\theta_0\\right)\\). Suponha que foi observado \\(\\boldsymbol X = \\boldsymbol x_o\\), é possível calcular o nível descritivo (ou p-value) da seguinte forma: \\(p(\\boldsymbol x_o)\\) \\(=P\\left(\\left\\{\\boldsymbol x: \\dfrac{f(\\boldsymbol x|\\theta_0)}{f(\\boldsymbol x|\\theta_1)} \\leq \\dfrac{f(\\boldsymbol x_o|\\theta_0)}{f(\\boldsymbol x_o|\\theta_1)}\\right\\}\\Big|\\theta_0\\right)\\) \\(~\\) É possível obter testes UMP em alguns casos particulares. Em especial, nos casos em que a família de distribuições para \\(\\boldsymbol X\\) condicional a \\(\\theta\\) possui a propriedade de razão de verossimilhanças monótona, é possível construir testes UMP para hipóteses do tipo \\(H_1: \\theta \\leq \\theta_0\\) contra \\(H_1: \\theta &gt; \\theta_0\\). Para problemas onde as hipóteses são da forma \\(H_1: \\theta = \\theta_0\\) contra \\(H_1: \\theta \\neq \\theta_0\\), bastante comuns no dia a dia de um estatístico, não existe teste UMP, em geral. \\(~\\) Nos casos em que não existe um teste UMP, o teste mais utilizado sob a abordagem frequentista certamente é o teste da razão de verossimilhança generalizada (RVG). Primeiramente, considere a razão de verossimilhanças generalizada, dada por \\(\\lambda(\\boldsymbol x)=\\dfrac{\\displaystyle \\sup_{\\theta\\in\\Theta_0} f(\\boldsymbol x|\\theta)}{\\displaystyle \\sup_{\\theta\\in\\Theta} f(\\boldsymbol x|\\theta)}\\). Note que \\(0\\leq \\lambda(\\boldsymbol x) \\leq 1\\), \\(\\forall ~\\boldsymbol x \\in \\mathfrak{X}\\) e \\(\\forall ~\\Theta_0 \\subseteq \\Theta\\). Um teste RVG é qualquer teste da forma \\({\\varphi}_{RV}(\\boldsymbol x)=\\left\\{\\begin{array}{rl} 1,&amp; \\lambda(\\boldsymbol x) \\leq k\\\\ 0,&amp; c.c.\\end{array}\\right.~.\\) \\(~\\) Novamente, \\(k\\) pode ser escolhido de modo que o teste resultante seja de nível \\(\\alpha\\), isto é, \\(\\displaystyle \\sup_{\\theta \\in \\Theta_0} P\\left(\\lambda(\\boldsymbol x) \\leq k \\Big | \\theta\\right) \\leq \\alpha\\). Do mesmo modo, se foi observado \\(\\boldsymbol X=\\boldsymbol x_o\\), um p-value é \\(p(\\boldsymbol x_o)\\) \\(=\\displaystyle \\sup_{\\theta \\in \\Theta_0}P\\left(\\left\\{\\boldsymbol x: \\lambda(\\boldsymbol x) \\leq \\lambda(\\boldsymbol x_o)\\right\\}\\big|\\theta\\right)\\). Por fim, em casos onde é difícil fazer os cálculos de forma exata e o tamanho amostral \\(n\\) é razoavelmente grande, é possível usar a distribuição assintótica da RVG \\(~-2\\log \\lambda(\\boldsymbol x)\\overset{\\mathcal{D}}{~\\longrightarrow~}\\chi_d^2\\), onde \\(d=\\dim(\\Theta)-\\dim(\\Theta_0)\\). \\(~\\) \\(~\\) 6.3 Abordagem Bayesiana (via Teoria da Decisão) Sob a abordagem de teoria da decisão, podemos ver um teste de hipóteses como um problema de decisão onde temos duas possíveis decisões \\(\\mathcal{D}=\\{d_0,d_1\\}\\), onde \\(d_0\\) é decidir por \\(H_0:\\theta \\in \\Theta_0\\) e \\(d_1\\) é decidir por \\(H_1:\\theta \\in \\Theta_1\\), com \\(\\Theta=\\Theta_0\\cup\\Theta_1\\). Um teste de hipóteses nesse contexto é uma função de decisão \\(\\varphi: \\mathfrak{X} \\longrightarrow \\{0,1\\}\\), de modo que quando \\(\\varphi(\\boldsymbol x)=i\\), decide-se por \\(d_i\\), \\(i\\in \\{0,1\\}\\). \\(~\\) Primeiramente, considere o contexto apresentado no Lema de Neyman-Pearson, onde \\(\\Theta=\\{\\theta_0,\\theta_1\\}\\) e deseja-se testar \\(H_0: \\theta=\\theta_0\\) contra \\(H_1: \\theta = \\theta_1\\). Considere que, a priori, \\(f(\\theta_0) = \\pi\\), a função de verossimilhança é \\(f(\\boldsymbol x |\\theta)\\) e a função de perda apresentada na tabela a seguir. \\(L(d,\\theta)\\) \\(\\theta_0\\) \\(\\theta_1\\) \\(d_0\\) \\(0\\) \\(b\\) \\(d_1\\) \\(a\\) \\(0\\) Então, o risco de uma função de decisão \\(\\varphi\\) é \\(\\rho(\\varphi,P)\\) \\(=E\\left[L(\\varphi(\\boldsymbol X),\\theta)\\right]\\) \\(= \\pi~E\\left[L(\\varphi(\\boldsymbol X),\\theta)\\big|\\theta_0)\\right] + (1-\\pi)E\\left[L(\\varphi(\\boldsymbol X),\\theta)\\big|\\theta_1)\\right]\\) \\(= a~\\pi~P\\left(\\varphi(\\boldsymbol x)=1\\big|\\theta_0\\right) + b~(1-\\pi)~P\\left(\\varphi(\\boldsymbol x)=0\\big|\\theta_1\\right)\\) \\(= a~\\pi~\\alpha_\\varphi + b~(1-\\pi)~\\beta_\\varphi\\) Como o risco acima é uma combinação linear das probabilidades dos erro tipo I e tipo II, podemos aplicar o Lema de Neyman-Pearson e obter a função de decisão \\(\\varphi^*\\) que minimiza o risco \\({\\varphi}^*(\\boldsymbol x)=\\left\\{\\begin{array}{rl} 1,&amp; \\dfrac{f(x|\\theta_0)}{f(x|\\theta_1)}\\leq \\dfrac{b~(1-\\pi)}{a~\\pi}\\\\ 0,&amp; c.c.\\end{array}\\right.~.\\) Esse resultado é apresentado por DeGroot (1986) e é uma espécie de Lema de Neyman-Pearson Generalizado. \\(~\\) A solução para esse mesmo problema pode também ser obtida usando a forma extensiva. O risco posterior para as suas decisões é \\(r_x(d_0) = b f(\\theta_1|\\boldsymbol x)\\) e \\(r_x(d_1) = a f(\\theta_0|\\boldsymbol x)\\), de modo que rejeitamos \\(H_0\\) (decidimos por \\(d_1\\) ou \\(\\varphi(\\boldsymbol x)=1\\)) se \\(r_x(d_1) \\leq r_x(d_0)\\) \\(\\Longleftrightarrow a f(\\theta_0|\\boldsymbol x) \\leq b f(\\theta_1|\\boldsymbol x)\\) \\(\\Longleftrightarrow a f(\\theta_0|\\boldsymbol x) \\leq b \\left[1-f(\\theta_0|\\boldsymbol x)\\right]\\) \\(\\Longleftrightarrow f(\\theta_0|\\boldsymbol x) \\leq \\dfrac{b}{a+b}\\) De modo que o teste de Bayes também pode ser apresentado como \\({\\varphi}^*(\\boldsymbol x)=\\left\\{\\begin{array}{rl} 1,&amp; f(\\theta_0|\\boldsymbol x) \\leq \\dfrac{b}{a+b} \\\\ 0,&amp; c.c.\\end{array}\\right.~.\\) A interpreção nesse caso é mais direta, a hipótese é rejeitada se sua probabilidade posterior é pequena. Como vimos, essas soluções são equivalentes. De fato, \\(f(\\theta_0|\\boldsymbol x)\\) \\(=\\dfrac{f(\\theta_0)f(\\boldsymbol x|\\theta_0)}{f(\\boldsymbol x)}\\) \\(=\\pi~\\dfrac{f(\\boldsymbol x|\\theta_0)}{f(\\boldsymbol x)}\\); \\(f(\\theta_1|\\boldsymbol x)\\) \\(=\\dfrac{f(\\theta_1)f(\\boldsymbol x|\\theta_1)}{f(\\boldsymbol x)}\\) \\(=(1-\\pi)~\\dfrac{f(\\boldsymbol x|\\theta_1)}{f(\\boldsymbol x)}\\). Assim, \\(r_x(d_1) \\leq r_x(d_0)\\) \\(\\Longleftrightarrow a \\pi~\\dfrac{f(\\boldsymbol x|\\theta_0)}{f(\\boldsymbol x)} \\leq b (1-\\pi)~\\dfrac{f(\\boldsymbol x|\\theta_1)}{f(\\boldsymbol x)}\\) \\(\\Longleftrightarrow \\dfrac{f(\\boldsymbol x|\\theta_0)}{f(\\boldsymbol x|\\theta_1)} \\leq \\dfrac{b (1-\\pi)}{a \\pi}\\). \\(~\\) \\(~\\) Considere agora um caso mais geral, onde \\(\\Theta=\\Theta_0 \\dot{\\cup} \\Theta_1\\) e deseja-se testar \\(H_0: \\theta \\in \\Theta_0\\) contra \\(H_1: \\theta \\in \\Theta_1\\). Considere também a função de perda mais geral apresentada a seguir, com \\(a_0 \\leq a_1\\) e \\(b_0 \\leq b_1\\). \\(L(d,\\theta)\\) \\(\\Theta_0\\) \\(\\Theta_1\\) \\(d_0\\) \\(a_0\\) \\(b_1\\) \\(d_1\\) \\(a_1\\) \\(b_0\\) O risco posterior de cada uma das decisões é \\(r_x(d_0,\\theta)\\) \\(=a_0P(\\theta\\in\\Theta_0|x)+b_1P(\\Theta_1|x)\\) \\(=a_0P(\\theta\\in\\Theta_0|x)+b_1\\left[1- P(\\Theta_0|x)\\right]\\) \\(=a_0P(\\theta\\in\\Theta_0|x)+b_1-b_1P(\\Theta_0|x)\\), \\(r_x(d_1,\\theta)\\) \\(=a_1P(\\theta\\in\\Theta_0|x)+b_0-b_0P(\\Theta_0|x)\\), De modo que rejeita-se \\(H_0\\), \\(\\varphi(\\boldsymbol x)=1\\), se \\(r_x(d_1,P)\\leq r_x(d_0,P)\\) \\(\\Leftrightarrow (a_1-b_0)P(\\Theta_0|x)+b_0 \\leq (a_0-b_1)P(\\Theta_0|x)+b_1\\) \\(\\Leftrightarrow P(\\Theta_0|x) \\leq \\dfrac{(b_1-b_0)}{(a_1-a_0)+(b_1-b_0)}\\) Assim, o teste de bayes nesse caso é \\({\\varphi}(\\boldsymbol x)=\\left\\{\\begin{array}{rl} 1,&amp; P(\\Theta_0|x) \\leq \\dfrac{(b_1-b_0)}{(a_1-a_0)+(b_1-b_0)} \\\\ 0,&amp; c.c.\\end{array}\\right.~.\\) \\(~\\) \\(~\\) 6.4 Probabilidade Posterior de \\(H_0\\) Resultado. Seja \\(\\Theta=\\Theta_0 \\dot{\\cup} \\Theta_1\\) e suponha que deseja-se testar \\(H_0: \\theta \\in \\Theta_0\\) contra \\(H_1: \\theta \\in \\Theta_1\\) considerando a função de perda a seguir, com \\(a_0 \\leq a_1\\) e \\(b_0 \\leq b_1\\). \\(L(d,\\theta)\\) \\(\\Theta_0\\) \\(\\Theta_1\\) \\(d_0\\) \\(a_0\\) \\(b_1\\) \\(d_1\\) \\(a_1\\) \\(b_0\\) Então, o teste de bayes é \\({\\varphi}(\\boldsymbol x)=\\left\\{\\begin{array}{rl} 1,&amp; P(\\Theta_0|x) \\leq \\dfrac{(b_1-b_0)}{(a_1-a_0)+(b_1-b_0)} \\\\ 0,&amp; c.c.\\end{array}\\right.\\). \\(~\\) Exemplo 1. Considere \\(X_1,\\ldots,X_n\\) c.i.i.d. tais que \\(X_i|\\theta \\sim Ber(\\theta)\\) com \\(\\Theta = \\left\\{1/2,3/4\\right\\}\\). Suponha que, a priori, \\(f(\\theta=1/2)=f(\\theta=3/4)=1/2\\) e deseja-se testar \\(H_0: \\theta=1/2\\) contra \\(H_1: \\theta=3/4\\). Tem-se que \\(T(\\boldsymbol X)=\\sum X_i~|~\\theta\\sim Bin(n,\\theta)\\) é uma estatística suficiente para \\(\\theta\\). Então, \\(P(\\theta=1/2|T(\\boldsymbol X)=t)\\) \\(=\\dfrac{f(t|\\theta=1/2)f(\\theta=1/2)}{\\displaystyle\\sum_{\\theta \\in \\left\\{1/2~,~3/4\\right\\}} f(t|\\theta)f(\\theta)}\\) \\(=\\dfrac{\\displaystyle\\binom{n}{t}\\left(\\dfrac{1}{2}\\right)^n}{\\displaystyle\\binom{n}{t}\\left(\\dfrac{1}{2}\\right)^n+\\binom{n}{t}\\left(\\dfrac{3}{4}\\right)^t\\left(\\dfrac{1}{4}\\right)^{n-t}}\\) \\(=\\dfrac{1}{1+\\dfrac{3^t}{2^n}}\\). \\(~\\) Considere a função de perda \\(L(d,\\theta) = a_0~\\mathbb{I}(d_0,\\Theta_0) + b_1~\\mathbb{I}(d_0,\\Theta_1) + a_1~\\mathbb{I}(d_1,\\Theta_0) + b_0~\\mathbb{I}(d_1,\\Theta_1)\\) como no resultado anterior. Então, rejeita-se \\(H_0\\) se \\(P(\\theta \\in \\Theta_0 | \\boldsymbol x) &lt; K\\), com \\(K = \\dfrac{b_1-b_0}{(a_1-a_0)+(b_1-b_0)}\\). Assim, \\(P(\\theta=1/2|T=t)\\leq K\\) \\(~\\Longleftrightarrow~ \\dfrac{1}{1+\\frac{3^t}{2^n}} \\leq K\\) \\(~\\Longleftrightarrow~ {1+\\dfrac{3^t}{2^n}} \\geq \\dfrac{1}{K}\\) \\(~\\Longleftrightarrow~ 3^t\\geq 2^n\\left(\\dfrac{1}{K}-1\\right)\\) \\(~\\Longleftrightarrow~ t\\geq n\\log_3(2)+\\log_3\\left(\\dfrac{1-K}{K}\\right)\\) \\(~\\Longleftrightarrow~ t\\geq nlog_3(2)+log_3\\left(\\dfrac{a_1-a_0}{b_1-b_0}\\right)\\). \\(~\\) Tomando \\(a_1=b_1=1\\) e \\(a_0=b_0=0\\), rejeita \\(H\\) se \\(\\sum X_i \\geq n\\log_3(2)+\\log_3(1)\\) \\(~\\Longleftrightarrow~ \\sum X_i \\geq n\\log_3(2)\\) \\(~\\Longrightarrow~ \\bar{X} \\geq \\log_3(2)\\approx 0,631\\). \\(~\\) O teste de Bayes é \\({\\varphi}(\\boldsymbol x) =\\left\\{\\begin{array}{rl} 1,&amp; f(\\theta=1/2|\\boldsymbol x) \\leq \\dfrac{(b_1-b_0)}{(a_1-a_0)+(b_1-b_0)} = \\dfrac{1}{2} \\\\ 0,&amp; c.c.\\end{array}\\right.\\) \\(~\\Longrightarrow~ {\\varphi}(\\boldsymbol x) =\\left\\{\\begin{array}{rl} 1,&amp; \\bar{X} \\geq \\log_3(2) \\\\ 0,&amp; c.c.\\end{array}\\right.\\). \\(~\\) \\(~\\) Exemplo 2: \\(X_1,...,X_n\\) c.i.i.d. tais que \\(X_i|\\theta\\sim N(\\theta,{\\sigma}_0^2)\\) com \\({\\sigma}_0^2\\) conhecido. Suponha que, a priori, \\(\\theta \\sim N(m,v^2)\\) e \\(\\bar{X}\\) é estatística suficiente para \\(\\theta\\) com \\(\\bar{X}|\\theta \\sim \\left(\\theta,{\\sigma}_0^2/n\\right)\\), de modo que \\(\\theta|\\boldsymbol X \\sim N\\left(\\dfrac{{\\sigma}_0^2~m+nv^2~\\bar x}{{\\sigma}_0^2+nv^2}~,~\\dfrac{{\\sigma}_0^2~v^2}{{\\sigma}_0^2+nv^2}\\right)\\). Suponha ainda que o objetivo é testar \\(H_0: \\theta\\leq \\theta_0\\) contra \\(H_1:\\theta &gt; \\theta_0\\). \\(~\\) Utilizando novamente o resultado anterior, temos \\(P\\left(\\theta\\in\\Theta_0|\\boldsymbol x\\right)\\) \\(= P\\left(\\theta \\leq \\theta_0|\\boldsymbol x\\right)\\) \\(= P\\left(Z\\leq\\dfrac{\\theta_0-\\frac{{\\sigma}_0^2~m+nv^2~\\bar x}{{\\sigma}_0^2+nv^2}}{\\sqrt{\\frac{{\\sigma}_0^2~v^2}{{\\sigma}_0^2+nv^2}}}~\\Bigg|~\\bar x\\right)\\) \\(= \\Phi\\left(\\dfrac{\\theta_0-\\frac{{\\sigma}_0^2~m+nv^2~\\bar x}{{\\sigma}_0^2+nv^2}}{\\sqrt{\\frac{{\\sigma}_0^2~v^2}{{\\sigma}_ 0^2+nv^2}}}\\right)\\) \\(= \\Phi\\left(\\dfrac{({\\sigma}_0^2+nv^2)\\theta_0-{\\sigma}_0^2~m-nv^2~\\bar x}{{\\sigma}_0~v~\\sqrt{{\\sigma}_ 0^2+nv^2}}\\right)\\), e deve-se rejeitar \\(H_0\\) se \\(P\\left(\\theta\\in\\Theta_0|\\boldsymbol x\\right) \\leq \\dfrac{(b_1-b_0)}{(a_1-a_0)+(b_1-b_0)} = K\\) \\(~\\Longleftrightarrow~\\Phi\\left(\\dfrac{({\\sigma}_0^2+nv^2)\\theta_0-{\\sigma}_0^2~m-nv^2~\\bar x}{{\\sigma}_0~v~\\sqrt{{\\sigma}_ 0^2+nv^2}}\\right) \\leq K\\) \\(~\\Longleftrightarrow~ \\bar x ~\\geq~ \\dfrac{({\\sigma}_0^2+nv^2)\\theta_0-{\\sigma}_0^2~m}{nv^2} - {\\Phi}^{-1}(K)\\dfrac{{\\sigma}_0~\\sqrt{{\\sigma}_ 0^2+nv^2}}{nv}\\) \\(~\\Longleftrightarrow~ \\bar x ~\\geq~ \\dfrac{{\\sigma}_0^2(\\theta_0-m)+nv^2\\theta_0}{nv^2} - {\\Phi}^{-1}(K)\\dfrac{{\\sigma}_0~\\sqrt{{\\sigma}_ 0^2+nv^2}}{nv}\\). \\(~\\) Se \\(a_0=b_0=0\\) e \\(a_1=b_1=1\\), então \\(\\Phi^{-1}(K=1/2)=0\\) e rejeita-se \\(H_0\\) se \\(\\bar x ~\\geq~ \\dfrac{{\\sigma}_0^2(\\theta_0-m)+nv^2\\theta_0}{nv^2} ~\\underset{n\\uparrow\\infty}{\\longrightarrow}~ \\theta_0\\). \\(~\\) \\(~\\) 6.5 Fator de Bayes Voltando ao resultado, tem-se que rejeita-se \\(H_0\\) se \\(r_x(d_0) \\geq r_x(d_1)\\) \\(~\\Longleftrightarrow~ a_0P(\\Theta_0|\\boldsymbol x)+b_1P(\\Theta_1|\\boldsymbol x) \\geq a_1P(\\Theta_0|\\boldsymbol x)+b_0P(\\Theta_1|\\boldsymbol x)\\) \\(~\\Longleftrightarrow~ \\dfrac{P(\\Theta_0|\\boldsymbol x)}{P(\\Theta_1|\\boldsymbol x)}\\leq\\dfrac{b_1-b_0}{a_1-a_0}\\) \\(~\\Longleftrightarrow~ BF(\\boldsymbol x)\\) \\(=\\dfrac{\\frac{P(\\Theta_0|\\boldsymbol x)}{P(\\Theta_1|\\boldsymbol x)}}{\\frac{P(\\Theta_0)}{P(\\Theta_1)}}\\) \\(=\\dfrac{\\frac{P(\\Theta_0|\\boldsymbol x)}{P(\\Theta_0)}}{\\frac{P(\\Theta_1|\\boldsymbol x)}{P(\\Theta_1)}}\\) \\(=\\dfrac{f(\\boldsymbol x | \\Theta_0)}{f(\\boldsymbol x | \\Theta_1)}\\) \\(\\leq \\dfrac{(b_1-b_0)}{(a_1-a_0)}\\dfrac{P(\\Theta_1)}{P(\\Theta_0)}\\), onde \\(BF\\) é o Fator de Bayes, frequentemente utilizado na literatura bayesiana para testar hipóteses. Ele pode ser visto como uma razão de chances que representa o aumento na chance da hipótese nula ser mais plausível que a hipótese alternativa após observar os dados em relação a sua opinião a priori. O \\(BF\\) também pode ser reescrito como \\(BF(\\boldsymbol x)\\) \\(=\\dfrac{f_0(\\boldsymbol x)}{f_1(\\boldsymbol x)}\\) \\(=\\dfrac{f(\\boldsymbol x | \\Theta_0)}{f(\\boldsymbol x | \\Theta_1)}\\) \\(=\\dfrac{\\displaystyle \\int_{\\Theta}f(\\boldsymbol x|\\theta) f(\\theta|\\Theta_0) d\\theta} {\\displaystyle \\int_{\\Theta} f(\\boldsymbol x|\\theta)f(\\theta|\\Theta_1)d\\theta}\\) \\(=\\dfrac{\\displaystyle \\int_{\\Theta_0}f(x|\\theta)dP_0(\\theta)}{\\displaystyle \\int_{\\Theta_1}f(x|\\theta)dP_1(\\theta)}\\) \\(=\\dfrac{E\\left[f(\\boldsymbol x|\\theta)|\\theta\\in\\Theta_0\\right]}{E\\left[f(\\boldsymbol x|\\theta)|\\theta \\in \\Theta_1\\right]}\\). \\(~\\) No exemplo 1. Lembrando que, a priori, \\(P(\\theta=1/2)=P(\\theta=3/4)=1/2\\) e considerando novamente \\(a_0=b_0=0\\) e \\(a_1=b_1=1\\), temos que devemos rejeitar \\(H_0\\) se \\(BF(\\boldsymbol x)&lt;\\frac{(b_1-b_0)}{(a_1-a_0)}\\frac{P(\\theta=1/2)}{P(\\theta=3/4)}=1\\). Então, \\(BF(\\boldsymbol x)=\\dfrac{P(\\theta=1/2|\\boldsymbol x)}{P(\\theta=3/4|\\boldsymbol x)}\\dfrac{1/2}{1/2}\\) \\(=\\dfrac{\\frac{1}{1+3^t/2^n}}{\\frac{3^t/2^n}{1+3^t/2^n}}\\) \\(=\\dfrac{2^n}{3^t}\\leq 1\\) \\(~\\Longleftrightarrow~ \\bar{x} \\geq \\log_3(2)\\), de modo que a decisão baseada no fator de Bayes concorda com o resultado baseado na probabilidade a posteriori da hipótese. \\(~\\) No exemplo 2. \\(\\theta|\\boldsymbol X \\sim N\\left(\\dfrac{{\\sigma}_0^2~m+nv^2~\\bar x}{{\\sigma}_0^2+nv^2}~,~\\dfrac{{\\sigma}_0^2~v^2}{{\\sigma}_0^2+nv^2}\\right)\\) e o objetivo é testar \\(H_0: \\theta\\leq \\theta_0\\) contra \\(H_1:\\theta &gt; \\theta_0\\). A probabilidade a posteriori da hipótese \\(H_0\\) é \\(P\\left(\\theta\\in\\Theta_0|\\boldsymbol x\\right)\\) \\(= \\Phi\\left(\\dfrac{({\\sigma}_0^2+nv^2)\\theta_0-{\\sigma}_0^2~m-nv^2~\\bar x}{{\\sigma}_0~v~\\sqrt{{\\sigma}_ 0^2+nv^2}}\\right)\\), e, a priori, \\(P(\\theta \\in \\Theta_0)\\) \\(=P(\\theta \\leq \\theta_0)\\) \\(= \\Phi\\left(\\dfrac{\\theta_0-m}{v}\\right)\\), de modo que o fator de Bayes nesse caso é \\(BF(\\boldsymbol x)\\) \\(= \\dfrac{P(\\Theta_0|\\boldsymbol x)}{P(\\Theta_1|\\boldsymbol x)} \\dfrac{P(\\Theta_1)}{P(\\Theta_0)}\\) \\(= \\dfrac{\\Phi\\left(\\frac{({\\sigma}_0^2+nv^2)\\theta_0-{\\sigma}_0^2~m-nv^2~\\bar x}{{\\sigma}_0~v~\\sqrt{{\\sigma}_0^2+nv^2}}\\right)}{\\left[1-\\Phi\\left(\\frac{({\\sigma}_0^2+nv^2)\\theta_0-{\\sigma}_0^2~m-nv^2~\\bar x}{{\\sigma}_0~v~\\sqrt{{\\sigma}_0^2+nv^2}}\\right)\\right]} ~ \\dfrac{\\left[1-\\Phi\\left(\\frac{\\theta_0-m}{v}\\right)\\right]}{\\Phi\\left(\\frac{\\theta_0-m}{v}\\right)}\\). \\(~\\) m=0; v2=1 # Média e Variância da Priori sigma02=1 # Variância Populacional (conhecido) n=3 # tamanho amostral theta0=1 # H0: theta &lt;= theta0 a0=0; b0=0; a1=1; b1=1 # Função de Perda K1=(b1-b0)/(a1-a0+b1-b0) # corte Prob. Posterior K2=((b1-b0)*(1-pnorm((theta0-m)/sqrt(v2)))) / ((a1-a0)*pnorm((theta0-m)/sqrt(v2))) #corte Fator de Bayes K3=((sigma02+n*v2)*theta0-sigma02*m)/(n*v2) - qnorm(K1)*sqrt(sigma02*(sigma02+n*v2))/(n*sqrt(v2)) # corte xbar # Probabilidade a Posteriori de H0 (como função de Xbar) postH = function(xbar){ pnorm(((sigma02 + n*v2)*theta0 - sigma02*m - n*v2*xbar)/ sqrt(sigma02*v2*(sigma02+n*v2))) } # Fator de Bayes de H0 (como função de Xbar) bf = function(xbar){ (postH(xbar)*(1-pnorm((theta0-m)/sqrt(v2))))/ ((1-postH(xbar))*pnorm((theta0-m)/sqrt(v2))) } xbar=seq(0.5,2.5,0.001) PP=postH(xbar) BF=bf(xbar) FS=(max(PP)-min(PP))/(max(BF)-min(BF)) # var. aux. para transformção dos eixos tibble(xbar,PP,BF) %&gt;% ggplot() + geom_line(aes(x=xbar,y=PP,colour=&quot;Prob. Posterior&quot;)) + geom_line(aes(x=xbar,y=((BF-min(BF))*FS+min(PP)),colour=&quot;Fator de Bayes&quot;))+ scale_y_continuous(sec.axis = sec_axis(~./FS-min(PP)/FS+min(BF), name = &quot;BF&quot;))+ geom_hline(aes(yintercept=K1),lty=2, col=&quot;darkgrey&quot;) + geom_point(aes(x=K3,y=K1,colour=&quot;Prob. Posterior&quot;)) + geom_hline(aes(yintercept=((K2-min(BF))*FS+min(PP))),lty=2, col=&quot;darkgrey&quot;) + geom_point(aes(x=K3,y=((K2-min(BF))*FS+min(PP)),colour=&quot;Fator de Bayes&quot;)) + geom_vline(aes(xintercept=K3),lty=2, col=&quot;darkgrey&quot;) + theme_bw() + labs(colour = &quot;&quot;) Nesse exemplo, é possível ver que tanto o Fator de Bayes quanto a probabilidade posterior da hipótese nula ordenam o espaço amostral, representado aqui pela estatística suficiente, \\(\\bar{X}\\). Deste modo, quanto menores os valores dessas estatísticas de teste, mais desfavorável é o ponto amostral para a hipótese nula. Como visto anteriormente, as regras de decisão baseadas nessas estatísticas são equivalentes e, portanto, a ordenação do espaço amostral é a mesma. \\(~\\) \\(~\\) Problema: Suponha agora que, nesse mesmo exemplo, deseja-se testar \\(H_0:\\theta=0\\) contra \\(H_1: \\theta\\neq 0\\). Como a posteriori é Normal, temos que \\(P(\\theta \\in \\Theta_0|\\boldsymbol x)\\) \\(=P(\\theta=0|\\boldsymbol x)=0\\) , \\(\\forall~ \\boldsymbol x\\in \\mathfrak{X}~\\) e, desta forma, a hipótese nula \\(H_0\\) sempre é rejeitada. De fato, para qualquer cenário em que a hipótese \\(H_0\\) tem medida nula a priori, \\(P(\\theta \\in \\Theta_0)=0\\), tem-se que, a posteriori, \\(P(\\theta \\in \\Theta_0|\\boldsymbol x)=0\\). Isso ocorre frequentemente nos casos em que \\(H_0\\) é uma hipótese precisa, isto é, \\(dim(\\Theta_0)&lt;dim(\\Theta)\\). Neste cenário, é necessário definir procedimentos alternativos para testar hipóteses. \\(~\\) \\(~\\) 6.6 Teste de Jeffreys O teste de Jeffreys (1961?) consiste em atribuir uma probabilidade positiva para o conjunto que define a hipótese nula, \\(p_0=P(\\theta \\in \\Theta_0)&gt;0\\). \\(~\\) Exemplo 2. Suponha que deseja-se testar \\(H_0: \\theta=0\\) contra \\(H_1: \\theta\\neq 0\\). Suponha que sua opinião a priori é \\(\\theta \\sim Normal(0,2)\\). Contudo, já foi visto que \\(P(\\theta=0|\\boldsymbol x)=0\\), \\(\\forall \\boldsymbol x \\in \\mathfrak{X}\\). Deste modo, você opta por atribuir uma probabilidade positiva \\(p_0=0.2\\) para o ponto \\(\\theta=0\\), ou seja, você vai considerar uma distribuição mista \\(f(\\theta)=p_0\\mathbb{I}(\\theta=0)+(1-p_0)f_N(\\theta)\\), onde \\(f_N\\) é a densidade da \\(Normal(0,1)\\). Sua função de distribuição a prioi, \\(F(\\theta)=p_0~\\mathbb{I}(\\theta\\geq0)+(1-p_0)~\\Phi\\left(\\theta/\\sqrt{2}\\right)\\), está representada no gráfico a seguir. theta=c(seq(-5,-0.001,0.001),seq(0.001,5,0.001)) p=0.2 pprior = function(t){ p*I(t&gt;=0)+(1-p)*pnorm(t,0,2)*I(t!=0) } tibble(theta,Prior=pprior(theta)) %&gt;% ggplot()+geom_line(aes(x=theta,y=Prior))+ geom_point(aes(x=0,y=(1-p)*pnorm(0,0,2)))+ geom_point(aes(x=0,y=(1-p)*pnorm(0,0,2)+p)) Exercício. Calcule \\(P(\\theta=0|\\boldsymbol X=\\boldsymbol x)\\). \\(~\\) Exemplo 3. Seja \\(X_1,\\ldots,X_n\\) c.i.i.d. tais que \\(X_i|\\theta \\sim Ber (\\theta)\\) e considere que, a priori, \\(\\theta\\sim Beta(a,b)\\). Como \\(X=\\sum X_i\\) é estatística suficiente com \\(X\\big|\\theta \\sim Bin(n,\\theta)\\), tem-se que \\(\\theta\\big|x=\\sum x_i\\sim Beta\\left(a+\\sum x_i,b+n-\\sum x_i\\right)\\). A distribuição marginal de \\(X\\) é chamada distribuição preditiva a priori e pode ser calculada por \\(f(x)\\) \\(=\\displaystyle \\int_0^1 f(x,\\theta)d\\theta\\) \\(=\\displaystyle \\int_0^1 f(x|\\theta)f(\\theta)d\\theta\\) \\(=\\displaystyle \\binom{n}{x}~\\dfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}~\\int_0^1 \\theta^{a+x-1}(1-\\theta)^{b+n-x-1}d\\theta\\) \\(=\\displaystyle \\binom{n}{x}~\\dfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}~\\dfrac{\\Gamma(a+x)\\Gamma(b+n-x)}{\\Gamma(a+b+n)}\\) \\(=\\displaystyle \\binom{n}{x} \\dfrac{\\beta(a+x,b+n-x)}{\\beta(a,b)}~\\mathbb{I}_{\\{0,\\ldots,n\\}}(x)\\) \\(\\Longrightarrow X \\sim Beta-Binomial(n,a,b)\\). \\(~\\) Suponha agora que deseja-se testar \\(H_0: \\theta=\\theta_0\\) contra \\(H_1:\\theta\\neq \\theta_0\\), com \\(\\theta_0=1/2\\), utilizando o teste de Jeffreys. Desta forma, considere \\(p_0=P(\\theta=1/2)=1/2\\) e sua priori de Jeffreys é \\(f_J(\\theta)=p_0~\\mathbb{I}(\\theta=\\theta_0) +(1-p_0)f_\\beta(\\theta)~\\mathbb{I}(\\theta\\neq\\theta_0)\\), onde \\(f_\\beta\\) é a densidade da \\(Beta(a,b)\\). A distribuição preditiva com relação a priori \\(f_J\\) é \\(f_J(x)\\) \\(=\\displaystyle p_0f(x|\\theta_0)~\\mathbb{I}(\\theta=\\theta_0)+ (1-p_0)\\overbrace{\\int_0^1f(x|\\theta)f_\\beta(\\theta)~\\mathbb{I}(\\theta\\neq\\theta_0)d\\theta}^{f(x)}\\) \\(=\\displaystyle p_0\\binom{n}{x}{\\theta}_0^x(1-\\theta_0)^{n-x}~\\mathbb{I}(\\theta=\\theta_0) + (1-p_0)\\binom{n}{x}\\dfrac{\\beta(a+x,b+n-x)}{\\beta(a,b)}~\\mathbb{I}(\\theta\\neq\\theta_0)\\), de modo que a distribuição a posteriori é \\(f_J(\\theta| x)\\) \\(= \\dfrac{f( x|\\theta)f_J(\\theta)}{f_J(x)}\\) \\(= \\dfrac{p_0\\binom{n}{x} (1/2)^n}{f_J(x)}~\\mathbb{I}(\\theta=1/2) +\\dfrac{(1-p_0)\\binom{n}{x}\\theta^{a+x-1}(1-\\theta)^{b+n-x-1}}{\\beta(a,b)~f_J(x)}~\\mathbb I(\\theta\\neq 1/2)\\). \\(~\\) A probabilidade posterior da hipótese \\(H_0:\\theta=1/2\\) é \\(p_x=P(\\theta=1/2|x)\\) \\(=\\dfrac{p_0\\binom{n}{x}(1/2)^n}{p_0\\binom{n}{x}(1/2)^n+(1-p_0)\\binom{n}{x}\\frac{\\beta(a+x,b+n-x)}{\\beta(a,b)}}\\) \\(=\\dfrac{1}{1+\\dfrac{(1-p_0)}{p_0}\\dfrac{\\beta(a+x,b+n-x)}{(1/2)^n\\beta(a,b)}}\\). E, assim, o Fator de Bayes é dado por \\(B_j(x)=\\dfrac{\\dfrac{p_x}{1-p_x}}{\\dfrac{p_0}{1-p_0}}\\) \\(=\\dfrac{\\dfrac{1}{\\dfrac{(1-p_0)}{p_0}\\dfrac{\\beta(a+x,b+n-x)}{(1/2)^n\\beta(a,b)}}}{\\dfrac{p_0}{(1-p_0)}}\\) \\(=\\dfrac{(1/2)^n\\beta(a,b)}{\\beta(a+x,b+n-x)}\\). Note que, nesse caso, \\(BF(x)\\) não depende da probabilidade a priori \\(p_0\\) da hipótese \\(H_0\\). theta0=1/2 n=6; p=1/2 a=1;b=1 x=seq(0,n) # Fator de Bayes para cada x BF=(theta0^x)*((1-theta0)^(n-x))*beta(a,b)/beta(a+x,b+n-x) # Probabilidade a posteriori para cada x PP=(1 + (((1-p)*beta(a+x,b+n-x))/(p*(theta0^x)*((1-theta0)^(n-x))*beta(a,b))) )^(-1) tab=t(tibble(BF=round(BF,4),PP=round(PP,4))) colnames(tab)=x kable(tab, booktabs=TRUE, escape=FALSE) 0 1 2 3 4 5 6 BF 0.1094 0.6562 1.6406 2.1875 1.6406 0.6562 0.1094 PP 0.0986 0.3962 0.6213 0.6863 0.6213 0.3962 0.0986 Na tabela acima, são calculados \\(P(\\theta=1/2|x)\\) e \\(BF(x)\\) para cada \\(x\\) com \\(n=6\\), \\(p_0=1/2\\) e os parâmetros da Beta sendo \\(a=b=1\\). Considerando \\(a_0=b_0=0\\) e \\(a_1=b_1=1\\), os valores de corte para a probabilidade a posteriori e o \\(BF\\) são, respectivamente, \\(1/2\\) e \\(1\\). Desta forma, rejeita-se a hipótese nula para os valores extremos \\(\\left\\{0,1,5,6\\right\\}\\). \\(~\\) \\(~\\) 6.7 Hipóteses Precisas Probabilidade a posteriori da hipótese \\(H_0\\), \\(P(\\Theta_0|\\boldsymbol x)\\). Fator de Bayes \\(BF(\\boldsymbol x)\\). No caso absolutamente contínuo, quando \\(H_0\\) é hipótese precisa, \\(P(\\Theta_0|\\boldsymbol x)=0\\). Isso faz com que os testes anteriores sempre levem à rejeição de \\(H_0\\). Primeira alternativa: teste de Jeffreys. Problema: a priori deve dar probabilidade positiva à hipótese nula, conduzindo assim a uma priori artificial (mista). Serão apresentados dois procedimentos alternativos de teste: FBST e P-value. O primeiro deles foi pensado especificamente para hipóteses precisas \\(\\left(dim(\\Theta_0)&lt;dim(\\Theta)\\right)\\) mas ambos podem ser aplicados para hipóteses gerais. \\(~\\) \\(~\\) 6.8 FBST - Full Bayesian Significance Test Essa solução foi apresentada por Pereira e Stern em 1999. Suponha que o objetivo é testar \\(H_0:\\theta\\in\\Theta_0\\) contra \\(H_1:\\theta \\in \\Theta_1=\\Theta_0^c\\). Seja \\(T_x=\\left\\{\\theta\\in\\Theta ~:~f(\\theta|\\boldsymbol x)\\geq \\underset{\\theta\\in\\Theta_0}{sup}f(\\theta|\\boldsymbol x)\\right\\}\\) a região tangente à hipóteses \\(H_0\\), formada pelos pontos densidade posterior maior ou igual que qualquer ponto da hipótese nula. Se esse conjunto é grande (muito provável), a hipótese nula está em uma região de pouca densidade posterior e deve ser rejeitada. Assim, a medida de evidência (de Pereira-Stern) ou e-value é definido por \\(Ev(\\Theta_0,\\boldsymbol x)\\) \\(=1-P\\left(\\theta\\in T_x \\big|\\boldsymbol x \\right)\\), e deve-se rejeitar \\(H_0\\) se o e-value for pequeno. \\(~\\) Exemplo. \\(X_1,...,X_n\\) c.i.i.d. \\(N(\\theta,{\\sigma}_0^2)\\), com \\({\\sigma}_0^2\\) conhecido. Novamente, considere \\(\\theta\\sim N(m,v^2)\\), de modo que \\(\\theta|\\boldsymbol x \\sim N\\left(\\dfrac{{\\sigma}_0^2m+nv^2\\bar{x}}{{\\sigma}_0^2+nv^2},\\dfrac{{\\sigma}_0^2v^2}{{\\sigma}_0^2+nv^2}\\right)\\) e denote a média e a variância da posteriori por \\(M_x\\) e \\(V_x\\), respectivamente. Suponha que o interesse é testar \\(H_0:\\theta=\\theta_0\\) contra \\(H_1:\\theta\\neq\\theta_0\\). Sem perda de generalidade, suponha que \\(M_x \\geq \\theta_0\\). Então, como a normal é simétrica em torno de \\(M_x\\), a região tangente é da forma \\(T_x=[\\theta_0,2M_x-\\theta_0]\\). Note que quanto mais próximo \\(M_x\\) está de \\(\\theta_0\\), menor a região \\(T_x\\) e, portanto, maior o valor da evidência em favor de \\(H_0\\). O valor da evidência pode ser calculado por \\(Ev(\\Theta_0,x)=\\) \\(1-P\\left(\\theta_0\\leq \\theta\\leq2M-\\theta_0|x\\right)=\\) \\(1-P\\left(\\dfrac{\\theta_0-M}{\\sqrt V}\\leq Z\\leq \\dfrac{2M-\\theta_0-M}{\\sqrt V}|x\\right)=\\) \\(2\\Phi\\left(-\\dfrac{|\\theta_0-M|}{\\sqrt V}\\right)=\\) \\(2\\Phi\\left(-\\dfrac{\\left|\\dfrac{{\\sigma}_0^2m+nv^2\\bar{x}}{{\\sigma}_0^2+nv^2}-\\theta_0\\right|}{\\dfrac{{\\sigma}_0 v}{\\sqrt{{\\sigma}_0^2+nv^2}}}\\right)=\\) \\(2\\Phi\\left(-\\dfrac{\\sqrt{{\\sigma}_0^2+nv^2}}{{\\sigma}_0 v}\\dfrac{|{\\sigma}_0^2(m-\\theta_0)+nv^2(\\bar x-\\theta_0)|}{\\sqrt{{\\sigma}_0^2+nv^2}}\\right)=\\) \\(2\\Phi\\left(-\\dfrac{1}{\\sqrt{{\\sigma}_0^2+nv^2}}\\left|\\dfrac{{\\sigma}_0}{v}(m-\\theta_0)+\\dfrac{\\sqrt n v}{{\\sigma}_0}(\\bar x-\\theta_0)\\right|\\right)=\\) \\(2\\Phi\\left(-\\dfrac{1}{\\sqrt{{\\sigma}_0^2+nv^2}}\\left|\\dfrac{(m-\\theta_0)}{v/{\\sigma}_0}+\\sqrt nv\\dfrac{(\\bar x-\\theta_0)}{{\\sigma}_0/\\sqrt n}\\right|\\right)\\) \\(~\\) Sob a abordagem frequentista, temos que o p-value é \\(p(\\boldsymbol x)\\) \\(= 1-P\\left(-\\dfrac{|\\bar X-\\theta_0|}{{\\sigma}_0/\\sqrt n}\\leq Z \\leq \\dfrac{|\\bar X-\\theta_0|}{{\\sigma}_0/\\sqrt n}\\right)\\) \\(=2\\Phi\\left(-\\dfrac{|\\bar X-\\theta_0|}{{\\sigma}_0/\\sqrt n}\\right)\\) \\(\\Longleftrightarrow -\\dfrac{|\\bar X-\\theta_0|}{{\\sigma}_0/\\sqrt n}=\\Phi^{-1}\\left(\\dfrac{p-valor}{2}\\right)\\), de modo que, nesse exemplo, podemos escrever \\(Ev(\\Theta_0,x)=\\) \\(2\\Phi\\left(-\\dfrac{1}{\\sqrt{{\\sigma}_0^2+nv^2}}~\\left|\\dfrac{(m-\\theta_0)}{v/{\\sigma}_0}+\\sqrt nv~\\Phi\\left(\\dfrac{p(\\boldsymbol x)}{2}\\right)\\right|\\right)\\). A seguir, são apresentados gráficos do e-value e do p-value como função de \\(\\bar{x}\\) e do e-value como função do p-value usando da relação anterior. sigma02=4 m=0; v2=1 theta0 = 0 n=3 p=seq(0,1,length.out=5000) ep = function(p){ 2*pnorm(-abs(sqrt(sigma02/v2)*(m-theta0) + sqrt(n*v2)*qnorm(p/2))/ sqrt(sigma02+n*v2)) } graf=tibble(pv=p,ev=ep(p)) %&gt;% ggplot() + geom_line(aes(x=pv,y=ev),lwd=1.5) + geom_segment(x=0.05,xend=0.05,y=0,yend=round(ep(0.05),2),lty=2) + geom_segment(x=0,xend=0.05,y=round(ep(0.05),2),yend=round(ep(0.05),2),lty=2) + scale_y_continuous(breaks=c(0.00,round(ep(0.05),2),0.25,0.50,0.75,1.00)) + scale_x_continuous(breaks=c(0.00,0.05,0.25,0.50,0.75,1.00)) + theme_bw() if(knitr::is_latex_output()){ graf } else { plotly::ggplotly(graf) } Suponha que um estatístico frequentista decida rejeitar \\(H_0\\) se o p-value for menor que 0.05. Para que a decisão baseada no e-value concorde com o resultado frequentista (para esse particular exemplo), deve-se rejeitar a hipótese se o e-value for menor que 0.2, aproximadamente. Quando a variância da priori ou o tamanho amostral aumentam, os valores dessas duas medidas se aproximam. \\(~\\) Resultados Assintóticos (para esse exemplo) Suponha que \\(H_0: \\theta=\\theta_0\\) seja falso, isto é, o verdadeiro valor do parâmetro é \\(\\theta^* \\neq \\theta_0\\). Quando \\(n\\uparrow\\infty\\), pela Lei dos Grandes Números, \\(\\bar{X} ~\\overset{q.c.}{\\longrightarrow}~ \\theta^*\\) \\(~\\Longrightarrow~ \\dfrac{\\sqrt{n}|\\bar{X}-\\theta_0|}{{\\sigma}_0} ~{\\longrightarrow}~ +\\infty\\) \\(~\\Longrightarrow~ p(\\boldsymbol X)= 2\\Phi\\left(-\\dfrac{\\sqrt{n}|\\bar{X}-\\theta_0|}{{\\sigma}_0}\\right) ~{\\longrightarrow}~ 0\\), com probabilidade 1. Por outro lado, sob \\(H_0: \\theta=\\theta_0\\), pelo Teorema Central do Limite, \\(\\dfrac{\\sqrt{n}(\\bar{X}-\\theta_0)}{{\\sigma}_0} ~\\overset{\\mathcal{D}}\\longrightarrow~ Z \\sim N(0,1)\\) \\(~\\Longrightarrow~ p(\\boldsymbol X)= 2\\Phi\\left(-\\dfrac{\\sqrt{n}|\\bar{X}-\\theta_0|}{{\\sigma}_0}\\right) ~\\overset{\\mathcal{D}}\\longrightarrow~ U=2\\Phi\\left(-|Z|\\right) \\sim Unif(0,1)\\). \\(~\\) Esses resultados para o p-value são bastante conhecidos. No contexto desse exemplo, é possível obter resultados similares para o e-value. Novamente, considere que \\(H_0\\) é falso e, sem perda de generalidade, \\(\\theta=\\theta^*&gt;\\theta_0\\). Note que \\(\\dfrac{{\\sigma}_0(m-\\theta_0)}{v\\sqrt{{\\sigma}_0^2+nv^2}} ~\\longrightarrow~ 0\\) e, pela LGN, \\(\\bar{X} ~\\overset{q.c.}{\\longrightarrow}~ \\theta^*\\) \\(~\\Longrightarrow~ (\\bar{X}-\\theta_0) ~{\\longrightarrow}~ (\\theta^*-\\theta_0) &gt;0\\) \\(~\\Longrightarrow~ \\dfrac{nv(\\bar{X}-\\theta_0)}{{\\sigma}_0^2\\sqrt{{\\sigma}_0^2+nv^2}} ~{\\longrightarrow}~ +\\infty\\) \\(~\\Longrightarrow~ Ev(\\Theta_0,\\boldsymbol X)=2\\Phi\\left(-\\left|\\dfrac{{\\sigma}_0(m-\\theta_0)}{v{\\sqrt{{\\sigma}_0^2+nv^2}}}+\\dfrac{nv(\\bar x-\\theta_0)}{{\\sigma}_0{\\sqrt{{\\sigma}_0^2+nv^2}}}\\right|\\right) ~{\\longrightarrow}~ 2\\Phi\\left(-\\infty\\right)=0\\). Além disso, \\(\\dfrac{v\\sqrt{n}}{\\sqrt{{\\sigma}_0^2+nv^2}} ~\\longrightarrow~ 1\\) e, sob \\(H_0: \\theta = \\theta_0\\), \\(\\dfrac{v\\sqrt{n}}{\\sqrt{{\\sigma}_0^2+nv^2}}\\dfrac{\\sqrt{n}(\\bar{X}-\\theta_0)}{{\\sigma}_0} ~\\overset{\\mathcal{D}}\\longrightarrow~ Z \\sim N(0,1)\\), de modo que \\(Ev(\\Theta_0,\\boldsymbol{X})=2\\Phi\\left(-\\left|\\dfrac{{\\sigma}_0(m-\\theta_0)}{v{\\sqrt{{\\sigma}_0^2+nv^2}}}+\\dfrac{nv(\\bar x-\\theta_0)}{{\\sigma}_0{\\sqrt{{\\sigma}_0^2+nv^2}}}\\right|\\right) ~{\\longrightarrow}~ 2\\Phi\\left(-|Z|\\right)\\sim Unif(0,1)\\). Esse resultado pode não valer em outros contextos. Por exemplo, quando \\(dim(\\Theta_0)\\geq 2\\), a distribuição de \\(Ev(\\Theta_0,\\boldsymbol{X})\\) sob \\(H_0\\) não é \\(Unif(0,1)\\), em geral. \\(~\\) \\(~\\) 6.9 P-value - Nivel de Significância Adaptativo Recentemente, o p-value e a utilização do famoso nível \\(\\alpha=0.05\\) têm sido muito questionados, não apenas na área de testes de hipóteses mas na ciência como um todo. A ideia de fixar um nível de significância é que a probabilidade de erro tipo I fica controlada e a probabilidade do erro tipo II diminui quanto maior o tamanho amostral. Por essa razão, é comum nas áreas de planejamento de experimentos e amostragem, o cálculo do tamanho amostral para um determinado estudo. Infelizmente, na maior parte dos problemas do dia a dia de um estatístico, não há um planejamento cuidadoso ou as amostras disponíveis são amostras de conveniência. Simultaneamente, com a revolução da informação, a quantidade de dados disponível é cada vez maior. A consequência disso no cenário de testes de hipóteses é que os testes ficam muito poderosos e há uma tendência maior de rejeitar a hipótese nula. \\(~\\) Exemplo. Seja \\(X_1,\\ldots,X_n\\) c.i.i.d. tais que \\(X_i|\\theta \\sim Ber(\\theta)\\), \\(\\theta \\sim Beta(a,b)\\) de modo que \\(\\theta|x=\\sum x_i \\sim Beta(a+x,b+n-x)\\) e suponha que deseja-se testar \\(H_0: \\theta= 1/2\\). A seguir, para diferentes tamanhos amostrais \\(n\\) e supondo que em todos os casos \\(\\bar{x}_n=0.55\\), são apresentados os testes para esse caso vistos até aqui: p-value do teste RVG, probabilidade posterior e \\(BF\\) do teste de Jeffreys e e-value do FBST. a=1; b=1 p=0.5 alpha=0.05 theta0=0.5 xbar=0.55 N=c(1,5,10,20,30,40,50,100,150,300,seq(500,2000,250)) p_v=Vectorize(function(n){ x=n*xbar l = c(min(x,n-x),max(x,n-x)) pbinom(l[1],n,theta0) + 1-pbinom(l[2],n,theta0) }) Nalpha=seq(max(N[(p_v(N)-alpha)&gt;0]),min(N[(p_v(N)-alpha)&lt;0])) Nalpha=Nalpha[which.min(abs(p_v(Nalpha)-alpha))] bf=Vectorize(function(n){ x=n*xbar # exp(log(BF)) exp(x*log(theta0) + (n-x)*log(1-theta0) + lbeta(a,b) - lbeta(a+x,b+n-x)) }) prob_post=Vectorize(function(n){ x=n*xbar l = log(1-p)+lbeta(a+x,b+n-x)-log(p)-x*log(theta0)-(n-x)*log(1-theta0)-lbeta(a,b) 1/(1+exp(l)) }) e_v=Vectorize(function(n){ x=n*xbar f_Tx=function(t){ dbeta(t,a+x,b+n-x)-dbeta(theta0,a+x,b+n-x) } moda=(a+x-1)/(a+b+n-2) if(theta0==moda){ return(1) } if(theta0&lt;moda){ Tx=c(theta0,uniroot(f=f_Tx,lower=moda,upper=1)$root) }else{ Tx=c(uniroot(f=f_Tx,lower=0,upper=moda)$root,theta0) } pbeta(Tx[1],a+x,b+n-x)+1-pbeta(Tx[2],a+x,b+n-x) }) Dados=tibble(n=rep(N,4), Evidencia=c(p_v(N),prob_post(N),bf(N),e_v(N)), Estatistica=factor(rep(c(&quot;p-value&quot;,&quot;Prob. Post.&quot;,&quot;BF&quot;,&quot;e-value&quot;), each=length(N)),levels=c(&quot;Prob. Post.&quot;,&quot;BF&quot;,&quot;e-value&quot;,&quot;p-value&quot;)), corte=c(p_v(Nalpha),rep(NA,4*length(N)-1))) ggplot(Dados)+ geom_line(aes(x=n,y=Evidencia, colour=Estatistica),lwd=1.5)+ geom_vline(xintercept=Nalpha,lty=2,col=&quot;darkgrey&quot;)+ facet_wrap(~Estatistica, scales=&quot;free_y&quot;)+ geom_hline(data=subset(Dados,Estatística=&quot;p-value&quot;),aes(yintercept=corte), lty=2, col=&quot;darkgrey&quot;)+ theme_bw() Note que todas as medidas de suporte da hipótese tendem a zero conforme aumenta o tamanho amostral. Isso indica que se \\(|\\bar{x}_n-\\theta_0|=\\varepsilon\\) e for fixado um valor de corte para essas medidas que não dependa do tamanho amostral (por exemplo, o \\(\\alpha=0.05\\) para o p-value), existe um \\(n^*\\) tal que \\(H_0\\) será rejeitado para todo \\(n\\geq n^*\\). No gráfico, a linha vertical tracejada indica o valor \\(n^*\\) para o p-value considerando o corte \\(\\alpha=0.05\\). \\(~\\) Como visto anteriormente, o Lema de Neyman-Pearson Generalizado (DeGroot, 1986) garante que os testes Bayesianos (baseados na probabilidade posterior ou no BF) minimizam \\(a\\alpha + b\\beta\\). Baseado nesse resultado, O professor Carlos A. B. Pereira recentemente propôs um novo procedimento de teste para evitar o problema descrito anteriormente. Seja \\(BF(\\boldsymbol x) = \\dfrac{f_0(\\boldsymbol x)}{f_1(\\boldsymbol x)}\\) \\(=\\dfrac{\\displaystyle\\int_{\\Theta_0} f(\\boldsymbol x|\\theta) dP_0(\\theta)}{\\displaystyle \\int_{\\Theta_1} f(\\boldsymbol x|\\theta) dP_1(\\theta)}\\), onde \\(P_i\\) é a medida de probabilidade a priori para \\(\\theta\\) restrito à hipótese \\(H_i\\), \\(i=0,1\\); Defina \\(\\alpha_n = P\\left(\\left\\{\\boldsymbol x : BF(\\boldsymbol x)\\leq\\dfrac{b}{a}\\right\\} ~\\Big|~ \\Theta_0\\right)\\) e \\(\\beta_n = P\\left(\\left\\{\\boldsymbol x : BF(\\boldsymbol x)&gt;\\dfrac{b}{a}\\right\\} ~\\Big|~ \\Theta_1\\right)\\); Suponha que foi observado \\(\\boldsymbol X=\\boldsymbol x_o\\). O P-value é dado por \\(\\text{P-value}(\\boldsymbol x_o) = P\\left(\\left\\{\\boldsymbol x : BF(\\boldsymbol x)\\leq BF(\\boldsymbol x_o)\\right\\} ~\\Big|~ \\Theta_0\\right)\\); O procedimento de teste consiste em rejeitar \\(H_0\\) se \\(\\text{P-value}(\\boldsymbol x) &lt; \\alpha_n\\). \\(~\\) Voltando ao Exemplo. As distribuções preditivas sob \\(H_0: \\theta=\\theta_0=1/2\\) e \\(H_1:\\theta \\neq 1/2\\) são \\(f_0(\\boldsymbol x)\\) \\(= f(\\boldsymbol x | \\theta_0)\\) \\(=\\displaystyle \\binom{n}{x} {\\theta}_0^x (1-{\\theta}_0)^{n-x}\\) \\(=\\displaystyle \\binom{n}{x} {(1/2)^{n}}\\) ; \\(f_1(\\boldsymbol x)\\) \\(=\\displaystyle \\int_{\\Theta\\setminus\\theta_0} f(\\boldsymbol x|\\theta) f(\\theta) d\\theta\\) \\(=\\displaystyle \\int_0^1 \\binom{n}{x} \\dfrac{{\\theta}_0^{a+x-1} (1-{\\theta}_0)^{b+n-x-1}}{\\beta(a,b)}~d\\theta\\) \\(=\\displaystyle \\binom{n}{x}\\dfrac{\\beta(a+x,b+n-x)}{\\beta(a,b)}\\) . Deste modo, \\(BF(\\boldsymbol x)\\) \\(=\\dfrac{f_0(\\boldsymbol x)}{f_1(\\boldsymbol x)}\\) \\(=\\dfrac{\\beta(a,b)~{\\theta}_0^x (1-{\\theta}_0)^{n-x}}{\\beta(a+x,b+n-x)}\\) \\(=\\dfrac{\\beta(a,b)~(1/2)^{n}}{\\beta(a+x,b+n-x)}\\), e, assim, \\(\\alpha_n = \\displaystyle {(1/2)^{n}} \\sum_{\\left\\{\\boldsymbol x: BF(\\boldsymbol x)\\leq\\frac{b}{a}\\right\\}}\\binom{n}{x}\\), \\(\\beta_n = \\displaystyle \\dfrac{1}{{\\beta(a,b)}} \\sum_{\\left\\{\\boldsymbol x: BF(\\boldsymbol x)&gt;\\frac{b}{a}\\right\\}}\\binom{n}{x}\\beta(a+x,b+n-x)\\), \\(\\text{P-value}(\\boldsymbol x_0) = \\displaystyle {(1/2)^{n}} \\sum_{\\left\\{\\boldsymbol x: BF(\\boldsymbol x)\\leq BF(\\boldsymbol x_o)\\right\\}}\\binom{n}{x}\\) \\(~\\) Supondo novamente que foi observado \\(\\bar{x}=0.55\\), o gráfico abaixo apresenta esses valores para diversos tamanhos amostrais. a=1; b=1 a1=1; b1=1 p=0.5 alpha=0.05 theta0=0.5 xbar=0.55 N=c(2,5,seq(10,150,10),seq(150,1000,50)) bf=Vectorize(function(x,n){ exp(x*log(theta0) + (n-x)*log(1-theta0) + lbeta(a,b) - lbeta(a+x,b+n-x)) }, vectorize.args = c(&quot;x&quot;)) alphaN = Vectorize(function(n){ x=n*xbar s=seq(0,n) s=s[bf(s,n)&lt;=b1/a1] (0.5)^n*sum(choose(n,s)) }) betaN = Vectorize(function(n){ x=n*xbar s=seq(0,n) s=s[bf(s,n)&gt;b1/a1] sum(extraDistr::dbbinom(s,n,a,b)) }) P_v=Vectorize(function(n){ x=n*xbar s=seq(0,n) s=s[bf(s,n)&lt;=bf(x,n)] (0.5)^n*sum(choose(n,s)) }) Dados=tibble(n=N, alpha=alphaN(N), beta=betaN(N), Pvalue=P_v(N)) ggplot(Dados)+ geom_line(aes(x=n,y=Pvalue, colour=&quot;P-value&quot;),lwd=1.2)+ geom_line(aes(x=n,y=alpha, colour=&quot;alpha&quot;),lwd=1.2)+ geom_line(aes(x=n,y=beta, colour=&quot;beta&quot;),lwd=1.2)+ geom_line(aes(x=n,y=alpha+beta, colour=&quot;alpha+beta&quot;),lwd=1.2)+ theme_bw() + labs(colour=&quot;&quot;) \\(~\\) \\(~\\) "],["Comp.html", "7 Métodos Computacionais 7.1 Método de Monte Carlo 7.2 Monte Carlo com Amostragem de Importância 7.3 Método de Rejeição 7.4 ABC (Aproximated Bayesian Computation) 7.5 MCMC - Monte Carlo via Cadeias de Markov 7.6 Bibliotecas de R para Inferência Bayesiana", " 7 Métodos Computacionais Como visto, a inferência Bayesiana é baseada na aplicação monótona do teorema de Bayes \\(f(\\theta|\\boldsymbol x)=\\dfrac{f(\\boldsymbol x|\\theta)f(\\theta)}{\\displaystyle\\int_\\Theta f(\\boldsymbol x|\\theta)f(\\theta)d\\theta}\\) \\(= c(\\boldsymbol x) f(\\boldsymbol x|\\theta)f(\\theta)\\) \\(\\propto f(\\boldsymbol x|\\theta)f(\\theta)\\), e na obtenção de medidas resumo dessa distribuição, como \\(E[\\theta|\\boldsymbol x]\\), regiões HPD ou probabilidades a posteriori. A maior dificuldade na aplicação de Inferência Bayesiana está justamente no cálculo das integrais envolvidas, tanto no cálculo de \\(f(\\boldsymbol x)\\) para a obtenção da posteriori, quanto na obtenção das medidas resumos citadas anteriormente. Devido a isso, a inferência bayesiana ganhou muito força com o avanço computacional das últimas décadas. A seguir, serão apresentados um breve resumo de alguns recursos que podem ser utilizados na prática Bayesiana. Muitos dos métodos descritos baseiam-se na Lei dos Grande Números (LGN) e são uma bela aplicação de ideias frequentistas em um cenário controlado onde as suposições de i.i.d. são satisfeitas. \\(~\\) Lei \\(\\overset{\\textbf{(Fraca)}}{\\textbf{Forte}}\\) dos Grande Números. Seja \\(X_1,X_2...\\) uma sequência de v.a. i.i.d com \\(E[X_1]=\\mu\\) e \\(Var[X_1]=\\sigma^2&lt;\\infty\\), então \\(\\dfrac{1}{n}\\displaystyle \\sum_{i=1}^n X_i ~~\\underset{q.c.}{\\overset{P}{\\longrightarrow}}~~ E[X_1]=\\mu\\). \\(~\\) As integrais de interesse aqui serão escritas como o valor esperado de funções de variáveis aleatórias, isto é, \\(\\displaystyle \\int g(x) dP(x) = E\\left[g(X)\\right]\\). Deste modo, suponha que \\(X_1,X_2...\\) é uma sequência de v.a. i.i.d e \\(g:\\mathbb{R} \\longrightarrow\\mathbb{R}\\) é uma função (mensurável) tal que \\(Var\\left[g(X_1)\\right]&lt;\\infty\\). Então, pela LGN, \\(\\dfrac{1}{n}\\displaystyle \\sum_{i=1}^n g(X_i) ~\\longrightarrow~ E\\left[g(X_1)\\right]\\) \\(~\\) 7.1 Método de Monte Carlo Suponha que deseja-se calcular \\(\\displaystyle\\int_\\Theta g(\\theta)f(\\theta|\\boldsymbol x)d\\theta=E\\left[g(\\theta)|\\boldsymbol x\\right]\\) e é possível simular realizações \\(\\theta_1,...,\\theta_m\\) da distribuição de \\(\\theta |\\boldsymbol X=\\boldsymbol x\\) , \\(f(\\theta | \\boldsymbol x)\\). Então, a integral acima pode ser aproximada por \\(\\displaystyle \\dfrac{1}{m}\\sum_{i=1}^m g(\\theta_i)\\) A precisão da aproximação é usualmente estimada pelo erro padrão da estimativa \\(\\displaystyle EP\\left[\\dfrac{1}{m}\\sum_{i=1}^m g(\\theta_i)\\right]\\) \\(\\approx \\displaystyle \\sqrt{\\dfrac{1}{m}\\left(\\dfrac{1}{m}\\sum_{i=1}^m\\Big[g(\\theta_i)\\Big]^2-\\left[\\dfrac{1}{m}\\sum_{j=1}^mg(\\theta_j)\\right]^2\\right)}\\) \\(~\\) Exemplo 1. Suponha que deseja-se estimar o número \\(\\pi\\) usando o método de Monte Carlo. Considere então que o v.a. \\((X,Y)\\) tem distribuição uniforme em um quadrado centrado na origem, \\(\\mathfrak{X}=[-1,1]\\times[-1,1]\\), e um círculo \\(A\\) de raio \\(1\\) inscrito nesse quadrado, \\(x^2+y^2\\leq 1.\\) Como a distribuição é uniforme no quadrado, a probabilidade de escolher um ponto no círculo é \\(P(A)\\) \\(=\\dfrac{\\text{área da círculo}}{\\text{área do quadrado}}\\) \\(=\\dfrac{\\pi}{4}\\) \\(= \\displaystyle\\int_A f(x,y) dxdy\\) \\(= \\displaystyle\\int_{\\mathfrak{X}} \\mathbb{I}_A(x,y)~\\dfrac{1}{4}~dxdy\\) \\(=E\\left[\\mathbb{I}_A(X,Y)\\right]~.\\) Suponha que é possível gerar uma amostra \\(\\left\\{(x_1,y_1),\\ldots,(x_m,y_m)\\right\\}\\) de \\((X,Y)\\), de modo que podemos aproximar o valor de \\(\\pi\\) por \\(\\pi\\) \\(=4~P(A)\\) \\(=E\\left[4~\\mathbb{I}_A(X,Y)\\right]\\) \\(\\displaystyle \\approx \\dfrac{1}{m}\\sum_{i=1}^m 4~\\mathbb{I}_A(x_i,y_i)\\), e, denotando por \\(\\displaystyle t=\\sum_{i=1}^m ~\\mathbb{I}_A(x_i,y_i)\\), o erro estimado é \\(\\displaystyle \\sqrt{\\dfrac{1}{m}\\left(\\dfrac{1}{m}\\sum_{i=1}^m\\Big[4~\\mathbb{I}(x_i,y_i)\\Big]^2-\\left[\\dfrac{1}{m}\\sum_{j=1}^m 4~\\mathbb{I}(x_i,y_i)\\right]^2\\right)}\\) \\(=\\displaystyle \\sqrt{\\dfrac{1}{m}\\left(\\dfrac{16}{m}~t-\\left[\\dfrac{4}{m}~t\\right]^2\\right)}\\) \\(= \\displaystyle \\sqrt{\\dfrac{16}{m} \\dfrac{t}{m}\\left(1-\\dfrac{t}{m}\\right)}\\) \\(\\leq \\displaystyle \\sqrt{\\dfrac{16}{m}~\\dfrac{1}{4}}\\) \\(= \\dfrac{2}{\\sqrt{m}}~.\\) set.seed(666) M = 1000 # número de iterações df = tibble(t = 1:M, x = runif(length(t), -1, 1), y = runif(length(t), -1, 1)) %&gt;% mutate(Circ=ifelse(x^2+y^2&lt;=1,1,0), pi_est=round(4*cumsum(Circ)/t,4), erro=round(abs(pi-pi_est),4), erro_est=round(sqrt((cumsum(16*Circ)/t-pi_est^2)/t),4)) p &lt;- ggplot() + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank()) + ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = &quot;black&quot;) + geom_rect(aes(xmin = -1, ymin = -1, xmax = 1, ymax = 1), color = &quot;black&quot;, alpha = 0) + guides(color = FALSE) + geom_point(data = df, aes(x = x, y = y, colour = Circ), size = 3) + gganimate::transition_manual(t, cumulative = TRUE) p + labs(title = expression(paste(&quot;Método de Monte-Carlo para a estimação do &quot;,pi)), subtitle = &quot;m = {df$t[frame]} ; pi_est = 4 * ({cumsum(df$Circ)[frame]} / {df$t[frame]}) = {df$pi_est[frame]} ; erro = {df$erro[frame]} ; erro_est = {df$erro_est[frame]}&quot;) \\(~\\) Exemplo 2. Suponha que você não sabe que \\[\\displaystyle \\int_0^1 x^3(1-x)^5e^xdx = 74046 - 27240e\\approx0.0029928\\] e deseja estimar o resultado usando o método de Monte Carlo. Assim, considere as duas propostas a seguir 1. \\(U \\sim Unif (0,1)\\) e a integral pode ser escrita como \\(E\\left[U^3(1-U)^5e^U\\right]\\); 2. \\(Y \\sim Beta(4,6)\\) de modo que \\(\\displaystyle \\int_0^1 y^3(1-y)^5e^y dy\\) \\(=\\beta(4,6)\\displaystyle \\int_0^1 e^y~~\\frac{y^{4-1}(1-y)^{6-1}}{\\beta(4,6)}~dy\\) \\(=\\beta(4,6)E\\left[e^Y\\right]\\). \\(~\\) Exemplo 3. Região HPD Suponha que \\(\\boldsymbol \\theta=(\\mu,\\sigma^2) \\sim \\textit{Normal-Inv.Gama}(m,v,a,b)\\) e deseja-se obter estimativas pontuais e por região para \\(\\boldsymbol \\theta\\). \\(~\\) Se não houver um simulador da distribuição Normal-Inv.Gamma diretamente, é possível gerar um ponto \\(\\boldsymbol \\theta_i=(\\mu_i,\\sigma_i)\\) tomando \\({\\sigma}_i^2 \\sim \\textit{Inv.Gama}(a,b)\\) (ou \\(\\tau_i \\sim \\textit{Gama}(a,b)\\) e fazer \\({\\sigma}_i^2=1/\\tau_i\\)) e \\(\\mu_i \\sim \\textit{Normal}(m,{\\sigma}_i^2/v)\\). Nesse exemplo é fácil simular uma amostra da distribuição posterior e é possível obter estimativas pontuais simplesmente obtendo estatística resumo da amostra simulada, como média, moda, mediana, variância e desvio padrão. \\(~\\) Para construir a região HPD, primeiramente note que \\(R=\\left\\{\\boldsymbol{\\theta}\\in\\Theta :~f(\\boldsymbol{\\theta} | \\boldsymbol{x})\\geq h)\\right\\}=\\left\\{\\boldsymbol{\\theta}\\in\\Theta:~f(\\boldsymbol x|\\boldsymbol{\\theta})f(\\boldsymbol{\\theta})\\geq h^*=c\\cdot h\\right\\}\\), de modo que não é necessário conhecer a constante \\(c=f(\\boldsymbol{x})\\) para realizar essa tarefa. Como nesse exemplo a distribuição a posteriori é conhecida e fácil de ser simulada, considere o algorítmo a seguir para estimar a região HPD de probabilidade \\(\\gamma\\). \\(~\\) 1. Simular \\(\\boldsymbol{\\theta}_1,...,\\boldsymbol{\\theta}_m\\) de \\(f(\\boldsymbol{\\theta}|\\boldsymbol{x})\\); 2. Encontrar \\(h\\) tal que \\(\\displaystyle \\dfrac{1}{m}\\sum_{i=1}^m\\mathbb{I}_{R}(\\boldsymbol{\\theta}_i)=\\dfrac{1}{m}\\sum_{i=1}^m\\mathbb{I}(f(\\boldsymbol{\\theta}_i|\\boldsymbol{x})\\geq h)\\approx \\gamma\\) \\(~~~\\) i. Calcule \\(f(\\boldsymbol{\\theta}_i|\\boldsymbol{x})\\), \\(i=1,\\ldots,m\\); \\(~~~\\) ii. Ordene esses valores e tome \\(h\\) como o percentil de ordem \\(\\gamma\\); 3. Fazer o gráfico da curva de nível \\(f(\\boldsymbol{\\theta}|\\boldsymbol{x})=h\\). set.seed(666) a=7; b=7; m=0; v=0.5 # parametros da posteriori M=10000 # No. de simulações dpost=Vectorize(function(t1,t2){ #densidade posterior extraDistr::dinvgamma(t2,a,b)*dnorm(t1,m,sqrt(t2/v))}) # simulações df = tibble(sigma2=extraDistr::rinvgamma(M,a,b)) %&gt;% mutate(mu=rnorm(M,m,sqrt(sigma2/v))) #summarytools::dfSummary(df, graph.magnif = 0.75, valid.col = FALSE, na.col = FALSE) summarytools::dfSummary(df, plain.ascii = FALSE, style = &quot;grid&quot;, graph.magnif = 0.75, valid.col = FALSE, na.col = FALSE, varnumbers = FALSE, headings = FALSE, tmp.img.dir = &quot;./tmp&quot;) Variable Stats / Values Freqs (% of Valid) Graph sigma2 [numeric] Mean (sd) : 1.2 (0.5) min &lt; med &lt; max: 0.3 &lt; 1.1 &lt; 7.3 IQR (CV) : 0.6 (0.4) 10000 distinct values mu [numeric] Mean (sd) : 0 (1.5) min &lt; med &lt; max: -7.3 &lt; 0 &lt; 6.1 IQR (CV) : 2 (-253.7) 10000 distinct values df = df %&gt;% mutate(post=dpost(mu,sigma2)) # variáveis para os gráficos gama=c(0.99,0.95,0.9,0.8,0.5,0.3,0.1) # prob das regiões l=quantile(df$post,1-gama) d=100 x=seq(-4*extraDistr::qinvgamma(0.5,a,b)/v,4*extraDistr::qinvgamma(0.5,a,b)/v,length.out = d) y=seq(0,extraDistr::qinvgamma(0.996,a,b),length.out = d) z=matrix(apply(cbind(rep(x,d),rep(y,each=d)),1,function(t){dpost(t[1],t[2])}),ncol=d) # gráfico da posteriori plotly::plot_ly(alpha=0.1) %&gt;% plotly::add_surface(x=x, y=y, z=t(z), showscale = FALSE, colorscale=list(c(0,&#39;green&#39;),c(1/4,&#39;yellow&#39;),c(2/4,&#39;orange&#39;),c(3/4,&#39;red&#39;), c(1,&#39;darkred&#39;))) #colorscale = list(c(0,&#39;#BA52ED&#39;), c(1,&#39;#FCB040&#39;))) # gráfico das regiões HPD de prob. gama=c(0.99,0.95,0.9,0.8,0.5,0.3,0.1) tibble(x1=rep(x,d),y1=rep(y,each=d),z1=as.vector(z)) %&gt;% arrange(z1) %&gt;% mutate(p=1-(cumsum(z1)/sum(z1))) %&gt;% ggplot(aes(x1,y1,z=z1,fill = p)) + geom_raster(interpolate = TRUE) + jcolors::scale_fill_jcolors_contin(&quot;pal3&quot;) + #scale_fill_distiller(palette = &quot;YlOrRd&quot;) + geom_contour(breaks=l,col=&quot;black&quot;) + xlab(expression(mu)) + ylab(expression(sigma^2)) \\(~\\) 7.2 Monte Carlo com Amostragem de Importância Considere \\(f(\\theta|\\boldsymbol x)\\propto f(\\boldsymbol x| \\theta)f(\\theta)\\) e suponha que não se sabe simular observações desta distribuição mas tem-se interesse na quantidade \\(E\\left[g(\\theta)|\\boldsymbol x\\right]=\\displaystyle\\int_\\Theta g(\\theta)f(\\theta| \\boldsymbol x)d\\theta\\). Suponha também que existe uma distribuição \\(h(\\theta)\\) que seja uma aproximação para \\(f(\\theta|\\boldsymbol x)\\) (preferencialmente com caudas mais pesadas) da qual sabe-se simular. Então, \\(E\\left[g(\\theta)| \\boldsymbol x\\right]\\) \\(=\\displaystyle\\int_\\Theta g(\\theta)f(\\theta|\\boldsymbol x)d\\theta\\) \\(=\\dfrac{\\displaystyle \\int_\\Theta g(\\theta)f(\\boldsymbol x|\\theta)f(\\theta)d\\theta}{\\displaystyle\\int_\\theta f(\\boldsymbol x|\\theta)f(\\theta)d\\theta}\\) \\(=\\dfrac{\\displaystyle \\int g(\\theta)\\left(\\frac{f(\\boldsymbol x|\\theta)f(\\theta)}{h(\\theta)}\\right)h(\\theta)d\\theta}{\\displaystyle\\int\\left(\\frac{f(\\boldsymbol x|\\theta)f(\\theta)}{h(\\theta)}\\right)h(\\theta)d\\theta}\\) \\(=\\dfrac{\\displaystyle\\int g(\\theta)w(\\theta)h(\\theta)d\\theta}{\\displaystyle\\int w(\\theta)h(\\theta)d\\theta}\\) \\(\\approx \\displaystyle\\sum_{i=1}^m \\dfrac{w_i}{\\sum_{j=1}^mw_j}~g(\\theta_i)\\), onde \\(w_i=w(\\theta_i)=\\dfrac{f(\\boldsymbol x|\\theta_i)f(\\theta_i)}{h(\\theta_i)}\\). \\(~\\) 7.3 Método de Rejeição Considere novamente que o objetivo é simular observações de \\(f(\\theta|\\boldsymbol x)\\) mas não é possível fazer isso diretamente. Por outro lado, sabe-se simular dados de uma distribuição candidata, \\(h(\\theta)\\), tal que \\(f(\\theta|\\boldsymbol x)\\leq Mh(\\theta)\\), \\(\\forall \\theta \\in \\Theta\\) e para alguma constante \\(M\\). A ideia do método é rejeitar pontos gerados em regiões em que \\(h\\) atribui maior probabilidade que \\(f\\) com probabilidade \\(1-\\left[f(\\theta|\\boldsymbol x)~/~Mh(\\theta)\\right]\\). Para que a afirmação anterior faça sentido, \\(M\\) deve ser tal que \\(f(\\theta|\\boldsymbol x)~/~ Mh(\\theta) \\leq 1\\), \\(\\forall \\theta \\in \\Theta\\), de modo que a melhor escolha para \\(M\\) é \\(M^*=\\underset{\\Theta}{\\sup}\\dfrac{f(\\theta|\\boldsymbol x)}{h(\\theta)}\\). r = function(t){dbeta(t,4,2)/dbeta(t,1,1)} M = optimize(r,c(0,1),maximum = TRUE) tibble(t = seq(0,1,length.out = 1000)) %&gt;% mutate(f=dbeta(t,4,2), h=dbeta(t,1,1), Mh=M$objective*dbeta(t,1,1)) %&gt;% ggplot() + theme_bw() + geom_line(aes(x=t,y=f,colour=&quot;&#39;Dist. Interesse&#39; f&quot;),lwd=1.1) + geom_line(aes(x=t,y=h,colour=&quot;&#39;Dist. Candidata&#39; h&quot;),lwd=1.1) + geom_line(aes(x=t,y=Mh,colour=&quot;M.h&quot;),lwd=1.1) + geom_segment(x=0.5,xend=0.5,y=dbeta(0.5,4,2),yend=M$objective*dbeta(0.5,1,1),col=&quot;darkgrey&quot;) + geom_segment(x=0.5,xend=0.5,y=0,yend=dbeta(0.5,4,2)) + geom_segment(x=0.25,xend=0.25,y=dbeta(0.25,4,2),yend=M$objective*dbeta(0.25,1,1),col=&quot;darkgrey&quot;) + geom_segment(x=0.25,xend=0.25,y=0,yend=dbeta(0.25,4,2)) + geom_segment(x=0.75,xend=0.75,y=dbeta(0.75,4,2),yend=M$objective*dbeta(0.75,1,1),col=&quot;darkgrey&quot;) + geom_segment(x=0.75,xend=0.75,y=0,yend=dbeta(0.75,4,2)) + labs(colour=&quot;&quot;) No exemplo apresentado no gráfico acima, suponha que foram gerados os candidatos \\(0.25\\), \\(0.5\\) e \\(0.75\\). É possível notar que o ponto \\(0.75\\) deve ser aceito, o ponto \\(0.5\\) dever ser aceito com probabilidade \\(0.59\\) e o ponto \\(0.25\\) deve ser aceito com probabilidade \\(0.11\\). A seguir é apresentado o pseudo-algorítmo do método da rejeição. Para \\(i=1,...,m\\) \\(~~~\\) Repita \\(~~~~~~\\) Simule \\(u\\sim Unif(0,1)\\) \\(~~~~~~\\) Simule \\(\\theta^\\prime\\) da distribuição candidata \\(h(\\theta)\\) \\(~~~\\) Até \\(u \\leq \\frac{f(\\theta^\\prime|\\boldsymbol x)}{Mh(\\theta^\\prime)}\\) \\(~~~\\) \\(\\theta_i=\\theta^\\prime\\) Fim_Para. r = function(t){(0.4*dnorm(t,-1,1/2)+0.6*dt(t,5,1))/dt(t,1)} M = optimize(r,c(-8,10),maximum = TRUE) tibble(t = seq(-5,6,length.out = 1000)) %&gt;% mutate(f=(0.4*dnorm(t,-1,1/2)+0.6*dt(t,5,1)), h=dt(t,1), Mh=M$objective*dt(t,1)) %&gt;% ggplot() + theme_bw() + geom_line(aes(x=t,y=f,colour=&quot;&#39;Dist. Interesse&#39; f&quot;),lwd=1.1) + geom_line(aes(x=t,y=h,colour=&quot;&#39;Dist. Candidata&#39; h&quot;),lwd=1.1) + geom_line(aes(x=t,y=Mh,colour=&quot;M.h&quot;),lwd=1.1) + geom_vline(xintercept=M$maximum,linetype=&quot;longdash&quot;,col=&quot;darkgrey&quot;) + labs(colour=&quot;&quot;) A linha tracejada representa o ponto na escolha ótima para \\(M\\). Nesse exemplo é possível notar que na região central, onde é mais provável gerar observações de \\(h\\), a razão \\(f(\\theta|\\boldsymbol x)~/~Mh(\\theta)\\) é menor que \\(0.25\\), de modo que há uma grande probabilidade de rejeição. Isso justifica a escolha de distribuições candidatas com caudas pesadas. No caso geral, a probabilidade de aceitação do método é \\(P\\left(\\left\\{(U,\\theta) : U \\leq \\dfrac{f(\\theta|\\boldsymbol x)}{Mh(\\theta)}\\right\\}\\right)\\) \\(=E_{U,\\theta}\\left[\\mathbb{I}\\left(U \\leq \\dfrac{f(\\theta|\\boldsymbol x)}{Mh(\\theta)}\\right)\\right]\\) \\(=E_{\\theta}\\left[{E_{U|\\theta}\\left[\\mathbb{I}\\left(U \\leq \\dfrac{f(\\theta|\\boldsymbol x)}{Mh(\\theta)}\\right)\\right]}\\right]\\) \\(=E_{\\theta}\\left[P\\left(U \\leq \\dfrac{f(\\theta|\\boldsymbol x)}{Mh(\\theta)}~\\Big|~\\theta\\right)\\right]\\) \\(=E_\\theta\\left[\\dfrac{f(\\theta|\\boldsymbol x)}{Mh(\\theta)}\\right]\\) \\(=\\displaystyle\\int_\\Theta\\dfrac{f(\\theta|\\boldsymbol x)}{Mh(\\theta)}h(\\theta)d\\theta\\) \\(=\\dfrac{1}{M}\\displaystyle\\int_\\Theta f(\\theta|\\boldsymbol x)d\\theta=\\dfrac{1}{M}\\). \\(~\\) No exemplo, a probabilidade de aceitação é \\(1/2.434 = 0.41\\), ou seja, mais de metade das observações geradas seriam descartadas. \\(~\\) 7.4 ABC (Aproximated Bayesian Computation) O método ABC é uma forma bastante simples de gerar pontos da distribuição a posteriori. Para sua utilização é suficiente saber gerar pontos da distribuição dos dados e da priori, de modo que a verossimilhança nem precisa ser analiticamente conhecida, fato esse que faz com que o método seja dito ser likelihood-free. Suponha o caso em que \\(\\boldsymbol X\\) é discreto com função de verossimilhança \\(f(\\boldsymbol x|\\theta)\\), a priori é \\(f(\\theta)\\) e foi observado \\(\\boldsymbol X=\\boldsymbol x_o\\). Abaixo é apresentado o pseudo-algorítmo para simular observaçoes da posteriori \\(f(\\theta |\\boldsymbol x_o)\\) usando o método ABC. Algorítmo ABC (\\(\\boldsymbol X\\) discreto) \\(~\\) Para \\(i=1,...,m\\) \\(~~~\\) Repita \\(~~~~~~\\) Gere \\(\\theta^\\prime\\) de \\(f(\\theta)\\) (priori) \\(~~~~~~\\) Gere \\(\\boldsymbol y = (y_1,...,y_n)\\) de \\(f(\\boldsymbol x|\\theta^\\prime)\\) (verossimilhança) \\(~~~\\) Até \\(\\boldsymbol y =\\boldsymbol x_o\\) \\(~~~\\) \\(\\theta_i = \\theta^\\prime\\) Fim_Para Para verificar que o método funciona no caso discreto, basta ver que \\(f(\\theta_i)\\) \\(=\\displaystyle \\sum_{y\\in \\mathfrak{X}}f(\\theta_i)f(\\boldsymbol y|\\theta_i)\\mathbb{I}(\\boldsymbol y = \\boldsymbol x_o)\\) \\(=f(\\theta_i)f(\\boldsymbol x_o|\\theta_i)\\) \\(\\propto f(\\theta |\\boldsymbol x_o)\\). \\(~\\) No caso em que \\(\\boldsymbol X\\) é contínuo, a probabilidade de gerar uma nova amostra \\(\\boldsymbol Y\\) exatamente igual ao ponto observado \\(\\boldsymbol x_o\\) é zero, \\(P(\\boldsymbol Y=\\boldsymbol x_o)=0\\). Nesse caso, o algorítmo é adaptado de modo que são aceitos pontos gerados com \\(\\Delta\\left(\\eta(\\boldsymbol y),\\eta(\\boldsymbol x_o)\\right) \\leq \\varepsilon\\), onde \\(\\Delta\\) é uma medida de distância conveniente, \\(\\eta\\) é uma estatística (que pode não ser suficiente para \\(\\theta\\)) e \\(\\varepsilon\\) é uma constante de tolerância. O pseudo-algorítmo é apresentado a seguir. Algorítmo ABC (\\(\\boldsymbol X\\) qualquer) \\(~\\) Para \\(i=1,...,m\\) \\(~~~\\) Repita \\(~~~~~~\\) Gere \\(\\theta^\\prime\\) de \\(f(\\theta)\\) \\(~~~~~~\\) Gere \\(\\boldsymbol y\\) de \\(f(\\boldsymbol x|\\theta^\\prime)\\) \\(~~~\\) Até \\(\\Delta(\\eta(\\boldsymbol x),\\eta(\\boldsymbol y))\\leq \\varepsilon\\) \\(~~~\\) \\(\\theta_i=\\theta^\\prime\\) Fim_Para \\(~\\) \\(~\\) 7.5 MCMC - Monte Carlo via Cadeias de Markov 7.5.1 Pequena Introdução às Cadeias de Markov Definição Um processo estocástico (em tempo discreto) é uma sequência de v.a. \\(X_0,X_1,X_2,...\\) indexada em \\(\\mathbb{N}\\) (os indices podem indicar, por exemplo, tempo ou espaço ou ?). O conjunto \\(E\\) onde \\(X_i\\) toma valores é chamado de espaço de estados. \\(~\\) Definição Um processo estocásticos é dito uma Cadeia de Markov (em tempo discreto) se, \\(\\forall n \\geq 1\\) e \\(\\forall A \\subseteq E\\), \\(P(X_{n+1}\\in A|X_{n}=x_{n},...,X_1=x_1,X_0=x_0)\\) \\(=P(X_{n+1}\\in A|X_{n}=x_{n})\\) \\(~\\) Exemplo 1. Suponha uma sequência de v.a. \\(\\left(X_n\\right)_{n\\geq 1}\\) i.i.d. tais que \\(p=P(X_1=1)=1-P(X_1=-1)\\). Defina \\(S_n=\\displaystyle \\sum_{i=1}^n X_i\\) e \\(X_0=c\\). O processo estocástico \\((S_n)_{n\\geq 0}\\) é uma Cadeia de Markov. De fato, \\(P\\left(S_n=s_n|S_{n-1}=s_{n-1},\\ldots,S_0=s_0\\right)\\) \\(=\\displaystyle P\\left(X_n+S_{n-1}=s_{n}|S_{n-1}=s_{n-1},\\ldots,S_0=s_0\\right)\\) \\(=\\displaystyle P\\left(X_n=s_n-s_{n-1}|S_{n-1}=s_{n-1},\\ldots,S_0=s_0\\right)\\) \\(=\\displaystyle P\\left(X_n=s_n-s_{n-1}|S_{n-1}=s_{n-1}\\right)\\) \\(=\\displaystyle P\\left(S_n=s_n|S_{n-1}=s_{n-1}\\right)\\) \\(~\\) Uma Cadeia de Markov é caracterizada pela distribuição do estado inicial \\(X_0\\) e pelas probabilidades de transição \\(Q(x,A)=P(X_n\\in A|X_{n-1}=x)\\). Se \\(Q(x,A)\\) não depende de \\(n\\), dizemos que é homogênea no tempo. \\(~\\) Para cada \\(n\\), a cadeia pode Continuar no estado anterior \\(x\\), ou seja, \\(X_{n+1}=x,\\) com probabilidade \\(r(x),~ 0\\leq r&lt;1\\), ou Saltar para um estado \\(y\\) segundo uma função de densidade de probabilidade \\(q(x,y)\\), onde \\(0&lt;\\displaystyle\\int_E q(x,y)dy=1-r(x)\\leq 1\\) (sub-probabilidade). No caso discreto vamos considerar \\(q(x,x)=0\\). Assim, \\(Q(x,A)\\) \\(=P(X_{n+1}\\in A|X_{n}=x)\\) \\(=\\displaystyle\\int_A q(x,y)dy+r(x)\\mathbb{I}_A(x)\\). \\(~\\) Suponha que para um dado \\(n\\), \\(X_n\\) tem densidade \\(\\lambda\\), isto é, \\(P(X_n\\in A)=\\displaystyle\\int_A\\lambda(x)dx\\). Então, a densidade de \\(X_{n+1}\\) pode ser obtida por \\(P(X_{n+1}\\in A)~\\overset{\\begin{array}{c} \\text{regra da }\\\\ \\text{prob. total}\\end{array}}{=}~ \\displaystyle\\int_E \\lambda(x)~Q(x,A)dx\\) \\(=\\displaystyle\\int_E\\lambda(x)\\left[\\int_A q(x,y)dy+r(x)\\mathbb{I}_A(x)\\right]dx\\) \\(=\\displaystyle\\int_A\\int_E\\lambda(x)q(x,y)dxdy+\\int_E\\lambda(x)r(x)\\mathbb{I}_A(x)dx\\) \\(=\\displaystyle\\int_A\\int_E\\lambda(x)q(x,y)dxdy+\\int_A\\lambda(y)r(y)dy\\) \\(=\\displaystyle \\int_A\\underbrace{\\left[\\int_E\\lambda(x)q(x,y)dx+\\lambda(y)r(y)\\right]}_{\\text{f.d.p. de }X_{n+1}}dy\\). Assim, a f.d.p de \\(X_{n+1}\\) é \\(\\lambda Q(y)= \\displaystyle\\int_E\\lambda(x)q(x,y)dx+\\lambda(y)r(y)\\) \\(~\\) Dizemos que a densidade \\(\\pi\\) é invariante (estacionária) se as densidades de \\(X_{n}\\) e \\(X_{n+1}\\) são iguais (q.c), isto é, \\(\\pi=\\pi Q\\) ou \\(\\int_A \\pi(x)dx=\\int_E \\pi(x)Q(x,A)dx\\). \\(~\\) Resultado 1. A afirmação anterior é equivalente a \\(\\int\\pi(x)q(x,y)dx=(1-r(x))\\pi(y)\\). \\(~\\) Resultado 2. Se a função \\(q(x,y)\\) satisfaz a condição de reversibilidade, isto é, \\(\\pi(x)q(x,y)=\\pi(y)q(y,x)\\), então \\(\\pi\\) é uma medida invariante da cadeia com função de transição \\(Q(x,\\cdot)\\). \\(~\\) Demo 1. \\(\\displaystyle\\int_E\\pi(x)q(x,y)dx\\) \\(=\\displaystyle\\int_E\\pi(y)q(y,x)dx\\) \\(=\\displaystyle\\pi(y)\\underbrace{\\int_Eq(y,x)dx}_{1-r(y)}\\) \\(~\\) Demo 2. \\(\\displaystyle\\int_E \\pi(x)Q(x,A)dx\\) \\(=\\displaystyle\\int_E\\pi(x)\\left[\\int_Aq(x,y)dy\\right]dx+\\int_E \\pi(x) r(x)\\mathbb{I}_A(x)dx\\) \\(=\\displaystyle\\int_A\\left[\\int_E\\pi(x)q(x,y)dx\\right]dy+\\int_A\\pi(x)r(x)dx\\) \\(=\\displaystyle\\int_A\\left[\\int_E\\pi(y)q(y,x)dx\\right]dy+\\int_A\\pi(y)r(y)dx\\) \\(=\\displaystyle\\int_A\\pi(y)\\left[\\int_Eq(y,x)dx\\right]dy+\\int_A\\pi(y)r(y)dy\\) \\(=\\displaystyle\\int_A\\pi(y)\\Big[1-r(y)\\Big]dy+\\int_A\\pi(y)r(y)dy\\) \\(=\\displaystyle\\int_A\\pi(y)\\Big[1-r(y)+r(y)\\Big]dy\\) \\(=\\displaystyle\\int_A\\pi(y)dy\\). \\(~\\) 7.5.2 O algoritmo de Metrópolis-Hastings Suponha que deseja-se gerar observações de \\(\\pi(\\theta)\\propto f(\\boldsymbol x|\\theta)f(\\theta)\\propto f(\\theta|\\boldsymbol x)\\). Defina uma Cadeia de Markov \\((Y_n)_{n\\geq 1}\\) tal que, no instante \\(n\\), \\(Y_n=y\\). No instante \\(n+1\\), um candidato \\(z\\) é gerado segundo a densidade \\(q(y,z)\\) e é aceito com probabilidade \\(\\alpha(y,z)\\). Isto é, se \\(Y_n=y\\), \\(Y_{n+1}=\\left\\{\\begin{array}{rcl} z,&amp; \\text{com probabilidade}&amp; \\alpha(y,z)\\\\ y,&amp; \\text{com probabilidade}&amp; 1-\\alpha(y,z)\\end{array}\\right.\\), em que \\(\\alpha\\) é dado por \\(\\alpha(y,z)=\\left\\{\\begin{array}{cl} min\\left\\{ \\dfrac{\\pi(z)q(z,y)}{\\pi(y)q(y,z)}~,~1\\right\\},&amp; \\text{ se }~ \\pi(y)q(y,z)&gt;0\\\\ 1,&amp; \\text{c.c.}\\end{array}\\right.\\) \\(~\\) Resultado: O algoritmo de M-H gera uma cadeia reversível com respeito a \\(\\pi\\) e, portanto, tem \\(\\pi\\) como distribuição estacionária. \\(~\\) Demo. Deve-se mostrar que \\(\\pi(y)\\underbrace{q(y,z)\\alpha(y,z)}_{p(y,z)}=\\pi(z)\\underbrace{q(z,y)\\alpha(z,y)}_{p(z,y)}~.\\) Suponha \\(\\pi(z)q(z,y)\\geq \\pi(y)q(y,z)\\) (o caso \\(~\\leq~\\) é análogo) i) Se \\(\\pi(z)q(z,y)=0\\Rightarrow\\pi(y)q(y,z)=0\\) e vale a reversibilidade. ii) \\(\\pi(z)q(z,y)&gt;0 ~\\Rightarrow~ \\alpha(y,z)=1\\) e \\(\\alpha(z,y)=\\dfrac{\\pi(y)q(y,z)}{\\pi(z)q(z,y)}\\). Nesse caso, \\(\\pi(z)q(z,y)\\alpha(z,y)=\\) \\(\\pi(z)q(z,y)\\dfrac{\\pi(y)q(y,z)}{\\pi(z)q(z,y)}\\) \\(=\\pi(y)q(y,z)\\) \\(=\\pi(y)q(y,z)\\alpha(y,z)\\) \\(~\\) \\(~\\) 7.5.3 Amostrador de Gibbs Suponha que a \\(dim(\\Theta)&gt;1\\) e deseja-se amostrar \\(f(\\boldsymbol \\theta| \\boldsymbol x)\\) e suponha que é possível obter amostras das distribuições condicionais completas, isto é, de \\(f(\\theta_i| \\boldsymbol \\theta_{-i},\\boldsymbol x)\\), onde \\(\\boldsymbol \\theta_{-i}=(\\theta_1,...,\\theta_{i-1},\\theta_{i+1},\\theta_k)\\). Note que \\(f(\\theta_i| \\boldsymbol \\theta_{-i},\\boldsymbol x)\\propto f(\\boldsymbol \\theta| \\boldsymbol x)=f(\\boldsymbol x|\\boldsymbol \\theta)f(\\boldsymbol \\theta)\\). O método do Amostrador de Gibbs é um caso particular do algorítmo de Metrópolis-Hastings em que é gerada uma cadeia \\(\\left(\\boldsymbol \\theta^{(n)}\\right)_{n\\geq 1}\\) com \\(\\alpha(\\boldsymbol{y},\\boldsymbol{z})=1\\) e \\(q(\\boldsymbol{y},z)=f\\left({\\theta}_j=\\boldsymbol{z}~ \\big|~ \\boldsymbol{\\theta}_{-j}=\\boldsymbol{y},~\\boldsymbol{x}\\right)\\), gerada segundo o algorítmo a seguir. Algorítmo - Amostrador de Gibbs \\(~\\) \\(~~~\\) Defina uma chute inicial \\(\\boldsymbol \\theta^{(0)}\\) (por exemplo, gerado da priori \\(f(\\boldsymbol \\theta)\\) ou fixado) Para \\(i=1,...,m\\) \\(~~~\\) Gere \\(\\theta_1^{(i)}\\) de \\(f(\\theta_1| \\boldsymbol \\theta_{-1}^{(i-1)},\\boldsymbol x)\\) \\(~~~\\) Gere \\(\\theta_2^{(i)}\\) de \\(f(\\theta_2| \\theta_{1}^{(i)}, \\theta_{3}^{(i-1)},\\ldots, \\theta_{k}^{(i-1)} ,\\boldsymbol x)\\) \\(~~~\\) \\(~~~\\vdots\\) \\(~~~\\) Gere \\(\\theta_{k-1}^{(i)}\\) de \\(f(\\theta_{k-1}| \\theta_{1}^{(i)},\\ldots, \\theta_{k-2}^{(i)}, \\theta_{k}^{(i-1)} ,\\boldsymbol x)\\) \\(~~~\\) Gere \\(\\theta_k^{(i)}\\) de \\(f(\\theta_k| \\boldsymbol \\theta_{-k}^{(i)} ,\\boldsymbol x)\\) Fim_Para \\(~\\) Os métodos de Metropolis-Hastings descritos anteriormente geram observações de cadeias de Markov com distribuição estacionária que coincide com a posteriori. Contudo, deve-se tomar dois cuidados para a utilização de métodos de Monte Carlo usando essas observalções. O primeiro é que é necessário verificar se a cadeia já atingiu a estacionariedade. Essa verificação é feita, em geral, observando os gráficos das cadeias geradas e, em geral, as primeiras \\(b\\) observações são descartadas (burn-in). Outra possibilidade para verificar a estacionariedade da cadeia, bem como a influência do chute inicial, é gerar duas ou mais cadeias iniciando-se de pontos distintos. Outro problema é a dependência entre as observações geradas. Para contornar esse problema, normalmente uma distância \\(k\\) entre as observações que serão consideradas na amostra final (thin) e as observações entre estas são descartadas. Assim, a amostra final é formada pelos pontos \\(\\boldsymbol \\theta_b, \\boldsymbol \\theta_{b+k}, \\boldsymbol \\theta_{b+2k},\\ldots,\\boldsymbol \\theta_{M}\\). \\(~\\) Exemplo 1. Seja \\(X_1,\\ldots,X_n\\) c.i.i.d. tais que \\(X_i~|~\\theta_1,\\theta_2 \\sim \\textit{Exp}\\left(\\theta_1\\theta_2\\right)\\) e considere que a priori \\(\\theta_i \\sim \\textit{Gama}\\left(a_1,b_i\\right)\\), \\(i=1,2\\). Assim, \\(f(\\boldsymbol \\theta|\\boldsymbol x)\\) \\(\\propto f(\\boldsymbol x|\\boldsymbol \\theta) f(\\theta_1)f(\\theta_2)\\) \\(\\propto (\\theta_1 \\theta_2)^n~e^{-\\theta_1\\theta_2\\sum x_i}~~\\theta_1^{a_1-1}e^{-b_1\\theta_1}~~\\theta_2^{a_2-1}e^{-b_2\\theta_2}\\) \\(\\propto \\theta_1^{a_1+n-1}~\\theta_2^{a_2+n-1}~~e^{-b_1\\theta_1-b_2\\theta_2 -\\theta_1\\theta_2\\sum x_i}~.\\) Essa distribuição não é conhecida mas é possível obter as distribuições condicionais completas \\(f(\\theta_i|\\theta_j,\\boldsymbol x) \\propto \\theta_i^{a_i+n-1}~e^{-\\left[b_i \\theta_j\\sum x_i\\right]\\theta_i}\\) \\(\\Longrightarrow \\theta_i~|~\\theta_j,\\boldsymbol x \\sim \\textit{Gama}\\left(a_i+n,b_i+\\theta_j\\sum x_i\\right)\\), e, portanto, é possível simular observações da posteriori usando o amostrador de Gibbs. set.seed(666) a1=2; b1=3 a2=3; b2=2 n=8 sumx=4 M=10000 theta1=vector(length = M) theta2=vector(length = M) theta1[1] = 4 theta2[1] = 4 for(i in 2:M){ theta1[i] = rgamma(1,a1+n,b1+theta2[i-1]*sumx) theta2[i] = rgamma(1,a2+n,b2+theta1[i]*sumx) } m=1000; sel=seq(1,m) tibble(theta1=theta1[sel],theta2=theta2[sel],t=seq(1,length(sel))) %&gt;% ggplot() + theme_bw() + geom_path(aes(theta1,theta2),col=&quot;darkgrey&quot;) + geom_point(aes(theta1,theta2)) + xlab(expression(theta[1])) + ylab(expression(theta[2])) + gganimate::transition_manual(t, cumulative = TRUE) A seguir, são apresentados os gráficos das cadeias geradas e das autocorrelações. Aparentemente, a cadeia converge rapidamente para a distribuição estacionária mas as autocorrelações entre observações consecutivas é alta. Assim, vamos descartar as 10 primeiras observações e considerar saltos de tamanho 5. Os novo gráficos são apresentados abaixo. Por fim são apresentadas as estimativas das densidades marginais e as regiões HPD. sel=seq(10,M,5) dpost=Vectorize(function(t1,t2){ #densidade posterior exp((n-1)*log(t1*t2)+dexp(sumx,t1*t2,log=TRUE)+dgamma(t1,a1,b1,log=TRUE)+dgamma(t2,a2,b2,log=TRUE))}) # simulações df = tibble(theta1=theta1[sel],theta2=theta2[sel]) %&gt;% mutate(post=dpost(theta2,theta2)) # variáveis para os gráficos gama=c(0.99,0.95,0.9,0.8,0.5,0.3,0.1) # prob das regiões l=quantile(df$post,1-gama) d=1000 x=seq(0,12,length.out = d) y=seq(0,12,length.out = d) z=apply(cbind(rep(x,d),rep(y,each=d)),1,function(t){dpost(t[1],t[2])}) # gráfico das regiões HPD de prob. gama=c(0.99,0.95,0.9,0.8,0.5,0.3,0.1) tibble(x1=rep(x,d),y1=rep(y,each=d),z1=z) %&gt;% arrange(z1) %&gt;% mutate(p=1-(cumsum(z1)/sum(z1))) %&gt;% ggplot(aes(x1,y1,z=z1,fill = p)) + geom_raster(interpolate = TRUE) + jcolors::scale_fill_jcolors_contin(&quot;pal3&quot;) + geom_contour(breaks=l,col=&quot;black&quot;) + xlab(expression(theta[1])) + ylab(expression(theta[2])) 7.6 Bibliotecas de R para Inferência Bayesiana Nessa seção serão apresentadas algumas bibliotecas do R para inferência Bayesiana, em especial, LaplacesDemon e Stan, que são bibliotecas utilizadas para simular dados da posteriori. Alguns dos gráficos apresentados nessa seção serão construídos com bibliotecas específicas para avaliar amostras da posteriori, ggmcmc e bayesplot. 7.6.1 O Modelo de Regressão Linear Inicialmente, será apresentado como exemplo o modelo de regressão linear, possivelmente um dos métodos mais usados nas aplicações de inferência estatística. Considere \\(n\\) observações de uma variável aleatória de interesse (chamada de variável dependente ou variável resposta) e de \\(p-1\\) características associadas a cada uma dessas observações (chamadas de variáveis dependentes ou explicativas ou covariáveis), supostamente fixadas. Um modelo de regressão linear pode ser escrito como \\[\\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] com \\(~\\boldsymbol{Y} = \\left[\\begin{array}{c} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_n \\end{array}\\right]~~\\); \\(~~~\\boldsymbol{X} = \\left[\\begin{array}{cccc} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1,p-1}\\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2,p-1}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{n,p-1} \\end{array}\\right]~~\\); \\(~~~\\boldsymbol{\\beta} = \\left[\\begin{array}{c} \\beta_1\\\\ \\beta_2\\\\ \\vdots\\\\ \\beta_p \\end{array}\\right]~~\\); \\(~~~\\boldsymbol{\\epsilon} = \\left[\\begin{array}{c} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n \\end{array}\\right]~~\\); \\(~~~\\boldsymbol{Z} = \\left[\\boldsymbol{X,Y}\\right]~~\\), em que \\(\\boldsymbol{Z}\\) é a matriz de dados (observada), \\(\\boldsymbol{\\beta}\\) é o vetor de parâmetros e \\(\\epsilon_i\\) é o erro aleatório associado a \\(i\\)-ésima observação, supostamente c.i.i.d. com distribuição \\(\\textit{Normal}(0,\\sigma^2)\\). De forma equivalente, o modelo pode ser escrito como \\(\\boldsymbol{Y}|\\boldsymbol{X},\\boldsymbol{\\beta},\\sigma \\sim \\textit{Normal}_{~n}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\) com \\(\\boldsymbol{\\mu}=\\boldsymbol{X}\\boldsymbol{\\beta}\\) e \\(\\boldsymbol{\\Sigma}=\\sigma^2\\boldsymbol{I}\\). \\(~\\) Na abordagem frequentista, se \\(\\boldsymbol{X}&#39;\\boldsymbol{X}\\) é não singular, os estimadores de máxima verossimilhança para os parâmetros \\((\\boldsymbol{\\beta},\\sigma^2)\\) são, respectivamente, \\(\\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}&#39;\\boldsymbol{X})^{-1}\\boldsymbol{X}&#39;\\boldsymbol{Y}\\) e \\(s^2 = \\dfrac{(\\boldsymbol{Y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}})&#39;(\\boldsymbol{Y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}})}{n-p}\\). \\(~\\) Exemplo. Vamos considerar um simples exemplo de regressão linear, com apenas uma covariável. Para isso, considere as variáveis speed e dist do conjunto de dados cars, disponível no R. Um ajuste usando a abordagem frequentista é apresentado a seguir. # a boring regression fit = lm(speed ~ 1 + dist, data = cars) coef(summary(fit)) # estimativa dos betas ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.2839056 0.87438449 9.473985 1.440974e-12 ## dist 0.1655676 0.01749448 9.463990 1.489836e-12 (summary(fit)$sigma)**2 # estimativa do sigma^2 ## [1] 9.958776 ggplot(cars, aes(y=speed, x=dist)) + theme_bw() + geom_point() + geom_smooth(method=lm) \\(~\\) \\(~\\) Sob a abordagem bayesiana, a distribuição Normal-Inversa Gama (NIG) é uma priori conjugada para \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\beta},\\sigma^2)\\) neste modelo. Assim, \\[(\\boldsymbol{\\beta},\\sigma^2) \\sim \\textit{NIG}(\\boldsymbol{\\beta}_0, \\boldsymbol{V}_0, a_0, b_0)~.\\] Isto é, \\(\\boldsymbol{\\beta} | \\sigma^2 \\sim Normal_p\\left(\\boldsymbol{\\beta}_0,\\sigma^2\\boldsymbol{V}_0\\right)~~\\); \\(~~ \\sigma^2 \\sim InvGamma\\left(a_0,b_0\\right)\\) ou, equivalentemente, \\(\\boldsymbol{\\beta} \\sim T_p\\left(2a_0; \\boldsymbol{\\beta}_0,\\frac{b_0 \\boldsymbol{V}_0}{a_0}\\right) ~~\\); \\(~~ \\sigma^2 | \\boldsymbol{\\beta} \\sim InvGamma\\left(a_0 + \\frac{p}{2},b_0 + \\frac{\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0\\right)^T \\boldsymbol{V}_0^{-1} \\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0\\right) }{2}\\right)~~\\), com \\(\\boldsymbol{\\beta}_0 \\in \\mathbb{R}^p\\), \\(\\boldsymbol{V}_0\\) matriz simétrica positiva definida e \\(a_0, b_0 \\in \\mathbb{R}_+\\). \\(~\\) Então: \\[(\\boldsymbol{\\beta},\\sigma^2)|\\boldsymbol{Z} \\sim \\textit{NIG}(\\boldsymbol{\\beta}_1, \\boldsymbol{V}_1, a_1, b_1)\\] com \\(\\boldsymbol{\\beta}_1 = \\boldsymbol{V}_1\\left(\\boldsymbol{V}_0^{-1}\\boldsymbol{\\beta}_0 + \\boldsymbol{X}^T\\boldsymbol{X} \\hat{\\boldsymbol{\\beta}}\\right) ~~\\); \\(~~ \\boldsymbol{V}_1 = \\left(\\boldsymbol{V}_0^{-1} + \\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}~~\\); \\(a_1 = a_0+\\frac{n}{2} ~~\\); \\(~~ b_1 = b_0 + \\frac{\\boldsymbol{\\beta}_0^T\\boldsymbol{V}_0^{-1}\\boldsymbol{\\beta}_0 + \\boldsymbol{Y}^T\\boldsymbol{Y} - \\boldsymbol{\\beta}_1^T\\boldsymbol{V}_1^{-1}\\boldsymbol{\\beta}_1}{2}\\). \\(~\\) Observação. Uma das maneiras de representar falta de informação nesse contexto é utilizar a priori de Jeffreys, \\(f(\\boldsymbol \\theta) = \\Big|~\\mathcal{I}(\\boldsymbol \\theta)~\\Big|^{1/2} \\propto 1/\\sigma^2\\). Nesse caso, distribuição a posteriori é \\[(\\boldsymbol{\\beta},\\sigma^2)\\Big|\\boldsymbol{Z} \\sim \\textit{NIG}\\left(\\hat{\\boldsymbol{\\beta}}, \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}, \\dfrac{n-p}{2}, \\dfrac{(n-p)s^2}{2}\\right)~.\\] \\(~\\) \\(~\\) Exemplo. Considere que, a priori, \\((\\boldsymbol{\\beta},\\sigma^2) \\sim \\textit{NIG}\\left(\\boldsymbol{\\beta}_0, \\boldsymbol{V}_0, a_0, b_0\\right)\\), com \\(~\\boldsymbol{\\beta}_0 = \\left[\\begin{array}{c} 0\\\\0\\end{array}\\right]~\\); \\(~\\boldsymbol{V}_0 = \\left[\\begin{array}{cc} 100 &amp; 0\\\\ 0 &amp; 100\\end{array}\\right] ~\\); \\(~a_0 = 3~\\); \\(~b_0 = 100~\\). A seguir são apresentadas as distribuições marginais dos parâmetros, a distribuição marginal e as regiões HPD bivariadas do parâmetro \\(\\boldsymbol{\\beta}\\). x = cars$dist # variável resposta y = cars$speed # variável explicativa n = length(x) # n=50 X = cbind(1,x) # Matrix de planejamento p = ncol(X) # p=2 g = n-p # gl=48 beta_est = solve(t(X)%*%X)%*%(t(X)%*%y) # (-17.6, 3.9) sigma_est = as.double(t(y-X%*%beta_est)%*%(y-X%*%beta_est)/(n-p)) #236.5 beta0 = c(0,0) # média priori betas V0 = matrix(c(100,0,0,100),ncol=2) # matriz de escala beta a0 = 3 # priori sigma b0 = 100 # priori sigma # parâmetros da posteriori V1 = solve(solve(V0) + t(X)%*%X) beta1 = V1%*%(solve(V0)%*%beta0 + t(X)%*%X%*%beta_est) a1 = a0 + n/2 b1 = as.double(b0 + (t(beta0)%*%solve(V0)%*%beta0 + t(y)%*%y - t(beta1)%*%solve(V1)%*%beta1)/2) V = b1*V1/a1 # Matrix de escala da posteriori marginal de beta beta1lim=c(beta1[1]-qt(0.9999,2*a1)*sqrt(V[1,1]),beta1[1]+qt(0.9999,2*a1)*sqrt(V[1,1])) beta2lim=c(beta1[2]-qt(0.9999,2*a1)*sqrt(V[2,2]),beta1[2]+qt(0.9999,2*a1)*sqrt(V[2,2])) sigma2lim=c(extraDistr::qinvgamma(0.0001,a1,b1),extraDistr::qinvgamma(0.9999,a1,b1)) b1plot &lt;- ggplot(data.frame(x=beta1lim), aes(x=x), colour = &quot;0.Posterior&quot;) + stat_function(fun = mnormt::dmt,args = list(mean = beta1[1], S = V[1,1], df=2*a1)) + theme_bw() + xlab(expression(beta[1])) + ylab(&quot;Posterior&quot;) b2plot &lt;- ggplot(data.frame(x=beta2lim), aes(x=x), colour = &quot;0.Posterior&quot;) + stat_function(fun = mnormt::dmt, args = list(mean = beta1[2], S = V[2,2], df=2*a1)) + theme_bw() + xlab(expression(beta[2])) + ylab(&quot;Posterior&quot;) s2plot &lt;- ggplot(data.frame(x=sigma2lim),aes(x=x), colour = &quot;0.Posterior&quot;)+ stat_function(fun = extraDistr::dinvgamma, args = list(alpha = a1, beta = b1)) + theme_bw() + xlab(expression(sigma^2)) + ylab(&quot;Posterior&quot;) multiplot(b1plot,b2plot,s2plot) # posteriori marginal bivariada dos betas posterior &lt;- function(theta0,theta1) { apply(cbind(theta0,theta1),1,function(w){ mnormt::dmt(w, mean=c(beta1), S=V, df=2*a1) }) } # Gráfico da posteriori marginal bivariada dos betas grx &lt;- seq(beta1lim[1], beta1lim[2],length.out=200) gry &lt;- seq(beta2lim[1], beta2lim[2],length.out=200) z1 &lt;- outer(grx,gry,posterior) #persp(grx,gry,z1) plotly::plot_ly(alpha=0.1) %&gt;% plotly::add_surface(x=grx, y=gry, z=t(z1), colorscale = list(c(0,&#39;#BA52ED&#39;), c(1,&#39;#FCB040&#39;)), showscale = FALSE) # Curvas de Probabilidade l = c(0.1,0.3,0.5,0.8,0.9,0.95,0.99) z1v = sort(as.vector(z1),decreasing = TRUE) v1 &lt;- z1v/sum(z1v) a=0; j=1; l1=NULL for(i in 1:length(v1)) { a &lt;- a+v1[i] if(j&lt;=length(l) &amp; a&gt;l[j]) { l1 &lt;- c(l1,z1v[i-1]) j &lt;- j+1 } } contour(grx,gry,z1,col=colors()[455],main=&quot;Regiões HPD para os Betas&quot;,xlab=expression(beta[1]),ylab=expression(beta[2]),levels=l1,labels=l) points(beta1[1],beta1[2],col=&quot;darkred&quot;,pch=16,cex=0.5) \\(~\\) \\(~\\) 7.6.2 Laplaces Demon LaplacesDemon é uma biblioteca do R que oferece diversos algoritimos implementados de MCMC, permitindo fazer Inferência Bayesiana aproximada. Os algoritimos de MCMC disponíveis são Automated Factor Slice Sampler (AFSS) Adaptive Directional Metropolis-within-Gibbs (ADMG) Adaptive Griddy-Gibbs (AGG) Adaptive Hamiltonian Monte Carlo (AHMC) Adaptive Metropolis (AM) Adaptive Metropolis-within-Gibbs (AMWG) Adaptive-Mixture Metropolis (AMM) Affine-Invariant Ensemble Sampler (AIES) Componentwise Hit-And-Run Metropolis (CHARM) Delayed Rejection Adaptive Metropolis (DRAM) Delayed Rejection Metropolis (DRM) Differential Evolution Markov Chain (DEMC) Elliptical Slice Sampler (ESS) Gibbs Sampler (Gibbs) Griddy-Gibbs (GG) Hamiltonian Monte Carlo (HMC) Hamiltonian Monte Carlo with Dual-Averaging (HMCDA) Hit-And-Run Metropolis (HARM) Independence Metropolis (IM) Interchain Adaptation (INCA) Metropolis-Adjusted Langevin Algorithm (MALA) Metropolis-Coupled Markov Chain Monte Carlo (MCMCMC) Metropolis-within-Gibbs (MWG) Multiple-Try Metropolis (MTM) No-U-Turn Sampler (NUTS) Oblique Hyperrectangle Slice Sampler (OHSS) Preconditioned Crank-Nicolson (pCN) Random Dive Metropolis-Hastings (RDMH) Random-Walk Metropolis (RWM) Reflective Slice Sampler (RSS) Refractive Sampler (Refractive) Reversible-Jump (RJ) Robust Adaptive Metropolis (RAM) Sequential Adaptive Metropolis-within-Gibbs (SAMWG) Sequential Metropolis-within-Gibbs (SMWG) Slice Sampler (Slice) Stochastic Gradient Langevin Dynamics (SGLD) Tempered Hamiltonian Monte Carlo (THMC) t-walk (twalk) Univariate Eigenvector Slice Sampler (UESS) Updating Sequential Adaptive Metropolis-within-Gibbs (USAMWG) Updating Sequential Metropolis-within-Gibbs (USMWG) https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/BayesianInference.pdf https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/LaplacesDemonTutorial.pdf https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/Examples.pdf https://cran.r-project.org/web/packages/LaplacesDemon/LaplacesDemon.pdf \\(~\\) Especificação do modelo require(LaplacesDemon) parm.names=as.parm.names(list(beta=rep(0,p), sigma2=0)) pos.beta=grep(&quot;beta&quot;, parm.names) pos.sigma=grep(&quot;sigma2&quot;, parm.names) MyData &lt;- list(J=p, X=X, y=y, mon.names=&quot;LP&quot;, parm.names=parm.names,pos.beta=pos.beta,pos.sigma=pos.sigma) Model &lt;- function(parm, Data) { ### Parameters beta &lt;- parm[Data$pos.beta] sigma2 &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf) parm[Data$pos.sigma] &lt;- sigma2 ### Log-Prior sigma.prior &lt;- dinvgamma(sigma2, a0, b0, log=TRUE) beta.prior &lt;- dmvn(beta, beta0, sigma2*V0, log=TRUE) ### Log-Likelihood mu &lt;- tcrossprod(Data$X, t(beta)) LL &lt;- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) ### Log-Posterior LP &lt;- LL + beta.prior + sigma.prior Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=rnorm(length(mu), mu, sigma2), parm=parm) return(Modelout) } Initial.Values &lt;- c(beta_est,sigma_est) burnin &lt;- 2000 thin &lt;- 3 N=(2000+burnin)*thin Exemplo 1: Metropolis-within-Gibbs (MWG) set.seed(666) Fit1 &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values, Covar=NULL, Iterations=N, Status=N/5, Thinning=thin, Algorithm=&quot;MWG&quot;, Specs=NULL) ## ## Laplace&#39;s Demon was called on Tue Nov 03 14:45:48 2020 ## ## Performing initial checks... ## Algorithm: Metropolis-within-Gibbs ## ## Laplace&#39;s Demon is beginning to update... ## Iteration: 2400, Proposal: Componentwise, LP: -146.2 ## Iteration: 4800, Proposal: Componentwise, LP: -147.2 ## Iteration: 7200, Proposal: Componentwise, LP: -142.7 ## Iteration: 9600, Proposal: Componentwise, LP: -143.5 ## Iteration: 12000, Proposal: Componentwise, LP: -145.1 ## ## Assessing Stationarity ## Assessing Thinning and ESS ## Creating Summaries ## Estimating Log of the Marginal Likelihood ## Creating Output ## ## Laplace&#39;s Demon has finished. #names(Fit1) print(Fit1) ## Call: ## LaplacesDemon(Model = Model, Data = MyData, Initial.Values = Initial.Values, ## Covar = NULL, Iterations = N, Status = N/5, Thinning = thin, ## Algorithm = &quot;MWG&quot;, Specs = NULL) ## ## Acceptance Rate: 0.35375 ## Algorithm: Metropolis-within-Gibbs ## Covariance Matrix: (NOT SHOWN HERE; diagonal shown instead) ## beta[1] beta[2] sigma2 ## 1.890044 1.890044 1.890044 ## ## Covariance (Diagonal) History: (NOT SHOWN HERE) ## Deviance Information Criterion (DIC): ## All Stationary ## Dbar 258.781 259.440 ## pD 4.338 4.237 ## DIC 263.119 263.677 ## Initial Values: ## [1] 8.2839056 0.1655676 9.9587760 ## ## Iterations: 12000 ## Log(Marginal Likelihood): -126.8415 ## Minutes of run-time: 0.27 ## Model: (NOT SHOWN HERE) ## Monitor: (NOT SHOWN HERE) ## Parameters (Number of): 3 ## Posterior1: (NOT SHOWN HERE) ## Posterior2: (NOT SHOWN HERE) ## Recommended Burn-In of Thinned Samples: 3200 ## Recommended Burn-In of Un-thinned Samples: 9600 ## Recommended Thinning: 36 ## Specs: (NOT SHOWN HERE) ## Status is displayed every 2400 iterations ## Summary1: (SHOWN BELOW) ## Summary2: (SHOWN BELOW) ## Thinned Samples: 4000 ## Thinning: 3 ## ## ## Summary of All Samples ## Mean SD MCSE ESS LB ## beta[1] 8.2478022 0.88164035 0.094717679 24.80187 6.6527588 ## beta[2] 0.1665176 0.01702535 0.003042654 12.48151 0.1240373 ## sigma2 12.5840386 2.48412045 0.125660593 830.27788 8.7682617 ## Deviance 258.7813640 2.94533567 0.202218161 136.68013 255.0038715 ## LP -143.5465138 1.23958817 0.087107901 109.45812 -146.5361533 ## Median UB ## beta[1] 8.2225293 10.1866171 ## beta[2] 0.1655676 0.1937371 ## sigma2 12.2724404 18.2059419 ## Deviance 258.1752506 265.5673438 ## LP -143.2451510 -142.1163971 ## ## ## Summary of Stationary Samples ## Mean SD MCSE ESS LB ## beta[1] 8.9659139 1.01640981 0.264577460 6.932756 6.5707408 ## beta[2] 0.1498047 0.02004864 0.007080864 3.675837 0.1240373 ## sigma2 12.4235537 2.25114323 0.223124198 148.522644 8.9164125 ## Deviance 259.4399279 2.91101586 0.562252817 35.086016 255.3738557 ## LP -143.8933516 1.24169010 0.191055588 27.276956 -146.8229317 ## Median UB ## beta[1] 9.0304813 10.6078726 ## beta[2] 0.1504279 0.1937371 ## sigma2 12.0978375 17.6632608 ## Deviance 258.9769661 265.8590137 ## LP -143.6581852 -142.1644366 Post1 &lt;- data.frame(Fit1$Posterior1,Algorithm=&quot;1.MWG&quot;) colnames(Post1) &lt;- c(&quot;beta1&quot;,&quot;beta2&quot;,&quot;sigma2&quot;,&quot;Algorithm&quot;) #head(Post1) plot(Fit1, BurnIn=0, MyData, PDF=FALSE, Parms=NULL) #plot(Fit1, BurnIn=burnin, MyData, PDF=FALSE, Parms=NULL) Exemplo 2: Adaptative Metropolis-within-Gibbs (MWG) set.seed(666) Fit2 &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values, Covar=NULL, Iterations=N, Status=N/5, Thinning=thin, Algorithm=&quot;AMWG&quot;, Specs=NULL) ## ## Laplace&#39;s Demon was called on Tue Nov 03 14:46:10 2020 ## ## Performing initial checks... ## Algorithm: Adaptive Metropolis-within-Gibbs ## ## Laplace&#39;s Demon is beginning to update... ## Iteration: 2400, Proposal: Componentwise, LP: -145.3 ## Iteration: 4800, Proposal: Componentwise, LP: -143 ## Iteration: 7200, Proposal: Componentwise, LP: -144.1 ## Iteration: 9600, Proposal: Componentwise, LP: -143.1 ## Iteration: 12000, Proposal: Componentwise, LP: -146.2 ## ## Assessing Stationarity ## Assessing Thinning and ESS ## Creating Summaries ## Creating Output ## ## Laplace&#39;s Demon has finished. #names(Fit2) #print(Fit2) Post2 &lt;- data.frame(Fit2$Posterior1,Algorithm=&quot;2.AMWG&quot;) colnames(Post2) &lt;- c(&quot;beta1&quot;,&quot;beta2&quot;,&quot;sigma2&quot;,&quot;Algorithm&quot;) #head(Post2) #plot(Fit2, BurnIn=burnin, MyData, PDF=FALSE, Parms=NULL) plot(Fit2, BurnIn=0, MyData, PDF=FALSE, Parms=NULL) Post &lt;- rbind(Post1,Post2) b1plot &lt;- ggplot(Post) + stat_function(aes(colour=&quot;0.Posterior&quot;), fun = mnormt::dmt,args = list(mean = beta1[1], S = V[1,1], df=2*a1)) + geom_density(aes(beta1, colour = Algorithm)) + theme_bw() + xlab(expression(beta[1])) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) b2plot &lt;- ggplot(Post) + stat_function(aes(colour=&quot;0.Posterior&quot;), fun = mnormt::dmt, args = list(mean = beta1[2], S = V[2,2], df=2*a1)) + geom_density(aes(beta2, colour = Algorithm)) + theme_bw() + xlab(expression(beta[2])) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) s2plot &lt;- ggplot(Post)+ stat_function(aes(colour=&quot;0.Posterior&quot;), fun = extraDistr::dinvgamma, args = list(alpha = a1, beta = b1)) + geom_density(aes(sigma2, colour = Algorithm)) + theme_bw() + xlab(expression(sigma^2)) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) multiplot(b1plot,b2plot,s2plot) Exemplo 3: Consort e Automated Factor Slice Sampler (AFSS) Consort(Fit2) ## ## ############################################################# ## # Consort with Laplace&#39;s Demon # ## ############################################################# ## Call: ## LaplacesDemon(Model = Model, Data = MyData, Initial.Values = Initial.Values, ## Covar = NULL, Iterations = N, Status = N/5, Thinning = thin, ## Algorithm = &quot;AMWG&quot;, Specs = NULL) ## ## Acceptance Rate: 0.34264 ## Algorithm: Adaptive Metropolis-within-Gibbs ## Covariance Matrix: (NOT SHOWN HERE; diagonal shown instead) ## beta[1] beta[2] sigma2 ## 1.17679178 0.02838563 9.45831955 ## ## Covariance (Diagonal) History: (NOT SHOWN HERE) ## Deviance Information Criterion (DIC): ## All Stationary ## Dbar 258.957 258.957 ## pD 4.957 4.957 ## DIC 263.915 263.915 ## Initial Values: ## [1] 8.2839056 0.1655676 9.9587760 ## ## Iterations: 12000 ## Log(Marginal Likelihood): NA ## Minutes of run-time: 0.28 ## Model: (NOT SHOWN HERE) ## Monitor: (NOT SHOWN HERE) ## Parameters (Number of): 3 ## Posterior1: (NOT SHOWN HERE) ## Posterior2: (NOT SHOWN HERE) ## Recommended Burn-In of Thinned Samples: 0 ## Recommended Burn-In of Un-thinned Samples: 0 ## Recommended Thinning: 105 ## Specs: (NOT SHOWN HERE) ## Status is displayed every 2400 iterations ## Summary1: (SHOWN BELOW) ## Summary2: (SHOWN BELOW) ## Thinned Samples: 4000 ## Thinning: 3 ## ## ## Summary of All Samples ## Mean SD MCSE ESS LB ## beta[1] 8.2386566 0.97788924 0.085914819 151.0105 6.1651888 ## beta[2] 0.1661183 0.01903241 0.002099194 209.2080 0.1294318 ## sigma2 12.5980978 2.45984860 0.071949821 1784.2000 8.7806779 ## Deviance 258.9573762 3.14870026 0.163068368 595.3985 255.1347857 ## LP -143.6311506 1.33506450 0.072413145 530.5654 -147.0582460 ## Median UB ## beta[1] 8.2641629 10.1040670 ## beta[2] 0.1655676 0.2078131 ## sigma2 12.3274919 18.1748415 ## Deviance 258.2440979 266.8653177 ## LP -143.2679823 -142.1324005 ## ## ## Summary of Stationary Samples ## Mean SD MCSE ESS LB ## beta[1] 8.2386566 0.97788924 0.085914819 151.0105 6.1651888 ## beta[2] 0.1661183 0.01903241 0.002099194 209.2080 0.1294318 ## sigma2 12.5980978 2.45984860 0.071949821 1784.2000 8.7806779 ## Deviance 258.9573762 3.14870026 0.163068368 595.3985 255.1347857 ## LP -143.6311506 1.33506450 0.072413145 530.5654 -147.0582460 ## Median UB ## beta[1] 8.2641629 10.1040670 ## beta[2] 0.1655676 0.2078131 ## sigma2 12.3274919 18.1748415 ## Deviance 258.2440979 266.8653177 ## LP -143.2679823 -142.1324005 ## ## Demonic Suggestion ## ## Due to the combination of the following conditions, ## ## 1. Adaptive Metropolis-within-Gibbs ## 2. The acceptance rate (0.3426389) is within the interval [0.15,0.5]. ## 3. At least one target MCSE is &gt;= 6.27% of its marginal posterior ## standard deviation. ## 4. Each target distribution has an effective sample size (ESS) ## of at least 100. ## 5. Each target distribution became stationary by ## 1 iteration. ## ## Quantiles of Absolute Posterior1 Correlation: ## 0% 25% 50% 75% 100% ## 0.02856704 0.03170699 0.85367854 1.00000000 1.00000000 ## ## Possibly excessive posterior correlation for a componentwise algorithm. ## ## WARNING: Diminishing adaptation did not occur. ## A new algorithm will be suggested. ## ## Laplace&#39;s Demon has not been appeased, and suggests ## copy/pasting the following R code into the R console, ## and running it. ## ## Initial.Values &lt;- as.initial.values(Fit2) ## Fit2 &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values, ## Covar=Fit2$Covar, Iterations=420000, Status=42857, Thinning=105, ## Algorithm=&quot;AFSS&quot;, Specs=list(A=Inf, B=NULL, m=100, ## n=0, w=1)) ## ## Laplace&#39;s Demon is finished consorting. Initial.Values &lt;- as.initial.values(Fit2) set.seed(666) Fit3 &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values, Covar=NULL, Iterations=N, Status=N/5, Thinning=thin, Algorithm=&quot;AFSS&quot;, Specs=list(A=Inf, B=NULL, m=100,n=0, w=1)) ## ## Laplace&#39;s Demon was called on Tue Nov 03 14:46:56 2020 ## ## Performing initial checks... ## Algorithm: Automated Factor Slice Sampler ## ## Laplace&#39;s Demon is beginning to update... ## ## Eigendecomposition will occur every 120 iterations. ## ## Iteration: 2400, Proposal: Multivariate, LP: -142.1 ## Iteration: 4800, Proposal: Multivariate, LP: -144.2 ## Iteration: 7200, Proposal: Multivariate, LP: -145.2 ## Iteration: 9600, Proposal: Multivariate, LP: -144.1 ## Iteration: 12000, Proposal: Multivariate, LP: -142.5 ## ## Assessing Stationarity ## Assessing Thinning and ESS ## Creating Summaries ## Estimating Log of the Marginal Likelihood ## Creating Output ## ## Laplace&#39;s Demon has finished. Post3 &lt;- data.frame(Fit3$Posterior1,Algorithm=&quot;3.AFSS&quot;) colnames(Post3) &lt;- c(&quot;beta1&quot;,&quot;beta2&quot;,&quot;sigma2&quot;,&quot;Algorithm&quot;) plot(Fit3, BurnIn=0, MyData, PDF=FALSE, Parms=NULL) Post &lt;- rbind(Post1,Post2,Post3) b1plot &lt;- ggplot(Post) + stat_function(aes(colour=&quot;0.Posterior&quot;), fun = mnormt::dmt,args = list(mean = beta1[1], S = V[1,1], df=2*a1)) + geom_density(aes(beta1, colour = Algorithm)) + theme_bw() + xlab(expression(beta[1])) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) b2plot &lt;- ggplot(Post) + stat_function(aes(colour=&quot;0.Posterior&quot;), fun = mnormt::dmt, args = list(mean = beta1[2], S = V[2,2], df=2*a1)) + geom_density(aes(beta2, colour = Algorithm)) + theme_bw() + xlab(expression(beta[2])) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) s2plot &lt;- ggplot(Post)+ stat_function(aes(colour=&quot;0.Posterior&quot;), fun = extraDistr::dinvgamma, args = list(alpha = a1, beta = b1)) + geom_density(aes(sigma2, colour = Algorithm)) + theme_bw() + xlab(expression(sigma^2)) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) multiplot(b1plot,b2plot,s2plot) p.interval(Post3$beta1, HPD=FALSE, MM=FALSE, plot=TRUE) ## Lower Upper ## [1,] 6.383021 10.175 ## attr(,&quot;Probability.Interval&quot;) ## [1] 0.95 set.seed(666) S0 &lt;- as.matrix(extraDistr::rinvgamma(N/thin-burnin,a1,b1)) M0 &lt;- apply(S0,1,function(s){mnormt::rmnorm(1,mean=beta1,varcov=s*V1)}) Post0 &lt;- data.frame(t(M0),S0,&quot;0.Posterior&quot;) colnames(Post0) &lt;- c(&quot;beta1&quot;,&quot;beta2&quot;,&quot;sigma2&quot;,&quot;Algorithm&quot;) Post = rbind(Post0,Post1,Post2,Post3) ggplot(Post) + theme_bw() + geom_point(aes(beta1,beta2,colour=Algorithm), shape=1) + facet_wrap(Algorithm ~ .) 7.6.3 Stan O Stan é uma plataforma de modelagem estatística de alto desempenho. Em particular, permite fazer inferência bayesiana usando o método de Monte Carlo Hamiltoniano (HMC) e a variação No-U-Turn Sampler (NUTS). Esses recursos convergem para distribuições alvo de altas dimensões muito mais rapidamente que métodos mais simples, como o amostrador de Gibbs ou outras variações do método de Metropolis-Hastings. A da linguagem utilizada é independente da plataforma e existem bibliotecas para R (rstan) e Python. https://mc-stan.org/ \\(~\\) Voltando ao Exemplo library(rstan) # Parametros do método Initial.Values &lt;- c(beta_est,sigma_est) burnin &lt;- 2000 thin &lt;- 3 N=(2000+burnin)*thin # Conjunto de dados stan_data &lt;- list(N = n, J = p, y = y, x = X) # Especificação do modelo rs_code &lt;- &#39; data { int&lt;lower=1&gt; N; int&lt;lower=1&gt; J; matrix[N,J] x; vector[N] y; } parameters { vector[J] beta; real&lt;lower=0&gt; sigma2; } model { sigma2 ~ inv_gamma(3, 100); beta ~ normal(0, sqrt(sigma2*100)); y ~ normal(x * beta, sqrt(sigma2)); }&#39; stan_mod &lt;- stan(model_code = rs_code, data = stan_data, init=Initial.Values, chains = 1, iter = N, warmup = burnin, thin = thin) ## ## SAMPLING FOR MODEL &#39;6b158d2712aa5479f81bd408884c9453&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 12000 [ 0%] (Warmup) ## Chain 1: Iteration: 1200 / 12000 [ 10%] (Warmup) ## Chain 1: Iteration: 2001 / 12000 [ 16%] (Sampling) ## Chain 1: Iteration: 3200 / 12000 [ 26%] (Sampling) ## Chain 1: Iteration: 4400 / 12000 [ 36%] (Sampling) ## Chain 1: Iteration: 5600 / 12000 [ 46%] (Sampling) ## Chain 1: Iteration: 6800 / 12000 [ 56%] (Sampling) ## Chain 1: Iteration: 8000 / 12000 [ 66%] (Sampling) ## Chain 1: Iteration: 9200 / 12000 [ 76%] (Sampling) ## Chain 1: Iteration: 10400 / 12000 [ 86%] (Sampling) ## Chain 1: Iteration: 11600 / 12000 [ 96%] (Sampling) ## Chain 1: Iteration: 12000 / 12000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.152 seconds (Warm-up) ## Chain 1: 0.676 seconds (Sampling) ## Chain 1: 0.828 seconds (Total) ## Chain 1: posterior &lt;- rstan::extract(stan_mod) Post4 &lt;- data.frame(posterior$beta[,1],posterior$beta[,2],posterior$sigma2,Algorithm=&quot;4.RStan&quot;) colnames(Post4) &lt;- c(&quot;beta1&quot;,&quot;beta2&quot;,&quot;sigma2&quot;,&quot;Algorithm&quot;) # gráficos posterioris marginais Post &lt;- rbind(Post3,Post4) %&gt;% mutate(Algorithm=ifelse(Algorithm==&quot;3.AFSS&quot;,&quot;1.LaplacesDemons&quot;,&quot;2.RStan&quot;)) b1plot &lt;- ggplot(Post) + stat_function(aes(colour=&quot;0.Posterior&quot;), fun = mnormt::dmt,args = list(mean = beta1[1], S = V[1,1], df=2*a1)) + geom_density(aes(beta1, colour = Algorithm)) + theme_bw() + xlab(expression(beta[1])) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) b2plot &lt;- ggplot(Post) + stat_function(aes(colour=&quot;0.Posterior&quot;), fun = mnormt::dmt, args = list(mean = beta1[2], S = V[2,2], df=2*a1)) + geom_density(aes(beta2, colour = Algorithm)) + theme_bw() + xlab(expression(beta[2])) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) s2plot &lt;- ggplot(Post)+ stat_function(aes(colour=&quot;0.Posterior&quot;), fun = extraDistr::dinvgamma, args = list(alpha = a1, beta = b1)) + geom_density(aes(sigma2, colour = Algorithm)) + theme_bw() + xlab(expression(sigma^2)) + ylab(&quot;Posterior&quot;) + labs(colour = &quot;Method&quot;) multiplot(b1plot,b2plot,s2plot) Post = rbind(Post0,Post4) %&gt;% mutate(Algorithm=ifelse(Algorithm==&quot;4.RStan&quot;,&quot;1.RStan&quot;,&quot;0.Posterior&quot;)) ggplot(Post) + theme_bw() + geom_point(aes(beta1,beta2,colour=Algorithm), shape=1) + facet_wrap(Algorithm ~ .) \\(~\\) stan_mod2 &lt;- stan(model_code = rs_code, data = stan_data, init=Initial.Values, chains = 2, iter = N, warmup = burnin, thin = thin) ## ## SAMPLING FOR MODEL &#39;6b158d2712aa5479f81bd408884c9453&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 12000 [ 0%] (Warmup) ## Chain 1: Iteration: 1200 / 12000 [ 10%] (Warmup) ## Chain 1: Iteration: 2001 / 12000 [ 16%] (Sampling) ## Chain 1: Iteration: 3200 / 12000 [ 26%] (Sampling) ## Chain 1: Iteration: 4400 / 12000 [ 36%] (Sampling) ## Chain 1: Iteration: 5600 / 12000 [ 46%] (Sampling) ## Chain 1: Iteration: 6800 / 12000 [ 56%] (Sampling) ## Chain 1: Iteration: 8000 / 12000 [ 66%] (Sampling) ## Chain 1: Iteration: 9200 / 12000 [ 76%] (Sampling) ## Chain 1: Iteration: 10400 / 12000 [ 86%] (Sampling) ## Chain 1: Iteration: 11600 / 12000 [ 96%] (Sampling) ## Chain 1: Iteration: 12000 / 12000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.308 seconds (Warm-up) ## Chain 1: 0.996 seconds (Sampling) ## Chain 1: 1.304 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;6b158d2712aa5479f81bd408884c9453&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 12000 [ 0%] (Warmup) ## Chain 2: Iteration: 1200 / 12000 [ 10%] (Warmup) ## Chain 2: Iteration: 2001 / 12000 [ 16%] (Sampling) ## Chain 2: Iteration: 3200 / 12000 [ 26%] (Sampling) ## Chain 2: Iteration: 4400 / 12000 [ 36%] (Sampling) ## Chain 2: Iteration: 5600 / 12000 [ 46%] (Sampling) ## Chain 2: Iteration: 6800 / 12000 [ 56%] (Sampling) ## Chain 2: Iteration: 8000 / 12000 [ 66%] (Sampling) ## Chain 2: Iteration: 9200 / 12000 [ 76%] (Sampling) ## Chain 2: Iteration: 10400 / 12000 [ 86%] (Sampling) ## Chain 2: Iteration: 11600 / 12000 [ 96%] (Sampling) ## Chain 2: Iteration: 12000 / 12000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.208 seconds (Warm-up) ## Chain 2: 1.04 seconds (Sampling) ## Chain 2: 1.248 seconds (Total) ## Chain 2: library(ggmcmc) #ggs(stan_mod2) %&gt;% ggmcmc(., file = &quot;ggmcmc.html&quot;) \\(~\\) Gráficos ggmcmc \\(~\\) 1. Histograma com as cadeias geradas combinadas. ggs(stan_mod2) %&gt;% ggs_histogram(.) 2. Gráficos das Densidades sobrepostos com cores diferentes por cadeia, permite comparar se as cadeias convergiram para distribuições semelhantes. ggs(stan_mod2) %&gt;% ggs_density(.) 3. Séries Temporais das cadeias geradas. É esperado que as cadeias geradas apresentem distribuições semelhantes em torno de uma mesma média, indicando assim que atingiu-se a estacionariedade. ggs(stan_mod2) %&gt;% ggs_traceplot(.) 4. Gráfico de Médias Móveis. É esperado que as curvas das médias das cadeias geradas se aproximem rapidamente de um mesmo valor. ggs(stan_mod2) %&gt;% ggs_running(.) 5. Densidades parcial e completa sobrepostas. Compara a última parte da cadeia (por padrão, os últimos 10% dos valores, em verde) com a cadeia inteira (em preto). Idealmente, as partes inicial e final da cadeia devem ser amostradas na mesma distribuição alvo, de modo que as densidades sobrepostas devem ser semelhantes. ggs(stan_mod2) %&gt;% ggs_compare_partial(.) 6. Gráfico de Autocorrelação. Espera-se alta correlação apenas no primeiro lag. Quando há um comportamento diferente do esperado, deve-se aumentar o tamanho dos saltos (thin) entre as observações da cadeia gerada que serão consideradas na amostra final. ggs(stan_mod2) %&gt;% ggs_autocorrelation(.) 6. Correlação Cruzada. Quando há alta correlação entre os parâmetros é possível que a convergência da cadeia seja mais lenta. ggs(stan_mod2) %&gt;% ggs_crosscorrelation(.) ggs(stan_mod2) %&gt;% ggs_pairs(., lower = list(continuous = &quot;density&quot;)) \\(~\\) \\(~\\) 7.6.4 MLG O modelos lineares generalizados (MLG) são uma extensão natural dos modelos lineares para casos em que a distribuição da variável resposta não é normal. Como exemplo, vamos considerar o particular caso onde a resposta é binária, conhecido como regressão logística. Considere \\(Y_1,\\ldots,Y_n\\) condicionalmente independentes tais que \\(Y_i|\\theta_i \\sim \\textit{Ber}(\\theta_i)\\), em que \\(\\theta_i\\) é tal que \\(\\log\\left(\\dfrac{\\theta_i}{1-\\theta_i}\\right) = \\boldsymbol x_i&#39; \\boldsymbol\\beta\\) ou, em outras palavras, \\(\\theta_i = \\dfrac{1}{1+e^{\\boldsymbol x_i&#39; \\boldsymbol\\beta}} = \\dfrac{e^{-\\boldsymbol x_i&#39; \\boldsymbol\\beta}}{1+e^{-\\boldsymbol x_i&#39; \\boldsymbol\\beta}}\\) com \\(\\boldsymbol x_i\\) as covariáveis da \\(i\\)-ésima observação e o vetor de parâmetros \\(\\boldsymbol\\beta=(\\beta_1,\\ldots,\\beta_p)\\). \\(~\\) Exemplo. Considere as variáveis vs (0 = motor em forma de V, 1 = motor reto) e mpg (milhas/galão(EUA)) do conjunto de dados mtcars do R. Suponha um modelo de regressão logística para a variável resposta vs com a covariável mpg em que, a priori, \\(\\beta_i \\sim \\textit{Laplace}(0,b_i)\\), \\(i=1,2\\), independentes. Deste modo, a posteriori é dada por \\(f(\\boldsymbol\\beta | \\boldsymbol{y},\\boldsymbol{x})\\) \\(\\propto f(\\boldsymbol{y}|\\boldsymbol\\beta,\\boldsymbol{x})f(\\boldsymbol\\beta)\\) \\(\\propto \\displaystyle\\prod_{i=1}^{n} \\left(\\dfrac{1}{1+e^{\\boldsymbol x_i&#39; \\boldsymbol\\beta}}\\right)^{y_i}\\left(\\dfrac{e^{\\boldsymbol x_i&#39; \\boldsymbol\\beta}}{1+e^{\\boldsymbol x_i&#39; \\boldsymbol\\beta}}\\right)^{1-y_i} \\prod_{j=1}^{p} \\dfrac{1}{2b_i} e^{-\\frac{|\\beta_i|}{b_i}}\\). library(rstan) dados &lt;- as_tibble(mtcars) # mpg: Miles/(US)gallon ; vs: Engine(0=V-shaped,1=straight) dados %&gt;% ggplot(aes(group=as.factor(vs),y=mpg,fill=as.factor(vs))) + geom_boxplot() + scale_fill_discrete(name=&quot;vs&quot;) + theme_bw() y &lt;- dados %&gt;% select(vs) %&gt;% pull() x &lt;- dados %&gt;% select(mpg) %&gt;% pull() n &lt;- length(y) X &lt;- as.matrix(cbind(1,x)) p &lt;- ncol(X) stan_data &lt;- list(N = n, J = p, y = y, x = X) rs_code &lt;- &#39; data { int&lt;lower=1&gt; N; int&lt;lower=1&gt; J; int&lt;lower=0,upper=1&gt; y[N]; matrix[N,J] x; } parameters { vector[J] beta; } model { beta ~ double_exponential(0, 100); y ~ bernoulli_logit(x * beta); }&#39; N=2000 thin=10 burnin=1000 stan_log &lt;- stan(model_code = rs_code, data = stan_data, init = c(0,0), chains = 1, iter = N*thin, warmup = burnin, thin = thin) ## ## SAMPLING FOR MODEL &#39;6525180a46ff3a51f06bbb110d962a1c&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 1: Iteration: 1001 / 20000 [ 5%] (Sampling) ## Chain 1: Iteration: 3000 / 20000 [ 15%] (Sampling) ## Chain 1: Iteration: 5000 / 20000 [ 25%] (Sampling) ## Chain 1: Iteration: 7000 / 20000 [ 35%] (Sampling) ## Chain 1: Iteration: 9000 / 20000 [ 45%] (Sampling) ## Chain 1: Iteration: 11000 / 20000 [ 55%] (Sampling) ## Chain 1: Iteration: 13000 / 20000 [ 65%] (Sampling) ## Chain 1: Iteration: 15000 / 20000 [ 75%] (Sampling) ## Chain 1: Iteration: 17000 / 20000 [ 85%] (Sampling) ## Chain 1: Iteration: 19000 / 20000 [ 95%] (Sampling) ## Chain 1: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.182 seconds (Warm-up) ## Chain 1: 2.144 seconds (Sampling) ## Chain 1: 2.326 seconds (Total) ## Chain 1: print(stan_log) ## Inference for Stan model: 6525180a46ff3a51f06bbb110d962a1c. ## 1 chains, each with iter=20000; warmup=1000; thin=10; ## post-warmup draws per chain=1900, total post-warmup draws=1900. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## beta[1] -10.15 0.08 3.51 -17.92 -12.17 -9.65 -7.61 -4.64 1763 1 ## beta[2] 0.50 0.00 0.17 0.22 0.37 0.47 0.60 0.89 1735 1 ## lp__ -13.89 0.03 1.05 -16.67 -14.31 -13.55 -13.15 -12.88 1733 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Nov 10 14:16:19 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). library(bayesplot) post_log &lt;- extract(stan_log, inc_warmup = TRUE, permuted = FALSE) color_scheme_set(&quot;mix-brightblue-gray&quot;) mcmc_trace(post_log, pars = c(&quot;beta[1]&quot;, &quot;beta[2]&quot;), n_warmup = 0, facet_args = list(nrow = 2, labeller = label_parsed)) mcmc_acf(post_log, pars = c(&quot;beta[1]&quot;, &quot;beta[2]&quot;)) mcmc_areas(post_log,pars = c(&quot;beta[1]&quot;, &quot;beta[2]&quot;),prob=0.9) multiplot(mcmc_areas(post_log,pars = c(&quot;beta[1]&quot;),prob=0.9), mcmc_areas(post_log,pars = c(&quot;beta[2]&quot;),prob=0.9)) library(ggmcmc) #ggs(stan_log) %&gt;% ggmcmc(., file = &quot;ggmcmc_log.html&quot;) ggs(stan_log) %&gt;% ggs_pairs(., lower = list(continuous = &quot;density&quot;)) \\(~\\) Existe uma biblioteca para modelos lineares bayesianos usando o Stan chamada rstanarm. Nesta biblioteca, a função stan_glm pode ser utilizada para o ajuste de MLGs sob o ponto de vista bayesiano. https://cran.r-project.org/web/packages/rstanarm/ \\(~\\) \\(~\\) 7.6.5 Modelos Dinâmicos A Biblioteca walker para do R que usa o RStan para fazer inferência bayesiana em modelos lineares com coeficientes variando no tempo (modelos dinâmicos). Modelo de Regressão Dinâmico Bayesiano \\[y_t = \\boldsymbol x_t~\\boldsymbol\\beta_t + \\epsilon_t ~,~~ \\epsilon_t \\sim \\textit{Normal}(0,\\sigma_y^2)\\] \\[\\boldsymbol\\beta_{t+1} = \\boldsymbol\\beta_t + \\boldsymbol\\eta_t ~,~~ \\boldsymbol\\eta_t \\sim \\textit{Normal}_k(0,D)\\] onde \\(y_t\\): variável resposta no instante \\(t\\); \\(\\boldsymbol x_t\\): vetor com \\(k\\) variáveis preditoras no instante \\(t\\); \\(\\epsilon_t\\) e \\(\\boldsymbol\\eta_t\\): ruídos brancos; \\(\\boldsymbol\\beta_t\\): vetor dos \\(k\\) coeficientes de regressão no instante \\(t\\); \\(D=\\textit{diag}({\\sigma}_{\\eta_i})\\); \\(\\boldsymbol\\sigma=\\left(\\sigma_y,{\\sigma}_{\\eta_1},\\ldots,{\\sigma}_{\\eta_k}\\right)\\): vetor de parâmetros de variância. As distribuição a piori são dadas por \\[\\beta_1 \\sim \\textit{Normal}(m_\\beta,{s}_\\beta^2)\\] \\[\\sigma_i^2 \\sim \\textit{NormalTrunc}({m}_{\\sigma_i},{s}_{\\sigma_i}^2)\\] Sobre a biblioteca walker: https://cran.r-project.org/web/packages/walker/vignettes/walker.html https://rdrr.io/cran/walker/man/walker.html \\(~\\) 7.6.5.1 Dados de Amoxicilina (fonte: CEA) Dados mensais de venda de Amoxicilina e resistência da bacteria E. coli no período de 01/2008 à 12/2016. A venda de Amoxicilina foi avaliada pela média mensal das doses diárias definidas por 1000 habitantes-dia (DDD/1000hab/dia), que indica a quantidade média de determinado antibiótico que uma dada população consome diariamente (IMS Health Brazil/Pfizer). Para avaliar a resistência da bactéria E. coli à Amoxicilina foram utilizados dados obtidos a partir de amostras da rede de apoio do laboratório DASA, que atende principalmente a rede privada de assistência à saúde mas também inclui hospitais que atendem pacientes pelo Sistema Único de Saúde (SUS). Foram incluídas amostras resultantes de exames de sangue e urina positivas, avaliando a proporção de cepas isoladas resistentes à amoxicilina. Objetivo. Avaliar o impacto da regulamentação RDC 44 da ANVISA que obriga a retenção da prescrição médica para a venda de antimicrobianos, implementada em 26 de Outubro de 2010. library(lubridate) dados = read_csv(&quot;Amoxicillin.csv&quot;) %&gt;% select(Período=Periodo,Vendas=DDD1000hab_dia,Resistência=Resistencia_Amoxicilina.Clavulanato.k) dados$Vendas[61:62]=mean(c(2.073005,2.173923)) mV=mean(dados$Vendas); sdV=sd(dados$Vendas) mR=mean(dados$Resistência); sdR=sd(dados$Resistência) Tr &lt;- sdV/sdR # var. aux. para transformação dos eixos dados %&gt;% ggplot(aes(x=Período)) + theme_bw() + geom_line(aes(y=Vendas,colour=&quot;Vendas&quot;),lwd=1) + theme(legend.position=&quot;bottom&quot;) + geom_line(aes(x=Período,y=((Resistência-mR)*Tr+mV),colour=&quot;Resistência&quot;),lwd=1)+ labs(y=&quot;Vendas de Amoxicilina (DDD/1000 hab./dia)&quot;,colour=&quot;&quot;) + scale_y_continuous(sec.axis = sec_axis(~./Tr+(mR-mV/Tr), name = &quot;Prop. de Amostras de E.Colli Resistentes&quot;)) Para verificar se o efeito da venda de antimicrobianos influencia na resistência bacteriana e identificar possíveis mudanças após a implementação da lei, foi considerado um modelo de regressão dinâmico bayesiano, descrito a seguir. \\[(Y_t-\\bar{Y_0}) = \\beta_t (X_t-\\bar{X_0}) + \\epsilon_t ~,~~ \\epsilon_t \\sim Normal(0,\\sigma_y^2)\\] \\[\\beta_{t+1} = \\beta_t + W_t ~,~~ W_t \\sim Normal(0,\\sigma_r^2)\\] \\[\\sigma_y^2 \\sim Half-Normal(100)\\] \\[\\beta_t \\sim Normal(0,100)\\] \\[\\sigma_r^2 \\sim Half-Normal(100)\\] * \\(Y_t\\): proporção de testes de resistência positiva do microbiano E. coli no instante \\(t\\); * \\(X_t\\): quantidade de doses consumidas de Amoxicilina (DDD/1000hab/dia) no instante \\(t\\); * \\(\\beta_t\\): parâmetro que representa o efeito da venda do antimicrobiano na resistência bacteriana no instante \\(t\\); * \\(\\epsilon_t\\) e \\(W_t\\): ruídos brancos. library(walker) set.seed(666) # resistência média e venda média do período anterior a RDC 44 mAntes = dados %&gt;% filter(year(Período)&lt;2011) %&gt;% summarise(mean(Vendas),mean(Resistência)) %&gt;% t() fit1 &lt;- dados %&gt;% mutate(Vendas=Vendas-mAntes[1],Resistência=Resistência-mAntes[2]) %&gt;% walker(data=., formula = Resistência ~ -1+rw1(~ -1+Vendas,beta=c(0,100), sigma=c(0,100)),sigma_y_prior = c(0,100), chain=2) ## ## SAMPLING FOR MODEL &#39;walker_lm&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.002 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 20 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 8.376 seconds (Warm-up) ## Chain 1: 7.431 seconds (Sampling) ## Chain 1: 15.807 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;walker_lm&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.001 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 6.577 seconds (Warm-up) ## Chain 2: 6.288 seconds (Sampling) ## Chain 2: 12.865 seconds (Total) ## Chain 2: #Default: chain=4, iter=2000, warmup=1000, thin=1 multiplot( #mcmc_areas(as.matrix(fit1$stanfit), regex_pars = c(&quot;beta_fixed&quot;), prob=0.9), mcmc_areas(as.matrix(fit1$stanfit), regex_pars = c(&quot;sigma_rw1&quot;), prob=0.9), mcmc_areas(as.matrix(fit1$stanfit), regex_pars = c(&quot;sigma_y&quot;), prob=0.9)) plot_coefs(fit1, scales = &quot;free&quot;, alpha=0.8) + theme_bw() + scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017)) pp_check(fit1, alpha=0.8) + theme_bw() + scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017)) new_data &lt;- data.frame(Vendas=seq(dados$Vendas[108]-mAntes[1],-0.4,length.out=12)+c(0,rnorm(11,0,0.08))) pred1 &lt;- predict(fit1, new_data) plot_predict(pred1, alpha=0.8) + scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017)) new_data &lt;- data.frame(Vendas=seq(dados$Vendas[108]-mAntes[1],-1.1,length.out=12)+c(0,rnorm(11,0,0.08))) pred1 &lt;- predict(fit1, new_data) plot_predict(pred1, alpha=0.8) + scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017)) 7.6.5.2 Extensões: Efeitos mais suaves e modelos não gaussianos Ao modelar os coeficientes de regressão como uma passeio aleatório simples, as estimativas posteriores desses coeficientes podem ter grandes variações de curto prazo que podem não ser realistas na prática. Uma maneira de impor mais suavidade às estimativas é alternar dos coeficientes do passeio aleatório para coeficientes de passeio aleatório de segunda ordem integrados: \\[\\boldsymbol\\beta_{t+1} = \\boldsymbol\\beta_t + \\boldsymbol\\nu_t\\] \\[\\boldsymbol\\nu_{t+1} = \\boldsymbol\\nu_t + \\boldsymbol\\eta_t ~,~~ \\boldsymbol\\eta_t \\sim \\textit{Normal}_k(0,D)\\] library(walker) set.seed(666) fit2 &lt;- dados %&gt;% mutate(Vendas=Vendas-mAntes[1],Resistência=Resistência-mAntes[2]) %&gt;% walker(data=., formula = Resistência ~ -1+rw2(~ -1+Vendas, beta=c(0,100), sigma=c(0,100),nu=c(0,100)),sigma_y_prior=c(0,100), chain=2) ## ## SAMPLING FOR MODEL &#39;walker_lm&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.002 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 20 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 8.426 seconds (Warm-up) ## Chain 1: 8.302 seconds (Sampling) ## Chain 1: 16.728 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;walker_lm&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.001 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 8.87 seconds (Warm-up) ## Chain 2: 7.111 seconds (Sampling) ## Chain 2: 15.981 seconds (Total) ## Chain 2: #Default: chain=4, iter=2000, warmup=1000, thin=1 multiplot( mcmc_areas(as.matrix(fit2$stanfit), regex_pars = c(&quot;sigma_rw2&quot;), prob=0.9), mcmc_areas(as.matrix(fit2$stanfit), regex_pars = c(&quot;sigma_y&quot;), prob=0.9)) plot_coefs(fit2, scales = &quot;free&quot;, alpha=0.8) + theme_bw() + scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017)) pp_check(fit2, alpha=0.8) + theme_bw() + scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017)) new_data &lt;- data.frame(Vendas=seq(dados$Vendas[108]-mAntes[1],-0.4,length.out=12)+c(0,rnorm(11,0,0.08))) pred2 &lt;- predict(fit2, new_data) plot_predict(pred2, alpha=0.8) + scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017)) new_data &lt;- data.frame(Vendas=seq(dados$Vendas[108]-mAntes[1],-1.1,length.out=12)+c(0,rnorm(11,0,0.08))) pred2 &lt;- predict(fit2, new_data) plot_predict(pred2, alpha=0.8) + scale_x_continuous(breaks=seq(1,length(dados$Período)+1,12),labels=c(unique(year(dados$Período)),2017)) \\(~\\) A função walker_glm estende o pacote para lidar com observações de com distribuição de Poisson e Binomial, usando a metodologia similar à mencionada acima. \\(~\\) "],["medprob.html", "A Breve Resumo de Medida e Probabilidade A.1 Conceitos Básicos A.2 Valor Esperado de \\(X\\) (OU uma ideia da tal Integral de Lebesgue) A.3 Funções de Variáveis Aleatórias A.4 Função de Distribuição A.5 Probabilidade Condicional", " A Breve Resumo de Medida e Probabilidade Essa seção tem o objetivo de apresentar as ideias de probabilidade como uma medida e da integral de Lebesgue. Para maiores detalhes, ver Ash and Doleans-Dade (2000), Billingsley (1986), Shiryaev (1996) ou, para uma versão mais resumida, os Apêndices de Schervish (2012). A.1 Conceitos Básicos \\(\\Omega\\): espaço amostral (um conjunto não vazio). \\(\\mathcal{A}\\): \\(\\sigma\\)-álgebra de subconjuntos de \\(\\Omega\\), isto é, \\(\\Omega \\in \\mathcal{A}\\); \\(A \\in \\mathcal{A} \\Longrightarrow A^{c} \\in \\mathcal{A}\\); \\(\\displaystyle A_1, A_2, \\ldots \\in \\mathcal{A} \\Longrightarrow \\bigcup_{i\\geq1} A_i \\in \\mathcal{A}\\). Os elementos de \\(\\mathcal{A}\\) são chamados de eventos e serão denotados por \\(A, B, C, \\ldots, A_1, A_2, \\ldots\\) Uma coleção de eventos \\(A_1,A_2,\\ldots\\) forma uma partição de \\(\\Omega\\) se \\(A_i \\cap A_j = \\varnothing\\), \\(\\forall i \\neq j\\), e \\(\\displaystyle \\bigcup_{i=1}^{\\infty} A_i = \\Omega\\). \\((\\Omega, \\mathcal{A})\\): espaço mensurável. Usualmente, denota-se a \\(\\sigma\\)-álgebra gerada por um conjunto \\(\\mathcal{C}\\) como \\(\\sigma(\\mathcal{C})\\). Por exemplo: \\(\\sigma(\\Omega) = \\{\\varnothing,\\Omega\\}~~\\) (\\(\\sigma\\)-ágebra trivial); Para \\(A \\subset \\Omega\\), \\(\\sigma(A) = \\{\\varnothing, A, A^c, \\Omega\\}\\); \\(\\sigma(\\mathbb{N}) = \\mathcal{P}(\\mathbb{N})~~\\) (partes de \\(\\mathbb{N}\\), todos o subconjuntos de \\(\\mathbb{N}\\)); \\(\\sigma\\left(\\left\\{(-\\infty,x): x \\in \\mathbb{R}\\right\\}\\right) = \\mathcal{B}\\left(\\mathbb{R}\\right)~~\\) (borelianos de \\(\\mathbb{R}\\)) \\(~\\) Definição: A função \\(\\mu: \\mathcal{A} \\longrightarrow \\bar{\\mathbb{R}}_+\\) é uma medida se 1. \\(\\mu(\\varnothing) = 0\\); 2. \\(\\displaystyle A_1, A_2, \\ldots \\in \\mathcal{A}\\) com \\(A_i \\bigcap A_j = \\varnothing\\) , \\(\\forall i \\neq j\\) , \\(\\displaystyle \\mu\\left(\\bigcup_{i \\geq 1} A_i\\right) = \\sum_{i \\geq 1} \\mu\\left(A_i\\right)\\). \\((\\Omega,\\mathcal{A}, \\mu)\\) é chamado de espaço de medida. \\(~\\) Exemplo 1 (medida de contagem): Seja \\(\\Omega\\) um conjunto não vazio e \\(A\\subseteq \\Omega\\). Defina \\(\\nu(A)=|A|\\) como o número de elementos (cardinalidade) de \\(A\\). Assim, \\(\\nu(\\Omega) &gt; 0\\), \\(\\nu(\\varnothing)=0\\) e, se \\((A_n)_{n \\geq 1}\\) é uma sequência de eventos disjuntos, então \\(\\nu(\\cup A_n) = \\sum \\nu(A_n)\\). Note que \\(\\nu(A)=\\infty\\) é possivel se \\(\\Omega\\) tem infinitos elementos. \\(~\\) Exemplo 2 (medida de Lebesgue): Seja \\(\\Omega=\\mathbb{R}\\) e \\(A\\subseteq \\Omega\\) um intervalo. Se \\(A\\) é limitado, defina \\(\\lambda(A)\\) como o comprimento do intervalo \\(A\\). Se \\(A\\) não é limitado, \\(\\lambda(A)=\\infty\\). Note que \\(\\lambda(\\mathbb{R})=\\infty\\), \\(\\lambda(\\varnothing)=0\\) e, se \\(A_1 \\cap A_2 = \\varnothing\\) e \\(A_1 \\cup A_2\\) é um intervalo (ou uma união de intervalos disjuntos), então \\(\\lambda(A_1 \\cup A_2) = \\lambda(A_1) + \\lambda(A_2)\\). \\(~\\) Exemplo 3: Seja \\(f: \\mathbb{R} \\longrightarrow \\mathbb{R}_+\\) uma função contínua e não nula. Para cada intervalo \\(A\\), defina \\(\\displaystyle \\mu(A) = \\int_A f(x) dx = \\int_{\\mathbb{R}} \\mathbb{I}_A(x) f(x) dx\\). Então, \\(\\mu(\\mathbb{R})&gt;0\\), \\(\\mu(\\varnothing)=0\\) e, se \\(A_1 \\cap A_2 = \\varnothing\\) e \\(A_1 \\cup A_2\\) é um intervalo (ou uma união de intervalos disjuntos), então \\(\\mu(A_1 \\cup A_2) = \\mu(A_1) + \\mu(A_2)\\). \\(~\\) Se \\(\\mu(\\Omega) &lt; \\infty\\) dizemos que \\(\\mu\\) é uma medida finita. Se existe uma partição enumerável de \\(\\Omega\\), \\(A_1,A_2,\\ldots\\), tal que cada elemento da partição tem medida finita, \\(\\mu(A_i)&lt;\\infty\\), \\(\\forall i\\), dizemos que \\(\\mu\\) é uma medida \\(\\sigma\\)-finita. \\(~\\) Definição: \\(P: \\mathcal{A} \\longrightarrow [0,1]\\) é uma medida de probabilidade se 1. \\(P(\\Omega) = 1\\); 2. \\(\\displaystyle A_1, A_2, \\ldots \\in \\mathcal{A}\\) com \\(A_i \\bigcap A_j = \\varnothing\\) , \\(\\displaystyle P\\left(\\bigcup_{i \\geq 1} A_i\\right) = \\sum_{i \\geq 1} P\\left(A_i\\right)\\). \\((\\Omega, \\mathcal{A}, P)\\): espaço de probabilidade \\(~\\) Definição: Seja \\((\\Omega,\\mathcal{A})\\) e \\((\\mathfrak{X},\\mathcal{F})\\) dois espaços mensuráveis. Uma função \\(X: \\Omega \\longrightarrow \\mathfrak{X}\\) é chamado de quantidade aleatória se é uma função mensurável, isto é, se \\(\\forall B \\in \\mathcal{F}\\), o evento \\(A = X^{-1}(B)\\) \\(= \\left\\{\\omega \\in \\Omega:~X(\\omega)\\in B\\right\\}\\) pertence à \\(\\sigma\\)-álgebra original \\(\\mathcal{A}\\). Se \\(\\mathfrak{X} = \\mathbb{R}\\) e \\(\\mathcal{F}=\\mathcal{B}(\\mathbb{R})\\) (\\(\\sigma\\)-álgebra de Borel), \\(X\\) é chamada variável aleatória (v.a.). Considere \\((\\Omega,\\mathcal{A},P)\\). A medida de probabilidade \\(P_X\\) induzida por \\(X\\) recebe o nome de distribuição de \\(X\\). Se \\(B \\in \\mathcal{F}\\) e \\(A = \\{\\omega \\in \\Omega : X(\\omega) \\in B\\} \\in \\mathcal{A}\\), a medida induzida por \\(X\\) é \\[P_X(B) = P_X\\left(X \\in B\\right) = P\\left(\\{\\omega \\in \\Omega : X(\\omega) \\in B\\}\\right) = P(A)~.\\] A distribuição de \\(X\\) é dita ser discreta se existe um conjunto enumerável \\(A \\subseteq \\mathfrak{X}\\) tal que \\(P_X(A)=1\\). A distribuição de \\(X\\) é contínua se \\(P_X\\left(\\{x\\}\\right)=0\\) para todo \\(x \\in \\mathfrak{X}\\). \\(~\\) \\(~\\) A.2 Valor Esperado de \\(X\\) (OU uma ideia da tal Integral de Lebesgue) Por simplicidade, considere o espaço \\(\\Big(\\Omega = [0,1]~,~~ \\mathcal{A} = \\mathcal{B}\\left([0,1]\\right)~,~~ P=\\lambda\\Big)\\). Seja \\(X: \\Omega \\longrightarrow \\mathbb{R}_+\\) uma variável aleatória discreta, assumindo valores não negativos \\(\\mathfrak{X}=\\{x_1,x_2,\\ldots,x_k\\}\\) com probabilidades \\(\\{p_1,p_2,\\ldots,p_k\\}\\). Nos cursos básicos de probabilidade é visto que o valor esperado (ou esperança) de \\(X\\) é \\(E[X] =\\) \\(\\sum x_i P(X=x_i) =\\) \\(\\sum x_i p_i\\). Podemos definir essa v.a. como \\(X(\\omega) = \\left\\{\\begin{array}{ll} x_1, &amp; \\omega \\in [0,p_1] = A_1 \\\\ x_2, &amp; \\omega \\in [p_1,p_1+p_2] = A_2 \\\\ \\vdots &amp; \\\\ x_j, &amp; \\omega \\in \\left[\\displaystyle\\sum_{i=1}^{j-1} p_i,\\sum_{i=1}^{j} p_i\\right] = A_j \\\\ \\vdots &amp; \\\\ x_k, &amp; \\omega \\in [1-p_k,1] = A_k \\end{array}\\right.\\) Note que a medida \\(\\lambda\\) define uma distribuição uniforme no espaço \\((\\Omega,\\mathcal{A})\\). Assim, temos que \\(P_X(X=x_1)\\) \\(=P\\left(X^{-1}(x_1)\\right)\\) \\(=P\\left(\\{\\omega \\in \\Omega : X(\\omega)=x_1\\}\\right)\\) \\(=P(A_1)\\) \\(=\\lambda\\left([0,p_1]\\right)\\) \\(=p_1\\), \\(P_X(X=x_j)\\) \\(=P\\left(\\{\\omega \\in \\Omega : X(\\omega)=x_j\\}\\right)\\) \\(=\\lambda\\left(\\left[\\sum_{i=1}^{j-1} p_i,\\sum_{i=1}^{j} p_i\\right]\\right)\\) \\(=p_j ~,~\\) \\(j \\in \\{2,\\ldots,k\\}\\). \\(~\\) Definição: Uma função mensurável \\(X: \\Omega \\longrightarrow \\mathbb{R}_+\\) é dita simples se assumir um número finito de valores. \\(~\\) Definição: Considere um espaço de probabilidade \\((\\Omega, \\mathcal{A}, P)\\), \\(X:\\Omega\\longrightarrow \\mathbb{R}_+\\) v.a. assumindo valores \\(\\{x_1,x_2,\\ldots,x_k\\}\\) e \\(A_1,A_2,\\ldots,A_k\\) eventos disjuntos em \\(\\mathcal{A}\\). Seja \\(\\displaystyle X(\\omega) = \\sum_{i=1}^{k} x_i ~\\mathbb{I}_{A_i}(\\omega)\\), uma função simples com \\(A_i = X^{-1}(x_i)\\), \\(i=1,\\ldots,k\\). A integral de Lebesgue de \\(X\\) em relação à medida \\(P\\) é \\[E[X] = \\int_\\Omega X dP = \\sum_{i=1}^{k} x_i P(A_i).\\] \\(~\\) Propriedades: se \\(X, Y: \\Omega \\longrightarrow \\mathbb{R}_+\\) são funções simples, então 1. \\(\\displaystyle\\int_\\Omega X dP \\geq 0\\); 2. \\(\\displaystyle\\int_\\Omega cX dP = c\\int_\\Omega X dP\\); 3. \\(\\displaystyle\\int_\\Omega (X+Y) dP = \\int_\\Omega X dP + \\int_\\Omega Y dP\\). \\(~\\) Demo 1. Segue de \\(x_i \\geq 0\\) e \\(P(A_i) \\geq 0\\). Demo 2. Para \\(X\\) v.a. temos \\(X =\\displaystyle \\sum_{i=1}^kx_i~\\mathbb{I}_{A_i}\\) e \\(cX = \\displaystyle\\sum_{i=1}^k c~x_i ~\\mathbb{I}_{A_i}\\). Logo, \\(\\displaystyle\\int_\\Omega cX~dP = \\sum_{i=1}^k c~x_i~P(A_i)\\) \\(=\\displaystyle c\\sum_{i=1}^kx_i P(A_i) = c\\int_\\Omega X dP\\). Demo 3. \\(X = \\sum_{i=1}^kx_i~\\mathbb{I}_{A_i}\\) e \\(Y = \\sum_{j=1}^ly_j~\\mathbb{I}_{B_j}\\). \\(X + Y\\) \\(=\\displaystyle \\sum_{i=1}^k x_i ~\\mathbb{I}_{A_i} + \\sum_{j=1}^l y_j~\\mathbb{I}_{B_j}\\) \\(=\\displaystyle\\sum_{i=1}^k\\sum_{j=1}^lx_i~\\mathbb{I}_{A_i\\cap B_j} + \\sum_{i=1}^k\\sum_{j=1}^ly_j~\\mathbb{I}_{A_i\\cap B_j}\\) \\(=\\displaystyle\\sum_{i=1}^k\\sum_{j=1}^l(x_i+y_j)~\\mathbb{I}_{A_i\\cap B_j}\\). \\(\\displaystyle \\int_\\Omega (X + Y) dP\\) \\(=\\displaystyle \\sum_{i=1}^k\\sum_{j=1}^l (x_i + y_j)P(A_i\\cap B_j)\\) \\(=\\displaystyle\\sum_{i=1}^k\\sum_{j=1}^l x_iP(A_i\\cap B_j) + \\sum_{i=1}^k\\sum_{j=1}^l y_jP(A_i\\cap B_j)\\) \\(=\\displaystyle\\sum_{i=1}^k x_i P(A_i) + \\sum_{j=1}^l y_j P(B_j)\\) \\(=\\displaystyle\\int_\\Omega X dP + \\int_\\Omega Y dP\\). \\(~\\) \\(~\\) A generalização da integral de Lebesgue é feita usando resultados como o Lema de Fatou e os teoremas da convergência monótona e da convergência dominada. Aqui será apresentado apenas uma ideia dessa extensão. Para maiores detalhes, veja as referências citadas anteriormente (Ash and Doleans-Dade 2000; Schervish 2012; Billingsley 1986; Shiryaev 1996). \\(~\\) \\(~\\) Definição: Seja \\(X:\\Omega\\longrightarrow \\mathbb{R}_+\\) uma função mensurável não negativa e considere o conjunto de funções \\(\\mathcal{C}_X\\) \\(= \\{ f:\\Omega\\longrightarrow \\mathbb{R}_+~,~~f~~\\text{simples}~,~~f \\leq X\\}\\). O valor esperado de \\(X\\) é \\[E[X]=\\int_\\Omega XdP=\\sup\\left\\{\\int_\\Omega fdP: f\\in \\mathcal{C}_X\\right\\}~.\\] \\(~\\) Resultado: Para toda função \\(X:\\Omega \\longrightarrow \\mathbb{R}_+\\), existe uma sequência \\((X_n)_{n\\geq 1}\\) de funções simples não-negativas tais que \\(X_n(\\omega)\\leq X_{n+1}(\\omega)\\), \\(\\forall \\omega \\in \\Omega\\), \\(\\forall n \\in \\mathbb{N}\\) com \\(X_n(\\omega)\\uparrow X(\\omega)\\), \\(\\forall \\omega \\in \\Omega\\). \\(~\\) Exemplo de sequência \\((X_n)_{n\\geq 1}\\) atendendo as condições anteriores Para cada \\(n\\), considere \\(1+n2^n\\) conjuntos em \\(\\mathcal{A}:\\) \\(E_j^n = \\left\\{\\omega \\in \\Omega: \\dfrac{j}{2^n} \\leq X(\\omega) \\leq \\dfrac{j+1}{2^n} \\right\\}\\), \\(j = 0,1,\\ldots,n2^n-1.\\) \\(E_{n2^n}^n = \\Big\\{ \\omega \\in \\Omega: X(\\omega)\\geq n \\Big\\}\\) e defina \\(\\displaystyle X_n(\\omega) = \\sum_{j=0}^{n2^n} \\dfrac{j}{2^n} ~\\mathbb{I}_{E_j^n}(\\omega)\\). Pode-se provar que \\((X_n)_{n\\geq 1}\\) é tal que \\(X_n\\) é simples, \\(\\forall n \\geq 1\\) \\(X_n \\leq X_{n+1}\\) \\(X_{n}(\\omega) \\uparrow X(\\omega)\\) \\(~\\) A primeira função dessa sequência é \\(X_1(\\omega)\\) \\(= \\displaystyle\\sum_{i=0}^2 \\frac{i}{2}~\\mathbb{I}_{{E}_i^1}(\\omega)\\) \\(=\\displaystyle\\left\\{\\begin{array}{ll}0,&amp;\\omega\\in{E}_0^1\\\\ 0.5,&amp;\\omega\\in{E}_1^1\\\\1,&amp;\\omega\\in{E}_2^1 \\end{array}\\right.\\). O gráfico a seguir mostra os quatro primeiras funções da sequência \\(\\left(X_n\\right)_{n\\geq 1}\\) e é possível ter uma ideia da convergência para \\(X\\). \\(~\\) \\(~\\) Resultado: \\(X,Y: \\Omega \\longrightarrow\\mathbb{R}_+,\\) com \\(X\\leq Y\\). Então \\(E[X] \\leq E[Y]\\). Demo: Como \\(X \\leq Y\\) (isto é, \\(X(\\omega) \\leq Y(\\omega)\\) \\(\\forall \\omega \\in \\Omega\\)), \\(\\mathcal{C}_X \\subseteq \\mathcal{C}_Y\\) \\(\\Rightarrow \\sup\\left\\{\\displaystyle\\int_\\Omega f~dP:~ f\\in \\mathcal{C}_X\\right\\} \\leq \\sup\\left\\{\\displaystyle\\int_\\Omega g~dP:~ g\\in \\mathcal{C}_Y\\right\\}\\) \\(\\Rightarrow \\displaystyle\\int_\\Omega XdP \\leq \\displaystyle\\int_\\Omega YdP\\). \\(~\\) Definição: Seja \\(X:\\Omega \\longrightarrow\\mathbb{R}_+\\) e \\(E \\in \\mathcal{A}\\) definimos \\(E(X~\\mathbb{I}_E) = \\displaystyle\\int_EXdP\\) \\(=\\displaystyle\\int_\\Omega X~\\mathbb{I}_EdP\\). Se \\(E,F \\in \\mathcal{A}\\) com \\(E\\subseteq F\\), \\(\\displaystyle\\int_E XdP \\leq \\int_F XdP.\\) \\(~\\) \\(~\\) Propriedades: se \\(X, Y: \\Omega \\longrightarrow \\mathbb{R}_+\\) são funções mensuráveis positivas, então 1. \\(\\displaystyle\\int_\\Omega cXdP =\\) \\(c\\displaystyle\\int_\\Omega XdP, c\\geq 0\\); 2. \\(\\displaystyle\\int_\\Omega (X+Y)dP =\\) \\(\\displaystyle\\int_\\Omega XdP + \\int_\\Omega YdP\\). Demo 1. Seja \\(X_n\\uparrow X,\\) \\(X_n \\geq 0\\) simples. Então \\(cX_n\\uparrow cX,\\) \\(cX_n \\geq 0,\\) simples. \\(\\displaystyle\\int_\\Omega cX dP\\) \\(=\\displaystyle\\lim_{n\\rightarrow\\infty}\\int_\\Omega cX_n dP\\) \\(=\\displaystyle\\lim_{n\\rightarrow\\infty}c\\int_\\Omega X_n dP\\) \\(=\\displaystyle c\\lim_{n\\rightarrow\\infty}\\int_\\Omega X_n dP\\) \\(=\\displaystyle c\\int_\\Omega X dP\\). Demo 2. Exercício. \\(~\\) Exemplo: Suponha que \\(X\\) assume valores em \\(\\mathbb{N}\\). Pode-se escrever \\(X =\\displaystyle\\sum_{i=1}^\\infty i ~\\mathbb{I}_{A_i}~\\), com \\(A_i = X^{-1}\\left(\\{i\\}\\right)\\). Defina \\(X_n =\\displaystyle\\sum_{i=1}^{n-1} i ~\\mathbb{I}_{A_i}+n~\\mathbb{I}_{\\underset{j=n}{\\cup} A_j}\\). Então \\(X_n\\) é simples, \\(X_n \\geq 0~\\), \\(X_n \\leq X_{n+1}\\) e \\(X_n \\uparrow X\\), de modo que \\(E(X)\\) \\(=\\displaystyle\\int_\\Omega X dP\\) \\(=\\displaystyle\\lim_{n \\rightarrow\\infty}\\int_\\Omega X_n dP\\). Além disso, \\(\\displaystyle\\int_\\Omega X_n dP\\) \\(=\\displaystyle\\sum_{i=1}^{n-1} i~P(A_i) + n~P\\left(\\bigcup_{j=n}^{\\infty} A_j\\right)\\) \\(=\\displaystyle\\sum_{i=1}^{n-1}i~P(X = i) + n~P(X \\geq n)\\) \\(=\\displaystyle\\sum_{i=1}^{n-1} \\sum_{j=1}^{i} P(X = i) + n~P(X \\geq n)\\) \\(\\displaystyle=\\sum_{j=1}^{n-1} \\sum_{i=j}^{n-1} P(X = i) + n~P(X \\geq n)\\) \\(=\\displaystyle\\sum_{j=1}^{n-1}P(j \\leq X \\leq n-1) + n~P(X \\geq n)\\) \\(=\\displaystyle\\sum_{j=1}^n P(X \\geq j)\\), então, \\(E(X)\\) \\(\\displaystyle=\\lim_{n\\rightarrow \\infty}\\sum_{j=1}^nP(X \\geq j)\\) \\(\\displaystyle=\\sum_{j=1}^{\\infty}P(X \\geq j)\\). \\(~\\) Seja \\(X: \\Omega \\longrightarrow \\mathbb{R}\\) e \\(X^-,X^+: \\Omega \\longrightarrow \\mathbb{R}\\) dados por \\(X^- = \\max\\{-X,0\\}~\\) (parte negativa de \\(X\\)) e \\(X^+ = \\max\\{X,0\\}~\\) (parte positiva de \\(X\\)) \\(~\\) \\(~\\) Note que \\(X = X^+ - X^-\\) \\(~\\) Se \\(\\displaystyle\\int_\\Omega X^+ dP &lt; \\infty\\) ou \\(\\displaystyle\\int_\\Omega X^- dP &lt; \\infty\\), definimos \\(E[X]\\) \\(=\\displaystyle\\int X dP\\) \\(=\\displaystyle\\int_\\Omega X^+dP - \\int_\\Omega X^- dP\\) \\(=E\\left[X^+\\right] - E\\left[X^-\\right]\\). \\(~\\) Além disso, seja \\(|X| = X^+ + X^-\\). Então, \\(E\\left[~|X|~\\right] &lt; \\infty\\) se \\(E(X^+) &lt; \\infty\\) e \\(E(X^-) &lt; \\infty\\), e, nesse caso, dizemos que \\(X\\) é integrável. \\(~\\) Propriedades: se \\(X, Y: \\Omega \\longrightarrow \\mathbb{R}\\) são funções mensuráveis, então 1. \\(X \\leq Y \\Rightarrow E(X) \\leq E(Y)\\); 2. \\(c \\in \\mathbb{R},\\) \\(E(cX) = cE(X)\\); 3. \\(X,Y\\) integráveis. \\(E(X+Y) = E(X) + E(Y)\\). Demo 1. \\(X \\leq Y \\Rightarrow\\) \\(\\left\\{\\begin{array}{c}X^+ \\leq Y^+\\\\ X^- \\geq Y^-\\end{array}\\right.\\) \\(E(X) =\\) \\(E(X^+) - E(X^-)\\) \\(\\leq E(Y^+) - E(Y^-)\\) \\(=E(Y).\\) Demo 2. \\((cX)^+ =\\) \\(\\left\\{\\begin{array}{rcl}cX^+ &amp;,&amp; c \\geq 0\\\\ -cX^- &amp;,&amp; c &lt; 0 \\end{array}\\right.\\) \\((cX)^- =\\) \\(\\left\\{\\begin{array}{rcl}cX^- &amp;,&amp; c \\geq 0\\\\ -cX^+ &amp;,&amp; c &lt; 0 \\end{array}\\right.\\) Para \\(c&lt;0\\), \\(E[cX]\\) \\(= E[(cX)^+] - E[(cX)^-]\\) \\(= E[-cX^-] - E[-cX^+]\\) \\(= -cE[X^-] + cE[X^+]\\) \\(= cE[X]\\). Demo 3. \\(\\displaystyle\\int_\\Omega \\left(X^+ + Y^+\\right) dP &lt; \\infty\\) ou \\(\\displaystyle\\int_\\Omega \\left(X^- + Y^-\\right) dP &lt; \\infty\\) \\(X + Y\\) \\(= (X + Y)^+ - (X+Y)^-\\) \\(= X^+ - X^- + Y^+ - Y^-\\) \\(\\Rightarrow (X+Y)^+ + X^- + Y^-\\) \\(= X^+ + Y^+ + (X+Y)^-\\) \\(\\Rightarrow \\displaystyle\\int_\\Omega (X+Y)^+dP + \\int_\\Omega X^-dP + \\int_\\Omega Y^-dP\\) \\(=\\displaystyle\\int_\\Omega X^+dP + \\int_\\Omega Y^+dP + \\int_\\Omega (X+Y)^-dP\\). \\(|X+Y|\\) \\(= |X^+-X^-+Y^+-Y^-|\\) \\(\\leq X^++X^-+Y^++Y^-\\) \\(\\Rightarrow \\displaystyle\\int_\\Omega (X+Y)^+dP - \\int_\\Omega(X+Y)^-dP\\) \\(=\\displaystyle \\int_\\Omega X^+dP -\\int_\\Omega X^-dP + \\int_\\Omega Y^+dP -\\int_\\Omega Y^-dP\\). \\(\\Rightarrow \\displaystyle\\int_\\Omega(X+Y)dP = \\int_\\Omega XdP + \\int_\\Omega YdP\\) \\(~\\) \\(~\\) A.3 Funções de Variáveis Aleatórias Considere agora uma v.a. \\(X: \\Omega \\longrightarrow \\mathbb{R}\\) e uma função real \\(g: \\mathbb{R} \\longrightarrow \\mathbb{R}\\). Defina \\(Y = g(X)\\). Então \\[(\\Omega, \\mathcal{A},P) \\overset{X}{\\longrightarrow}(\\mathbb{R},\\mathcal{B}(\\mathbb{R}),P_X)\\overset{g}{\\longrightarrow}(\\mathbb{R},\\mathcal{B}(\\mathbb{R}),P_Y)\\] \\[(\\Omega, \\mathcal{A},P)\\overset{Y = g(X)}{\\longrightarrow}(\\mathbb{R},\\mathcal{B}(\\mathbb{R}),P_Y)\\] Logo, se \\(g\\) é uma função mensurável, \\(Y=g(X)\\) também é v.a. e as medidas induzidas por X e Y são \\(P_X(A)\\) \\(= P(X^{-1}(A))\\) \\(= P\\left(\\{\\omega \\in \\Omega : X(\\omega) \\in A\\}\\right)\\); \\(P_Y(B)\\) \\(= P_X(g^{-1}(B))\\) \\(= P_X\\left(\\{x \\in \\mathbb{R} : g(x) \\in B\\}\\right)\\) \\(= P\\left(\\{\\omega \\in \\Omega : g\\left(X(\\omega)\\right) \\in B\\}\\right)\\). Assim, uma pergunta natural é como obter o valor esperado de \\(Y\\). \\(E(Y) = \\displaystyle\\int_\\Omega YdP\\) \\(=\\displaystyle\\int_\\Omega g(X)dP\\) \\(\\overset{?}{=} \\displaystyle\\int_{\\mathbb{R}}g~dP_X\\). \\(~\\) Caso 1. Seja \\(g:\\mathbb{R}\\longrightarrow\\mathbb{R}_+\\) uma função simples tal que \\(g = \\sum_{i=1}^kg_i~\\mathbb{I}_{B_i}\\), \\(g_1,\\ldots,g_k \\in \\mathbb{R}\\) e \\(B_1,\\ldots,B_k \\in \\mathcal{B}(\\mathbb{R})\\). Então, \\(\\displaystyle\\int_\\Omega Y~dP\\) \\(=\\displaystyle\\int_\\Omega g(X)~dP\\) \\(=\\displaystyle\\int_\\Omega \\left(\\sum_{i=1}^k g_i ~\\mathbb{I}_{B_i}(X)\\right)dP\\) \\(=\\displaystyle\\int_\\Omega \\left(\\sum_{i=1}^k g_i ~\\mathbb{I}_{X^{-1}(B_i)}\\right)dP\\) \\(~\\displaystyle\\overset{def}{=}~\\sum_{i=1}^k g_i~P(X^{-1}(B_i))\\) \\(=\\displaystyle\\sum_{i=1}^k g_i~P_X(B_i)\\) \\(=\\displaystyle\\int_{\\mathbb{R}}\\left(\\sum_{i=1}^kg_i~\\mathbb{I}_{B_i}\\right)dP_X\\) \\(=\\displaystyle\\int_{\\mathbb{R}} g~dP_X\\). \\(~\\) Caso 2. Seja \\(g:\\mathbb{R}\\longrightarrow\\mathbb{R}_+\\) uma função não negativa e \\((g_n)_{n\\geq1}\\), \\(g_n \\geq 0\\), uma sequência crescente de funções simples tal que \\(g_n\\uparrow g\\). Como \\(g_n\\) é simples, \\(\\displaystyle\\int_\\Omega g_n(X)dP\\) \\(=\\displaystyle\\int_{\\mathbb{R}}g_n~dP_X\\) \\(\\displaystyle~\\underset{n\\uparrow\\infty}{\\longrightarrow}~ \\int_\\Omega g(X)dP\\) \\(=\\displaystyle\\int_{\\mathbb{R}}g~dP_X\\). \\(~\\) Caso 3. Agora para \\(g: \\mathbb{R} \\longrightarrow \\mathbb{R}\\), temos \\(\\displaystyle\\int_\\Omega g^+(X)dP\\) \\(=\\displaystyle\\int_{\\mathbb{R}}g^+dP_X\\) e \\(\\displaystyle\\int_\\Omega g^-(X)dP\\) \\(=\\displaystyle\\int_{\\mathbb{R}}g^-dP_X\\). Logo, \\(\\displaystyle\\int_\\Omega g(X)dP\\) \\(=\\displaystyle\\int_{\\mathbb{R}}g~dP_X\\). \\(~\\) \\(~\\) Suponha agora \\(X\\) v.a. discreta assumindo valores em \\(\\{x_1,x_2,\\ldots\\}\\) com probabilidade \\(1\\). Nesse caso, para \\(A\\subseteq\\mathcal{B}(\\mathbb{R})\\), \\(P_X(A)\\) \\(=P_X(X \\in A)\\) \\(=P\\left(\\{\\omega\\in\\Omega: X(\\omega) \\in A\\}\\right)\\) \\(=\\displaystyle\\sum_{i:~x_i\\in A} P_X(X=x_i)\\). Vamos verificar que \\(E\\left[g(X)\\right]\\) \\(=\\displaystyle\\sum_{i=1}^\\infty g(x_i)P_X(X=x_i)\\). Caso 1. \\(g\\) simples com \\(g = \\displaystyle\\sum_{i=1}^kg_i~\\mathbb{I}_{B_i}\\), \\(g_1,\\ldots,g_k \\in \\mathbb{R}_+\\) \\(B_1,\\ldots,B_k \\in \\mathcal{B}(\\mathbb{R})\\). Então, \\(E\\left[g(X)\\right]\\) \\(=\\displaystyle\\int_\\Omega g(X)~dP\\) \\(=\\displaystyle\\sum_{i=1}^k g_i~P\\left(X^{-1}(B_i)\\right)\\) \\(=\\displaystyle\\sum_{i=1}^k g_i~P_X(B_i)\\) \\(=\\displaystyle\\sum_{i=1}^k g_i \\sum_{j:~x_j \\in B_i}^k P_X(X = x_j)\\) \\(=\\displaystyle\\sum_{i=1}^k g_i \\sum_{j=1}^\\infty \\mathbb{I}_{B_i}(x_j)P_X(X=x_j)\\) \\(=\\displaystyle\\sum_{j=1}^\\infty \\underbrace{\\left(\\sum_{i=1}^k g_i ~\\mathbb{I}_{B_i}(x_j)\\right)}_{g(x_j)}P_X(X = x_j)\\). \\(~\\) Caso 2. \\(g\\geq 0,\\) \\(g_n\\geq0,\\) \\(g_n\\) simples tal que \\(g_n \\uparrow g\\) \\(\\displaystyle\\int_\\Omega g(X)dP\\) \\(=\\displaystyle\\lim_{n\\rightarrow\\infty}\\int_\\Omega g_n(X)dP\\) \\(=\\displaystyle\\lim_{n\\rightarrow\\infty}\\left\\{\\sum_{j=1}^\\infty g_n(x_j)P_X(X=x_j)\\right\\}\\) \\(=\\displaystyle\\sum_{j=1}^\\infty g(x_j)P_X(X = x_j)\\) \\(~\\) Caso 3. Agora para \\(g: \\mathbb{R} \\longrightarrow \\mathbb{R}\\), temos \\(\\displaystyle\\int_\\Omega g^+(X)dP\\) \\(=\\displaystyle\\sum_{j=1}^\\infty g^+(x_j)P_X(X = x_j)\\) e \\(\\displaystyle\\int_\\Omega g^-(X)dP\\) \\(=\\displaystyle\\sum_{j=1}^\\infty g^-(x_j)P_X(X = x_j)\\). Logo, \\(\\displaystyle\\int_\\Omega g(X)dP\\) \\(=\\displaystyle\\sum_{j=1}^\\infty g(x_j)P_X(X = x_j)\\). \\(~\\) \\(~\\) Suponha agora \\(X\\) v.a. absolutamente contínua com função de densidade de probabilidade \\(f_X\\), ou seja, pode-se escrever \\(P_X(X\\in A)\\) \\(=\\displaystyle\\int_Af_X(t)dt\\) \\(=\\displaystyle\\int_{\\mathbb{R}}\\mathbb{I}_A(t)f_X(t)dt\\). Vamos verificar que \\(E\\left[g(X)\\right]\\) \\(=\\displaystyle\\int_{\\mathbb{R}} g(x)f_X(x)dx\\). Caso 1. \\(g\\) simples com \\(g = \\displaystyle\\sum_{i=1}^kg_i~\\mathbb{I}_{B_i}\\), \\(g_1,\\ldots,g_k \\in \\mathbb{R}_+\\) \\(B_1,\\ldots,B_k \\in \\mathcal{B}(\\mathbb{R})\\). Então, \\(E\\left[g(X)\\right]\\) \\(=\\displaystyle\\int_\\Omega g(X)~dP\\) \\(=\\displaystyle\\int_\\Omega\\left(\\sum_{i=1}^k g_i~\\mathbb{I}_{B_i}(X)\\right)dP\\) \\(=\\displaystyle\\int_\\Omega\\left(\\sum_{i=1}^k g_i~\\mathbb{I}_{X^{-1}(B_i)}\\right)dP\\) \\(=\\displaystyle\\sum_{i=1}^k g_i~P(X^{-1}(B_i))\\) \\(=\\displaystyle\\sum_{i=1}^k g_i~P_X(B_i)\\) \\(=\\displaystyle\\sum_{i=1}^k g_i~\\int_{\\mathbb{R}}\\mathbb{I}_{B_i}(x)f_X(x)dx\\) \\(=\\displaystyle\\int_{\\mathbb{R}}\\sum_{i=1}^k g_i\\mathbb{I}_{B_i}(x)f_X(x)dx\\) \\(=\\displaystyle\\int_{\\mathbb{R}}g(x)f_X(x)dx\\). A extensão para funções positivas e para funções reais é análogo ao que foi feito nos exemplos anteriores. \\(~\\) \\(~\\) Assim, em geral, vale que: \\(X\\) discreto: \\(E[g(X)]\\) \\(=\\displaystyle\\sum_{j=1}^\\infty g(x_j)P_X(X=x_j)\\); \\(X\\) (absolutamente) contínuo: \\(E[g(X)]\\) \\(=\\displaystyle\\int_{\\mathbb{R}}g(x)f_X(x)dx\\). \\(~\\) Esses resultados valem também se \\(X: \\Omega \\longrightarrow \\mathbb{R}^k\\) e \\(g: \\mathbb{R}^k\\longrightarrow \\mathbb{R}.\\) \\(~\\) \\(~\\) Exemplo 1. Seja \\(X\\) uma v.a. definida em \\(\\mathbb{N}\\) com função de probabilidade \\(P_X(X=x)=\\dfrac{e^{-\\lambda}\\lambda^x}{x!}~\\mathbb{I}_{\\mathbb{N}}(x)\\), para \\(\\lambda&gt;0\\) fixado. Dizemos nesse caso que \\(X \\sim \\text{Poisson}(\\lambda)\\). Então, o valor esperado de \\(X\\) é \\(E\\left[X\\right]\\) \\(=\\displaystyle\\sum_{x=0}^\\infty x~P_X(X=x)\\) \\(=\\displaystyle\\sum_{x=0}^\\infty x~\\dfrac{e^{-\\lambda}\\lambda^x}{x!}\\) \\(=\\displaystyle\\sum_{x=1}^\\infty \\dfrac{e^{-\\lambda}\\lambda^x}{(x-1)!}\\) \\(=\\displaystyle\\lambda~\\sum_{x=1}^\\infty \\dfrac{e^{-\\lambda}\\lambda^{x-1}}{(x-1)!}\\) \\(~\\overset{t=x-1}{=}~\\displaystyle\\lambda~\\sum_{t=0}^\\infty \\dfrac{e^{-\\lambda}\\lambda^{t}}{t!}\\) \\(\\Longrightarrow E\\left[X\\right] = \\lambda\\). \\(~\\) Ainda neste exemplo, considere \\(g:\\mathbb{R}\\rightarrow\\mathbb{R}\\) com \\(g(t) = e^t\\). Então, \\(E\\left[g(X)\\right]\\) \\(=\\displaystyle\\sum_{x=0}^\\infty g(x)P_X(X=x)\\) \\(=\\displaystyle\\sum_{x=0}^\\infty e^x~\\dfrac{e^{-\\lambda}\\lambda^x}{x!}\\) \\(=\\displaystyle e^{-\\lambda}\\sum_{x=0}^\\infty \\dfrac{(\\lambda e)^x}{x!}\\) \\(=\\displaystyle e^{-\\lambda}e^{\\lambda e}\\underbrace{\\sum_{x=0}^{\\infty} \\dfrac{e^{-\\lambda e}(\\lambda e)^x}{x!}}_{1}\\) \\(=e^{\\lambda e-\\lambda}\\) \\(=e^{\\lambda(e-1)}\\). \\(~\\) Exemplo 2. Seja \\(X\\) uma v.a. definida em \\([0,1]\\) com função densidade de probabilidade \\(f_X(x)=\\dfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}~x^{a-1}(1-x)^{b-1}~\\mathbb{I}_{[0,1]}(x)\\), para \\(a,b&gt;0\\) fixados. Dizemos nesse caso que \\(X \\sim \\text{Beta}(a,b)\\). Então, o valor esperado de \\(X\\) é \\(E[X]\\) \\(=\\displaystyle\\int_{-\\infty}^\\infty x~f_X(x)dx\\) \\(=\\displaystyle\\int_0^1 x~\\dfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}~x^{a-1}(1-x)^{b-1}dx\\) \\(=\\displaystyle\\dfrac{\\Gamma (a+1)}{\\Gamma(a+1+b)}\\dfrac{\\Gamma(a+b)}{\\Gamma(a)}\\int_0^1\\dfrac{\\Gamma(a+1+b)}{\\Gamma(a+1)\\Gamma(b)}~x^{(a+1)-1}(1-x)^{b-1}dx\\) \\(=\\dfrac{\\Gamma (a+1)}{\\Gamma(a+1+b)}\\dfrac{\\Gamma(a+b)}{\\Gamma(a)}\\). \\(~\\) Considere agora \\(g:\\mathbb{R}\\rightarrow\\mathbb{R}\\) com \\(g(t) = t^c(1-t)^d\\), com \\(c,d&gt;0\\) fixados. Então, \\(E[g(X)]\\) \\(=\\displaystyle\\int_{-\\infty}^\\infty g(x)~f_X(x)dx\\) \\(=\\displaystyle\\int_0^1 \\dfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}~x^{a+c-1}(1-x)^{b+d-1}dx\\) \\(=\\displaystyle\\dfrac{\\Gamma (a+c)\\Gamma(b+d)}{\\Gamma(a+c+b+d)}\\dfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\int_0^1\\dfrac{\\Gamma(a+c+b+d)}{\\Gamma(a+b)\\Gamma(b+d)}~x^{(a+c)-1}(1-x)^{(b+d)-1}dx\\) \\(=\\dfrac{\\Gamma(a+c)\\Gamma(b+d)}{\\Gamma(a+c+b+d)}\\dfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\) \\(=\\dfrac{\\beta(a+c,b+d)}{\\beta(a,b)}\\). \\(~\\) \\(~\\) A.4 Função de Distribuição Definição: Uma função \\(F: \\mathbb{R} \\longrightarrow [0,1]\\) é uma função de distribuição (f.d.) se \\(F\\) é não-decrescente e contínua à direita; \\(\\displaystyle\\lim_{x\\downarrow-\\infty}F(x)=0\\) e \\(\\displaystyle\\lim_{x\\uparrow+\\infty}F(x)=1\\). \\(~\\) Proposição: Se \\(X\\) é uma v.a., então \\(F_X(x)=P_X(X\\leq x)\\) é uma f.d. Recíprocamente, se \\(F_X\\) é uma f.d, então existe uma v.a. \\(X\\) com f.d. \\(F_X\\). \\(~\\) É possível usar uma f.d. \\(F\\) para criar uma medida em \\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\). Para tal, defina \\(P\\left((a,b]\\right)=F(b)-F(a)\\) e essa medida pode ser estendida para a \\(\\sigma\\)-álgebra usando o Teorema de Extensão de Caratheodory (veja, por exemplo, Schervish (2012), pág. 578). Reciprocamente, se \\(P\\) é uma medida de probabilidade em \\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) então \\(F(x)=P\\left((-\\infty,x]\\right)\\) é uma f.d. Neste caso, se \\(g: \\mathbb{R}\\longrightarrow \\mathbb{R}\\) é uma função mensurável, não será feita distinção entre \\(\\displaystyle\\int g(x)dF(x)=\\) \\(\\displaystyle\\int g(x)~dP_X(x)\\). Se \\(P\\) é uma medida de probabilidade em \\((\\mathbb{R}^k,\\mathcal{B}(\\mathbb{R}^k))\\) então \\(F(x_1,\\ldots,x_k)=\\) \\(P((-\\infty,x_1]\\times \\ldots\\times (-\\infty,x_k])\\) é a função de distribuição conjunta do vector aleatório \\(\\boldsymbol{X} = (X_1,\\ldots,X_K)\\). \\(~\\) Definição: Uma função de distribuição é dita Discreta se existe um conjunto enumerável \\(B=\\{x_1,x_2,\\ldots\\}\\subset \\mathbb{R}\\) tal que \\(P_X(B)=1\\) e \\(F_d(x)=\\displaystyle\\sum_{x_i\\leq x} P_X(X=x_i)\\). Nesse caso, \\(f(x_i)=P_X(X=x_i)\\) é chamada função de probabilidade de \\(X\\); Absolutamente Contínua é contínua se existe \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) tal que \\(P_X\\left((a,b]\\right)=F_c(b)-F_c(a) = \\displaystyle\\int_{a}^{b} f(t)~dt\\). A função \\(f\\) é a função de densidade de probabilidade de \\(X\\); Singular se \\(F_s\\) é contínua com \\(F_s&#39;=0~\\) \\([\\lambda]\\) q.c. (\\(F_s\\) é singular com relação à medida de Lebesgue \\(\\lambda\\)). Resultado: Toda f.d. \\(F\\) pode ser escrita como \\(F=\\alpha_1F_d+\\alpha_2F_c+(1-\\alpha_1+\\alpha_2)F_s\\), com \\(\\alpha_1,\\alpha_2\\geq 0\\) tal que \\(\\alpha_1+\\alpha_2\\leq 1\\). \\(~\\) \\(~\\) Definição: Seja \\((\\Omega,\\mathcal{A})\\) um espaço mensurável e \\(\\mu_1\\) e \\(\\mu_2\\) medidas nesse espaço. Dizemos que \\(\\mu_2\\) é absolutamente contínua com relação à \\(\\mu_1\\) se, \\(\\forall A \\in \\mathcal{A}\\), \\(\\mu_1(A)=0\\) \\(~\\Rightarrow~ \\mu_2(A)=0\\). Nesse caso, dizemos que \\(\\mu_2\\) é dominada por \\(\\mu_1\\) ou que \\(\\mu_1\\) é uma medida dominante para \\(\\mu_2\\) e denotamos \\(\\mu_2 \\ll \\mu_1\\). \\(~\\) Teorema (de Radon-Nikodin): Seja \\(\\mu_2 \\ll \\mu_1\\) com \\(\\mu_1\\) \\(\\sigma\\)-finita. Então, \\(\\exists f: \\Omega \\longrightarrow [0,+\\infty]\\) tal que, \\(\\forall A \\in \\mathcal{A}\\), \\[\\mu_2(A) = \\int_A f(x) d\\mu_1(x).\\] Além disso, se \\(g:\\Omega \\longrightarrow \\mathbb{R}\\) é \\(\\mu_2\\)-integrável, então \\[\\int g(x) d\\mu_2(x) = \\int g(x) f(x) d\\mu_1(x).\\] A função \\(f=\\frac{d\\mu_2}{d\\mu_1}\\) é chamada de derivada de Radon-Nikodin da medida \\(\\mu_2\\) com relação à medida \\(\\mu_1\\) e é única \\([\\mu_1]\\) q.c. (ou seja, é única em todo conjunto \\(\\Omega\\) com eventual excessão de um conjunto \\(C\\) tal que \\(\\mu_1(C)=0\\)). \\(~\\) \\(~\\) Definição: \\((\\Omega, \\mathcal{A}, P)\\) espaço de probabilidade e \\((\\mathfrak{X},\\mathcal{F},\\mu)\\) espaço mensurável. Considere \\(X: \\Omega \\longrightarrow \\mathfrak{X}\\) uma v.a. e \\(P_X\\) a medida induzida por \\(X\\) de \\(P\\), i.e. \\(P_X(B) = P(X^{-1}(B))\\). Suponha que \\(P_X \\ll \\mu\\). Então, a derivada de Radon-Nikodin \\(f_X = \\dfrac{dP_X}{d\\mu}\\) é chamada densidade de \\(X\\) com respeito à \\(\\mu\\). Proposição: Se \\(h: \\mathfrak{X}\\longrightarrow\\mathbb{R}\\) é mensurável e \\(f_X = \\dfrac{dP_X}{d\\mu}\\) é a densidade de \\(X\\) com respeito à \\(\\mu\\), então \\(\\displaystyle\\int h(x)dF_X(x)\\) \\(=\\displaystyle\\int h(x)f_X(x)d\\mu\\). \\(~\\) Exemplo 1: Seja \\(\\Omega=\\mathfrak{X}=\\mathbb{R}\\) com a \\(\\sigma\\)-álgebra de Borel e \\(f\\) uma função não negativa tal que \\(\\displaystyle\\int f(x) dx = 1\\). Defina \\(\\displaystyle P(A)= \\int_A f(x) dx\\) e \\(X(\\omega)=\\omega\\). Então, \\(X\\) é uma variável aleatória absolutamente contínua com função de densidade de probabilidade (f.d.p.) \\(f\\) e \\(P_X = P\\). Além disso, \\(P_X\\) é absolutamente contínua com relação à medida de Lebesgue \\((P_X \\ll \\lambda)\\) e \\(\\frac{dP_X}{d\\lambda}=f\\). \\(~\\) Exemplo 2: Seja \\(\\Omega=\\mathbb{R}\\) com a \\(\\sigma\\)-álgebra de Borel, \\(\\mathfrak{X} = \\{x_1,x_2,\\ldots\\}\\) um conjunto enumerável. Seja \\(f\\) uma função não negativa definida em \\(\\mathfrak{X}\\) tal que \\(\\displaystyle \\sum_{i=1}^{\\infty} f(x_i) = 1\\). Defina \\(\\displaystyle P_X(A) = \\sum_{\\{i:~x_i \\in A\\}} f(x_i)\\). Então \\(X\\) é uma variável aleatória discreta com função de probabilidade (f.d.p.) \\(f\\). Além disso, \\(P_X\\) é absolutamente contínua com relação à medida de contagem \\((P_X \\ll \\nu)\\) e \\(\\frac{dP_X}{d\\nu}=f\\). \\(~\\) \\(~\\) Resultado Sejam \\((\\Omega,\\mathcal{A})\\) espaço mensurável, \\(P_1,P_2: \\mathcal{A}\\longrightarrow [0,1]\\) medidas de probabilidade, \\(X: \\Omega \\longrightarrow \\mathbb{R}\\) v.a. e \\(P=\\alpha P_1+(1-\\alpha)P_2\\) com \\(0\\leq\\alpha\\leq1\\). Então, \\(\\displaystyle\\int_\\Omega XdP\\) \\(=\\displaystyle\\alpha \\int_\\Omega XdP_1 + (1-\\alpha)\\int_\\Omega XdP_2\\). Caso 1. \\(X\\) simples, \\(X=\\displaystyle\\sum_{i=1}^kX_i~\\mathbb{I}_{A_i}\\). \\(\\displaystyle\\int_\\Omega XdP\\) \\(=\\displaystyle\\sum_{i=1}^kx_i~P(A_i)\\) \\(=\\displaystyle\\sum_{i=1}^k x_i[\\alpha P_1(A_i)+(1-\\alpha)P_2(A_i)]\\) \\(=\\displaystyle\\alpha \\sum_{i=1}^k x_iP_1(A_i)+(1-\\alpha)\\sum_{i=1}^k x_iP_2(A_i)\\) \\(=\\displaystyle\\alpha \\int_\\Omega XdP_1+(1-\\alpha)\\int_\\Omega XdP_2\\). Caso 2. \\(X \\geq 0\\). Considere a sequência \\(\\left(X_n\\right)_{n\\geq 1}\\) tal que \\(X_n \\uparrow X\\), \\(X_n \\geq 0\\) simples. Então, \\(=\\displaystyle\\int_\\Omega XdP\\) \\(=\\displaystyle\\lim_{n\\rightarrow\\infty}\\int_\\Omega X_n dP\\) \\(=\\displaystyle\\lim_{n\\rightarrow\\infty}\\left\\{\\alpha\\int_\\Omega X_ndP_1+(1-\\alpha)\\int_\\Omega X_ndP_2\\right\\}\\) \\(=\\displaystyle\\alpha\\lim_{n\\rightarrow\\infty}\\int_\\Omega X_ndP_1 + (1-\\alpha)\\lim_{n\\rightarrow\\infty}\\int_\\Omega X_n dP_2\\) \\(=\\displaystyle\\alpha\\int XdP_1 + (1-\\alpha)\\int_\\Omega XdP_2\\). Caso 3. \\(X\\) qualquer. Basta escrever \\(X=X^+-X^-\\) e repetir o procedimento anterior. \\(~\\) Seja \\(P_1\\) uma distribuição discreta com \\(P_1\\left(\\left\\{x_1,x_2,\\ldots\\right\\}\\right)=1\\), \\(P_2\\) uma distribuição absolutamente contínua com função densidade de probabilidade \\(f_x\\) e \\(X:\\Omega \\longrightarrow \\mathbb{R}\\) tal que \\(P_X(X \\in A)=\\) \\(\\alpha P_1\\left(X^{-1}(A)\\right)+(1-\\alpha)P_2\\left(X^{-1}(A)\\right)\\). Então, \\(E(X)\\) \\(=\\displaystyle\\int_\\Omega XdP\\) \\(=\\displaystyle\\alpha \\int_\\Omega XdP_1 + (1-\\alpha)\\int_\\Omega XdP_2\\) \\(=\\displaystyle\\alpha \\sum_{i=1}^\\infty x_iP_1(X=x_i)+(1-\\alpha)\\int_{-\\infty}^\\infty x f_X(x)dx~.\\) Exemplo. Considere uma v.a. \\(X\\) com f.d. dada por \\(F_X(t)=\\left\\{\\begin{array}{ll} 0, &amp; t&lt;0\\\\ \\dfrac{1}{15}+\\dfrac{2}{3}t, &amp; 0\\leq t &lt; 1\\\\ 1, &amp; t \\geq 1\\end{array}\\right.\\) Temos que \\(P(X=0)=1/15\\), \\(P(X=1)=4/15\\) e, assim, \\(P(0&lt;X&lt;1)=10/15=2/3=1-\\alpha\\), de modo que \\(\\dfrac{1}{15}\\) \\(=P(X=0)\\) \\(=\\alpha P_1(X=0)\\) \\(=1/3~P_1(X=0)\\) \\(\\Rightarrow P_1(X=0)=\\dfrac{1}{5} = 1-P_1(X=1)\\). \\(E(X)\\) \\(=\\displaystyle\\alpha\\int_\\Omega XdP_1+(1-\\alpha)\\int_\\Omega X dP_2\\) \\(=\\displaystyle\\dfrac{1}{3}\\left\\{0\\cdot\\dfrac{1}{5}+1\\cdot\\dfrac{4}{5}\\right\\}+\\dfrac{2}{3}\\int_{0}^{1} x~f_X(x)dx\\) \\(=\\displaystyle\\dfrac{1}{3}\\cdot\\dfrac{4}{5}+\\dfrac{2}{3}\\int_0^1 xdx\\) \\(=\\dfrac{4}{15}+\\dfrac{1}{3}\\) \\(=\\dfrac{4}{15}+\\dfrac{5}{15}\\) \\(=\\dfrac{9}{15}\\). \\(~\\) \\(~\\) A.5 Probabilidade Condicional Motivação: \\(P(B|A)=\\) \\(\\dfrac{P(A\\cap B)}{P(A)}\\) é bem definido se \\(P(A)&gt;0.\\) Seja \\(X,Y: \\Omega \\longrightarrow \\mathbb{R}\\) v.a. tais que \\(P_X\\left([0,1]\\right)=1\\) e \\(P_Y\\left(\\{0,1\\}\\right)=1\\). Considere um experimento em dois estagios onde seleciona-se \\(X\\) segundo uma distribuição absolutamente contínua \\(F_X\\) e, dado \\(X=x\\), \\(0\\leq x\\leq 1\\), uma moeda com probabilidade \\(x\\) é lançada \\(n\\) vezes. Nesse caso, é natural definir \\(Y~\\big|~X=x\\sim \\text{Bin}(n,x)\\), mesmo que \\(P(X=x)=0\\), \\(\\forall x \\in [0,1]\\). \\(~\\) Teorema da Medida Produto (para medidas de probabilidade) Seja \\((\\Omega_1, \\mathcal{A}_1,P_1)\\) um espaço de probabilidade e \\((\\Omega_2,\\mathcal{A}_2)\\) um espaço mensurável. Para cada \\(\\omega_1 \\in \\Omega_1,\\) defina uma medida de probabilidade \\(\\mu(\\omega_1,.)\\) em \\(\\mathcal{A}_2.\\) Assuma também que, para cada \\(B \\in \\mathcal{A}_2,\\) \\(\\mu(.,B)\\) é \\(\\mathcal{A}_1\\)-mensurável. Então, existe uma única medida de probabilidade \\(P\\) em \\(\\mathcal{A}= \\mathcal{A}_1\\times\\mathcal{A}_2\\) tal que \\(P(A\\times B)\\) \\(=\\displaystyle\\int_A \\mu(\\omega_1,B)dP_1(\\omega_1)~,~\\) \\(\\forall A\\in \\mathcal{A}_1,\\) \\(\\forall B\\in \\mathcal{A}_2.\\) \\(~\\) Se \\(D(\\omega_1)\\) denota uma secção de \\(D\\) em \\(\\omega_1,\\) isto é, \\(D(\\omega_1)=\\) \\(\\{\\omega_2\\in \\Omega_2: (\\omega_1,\\omega_2)\\in D\\},\\) \\(D\\in \\mathcal{A}=\\mathcal{A}_1\\times\\mathcal{A}_2,\\) então \\(P(D)\\) \\(=\\displaystyle\\int_{\\Omega_1}\\mu\\left(\\omega_1,D(\\omega_1)\\right)dP_1(\\omega_1).\\) \\(~\\) \\(~\\) Voltando à probabilidade condicional, interprete (informalmente, por enquanto) a medida \\(\\mu(x,B)\\) do teorema anterior como \\(P(Y\\in B| X=x).\\) Ainda informalmente, considere o evento \\(\\{X=x\\}\\). Intuitivamente, a probabilidade que \\(X\\in (x,x+dx]\\) é \\(dF_X(x).\\) Então, sabendo que \\(X=x\\) ocorreu, o evento \\(\\left\\{(X,Y)\\in C\\right\\}\\) ocorre se, e somente, \\(Y \\in C(x)\\) \\(=\\{y:(x,y)\\in C\\}\\) e a probabilidade desse evento é \\(\\mu(x,C(x)).\\) Pela regra da probabilidade total, \\(P(C)\\) \\(=P\\left(\\left\\{(X,Y)\\in C\\right\\}\\right)\\) \\(=\\displaystyle\\int_{\\mathbb{R}}\\mu\\left(x,C(x)\\right)dF(x).\\) Em particular, quando \\(C=\\{(x,y):~ x\\in A, y \\in B\\}\\) \\(=A\\times B~,\\) \\(C(x)=B\\) se \\(x\\in A\\) e \\(C(x)=\\varnothing\\) se \\(x \\notin A~,\\) então \\(P(C)\\) \\(=P(A\\times B)\\) \\(=\\displaystyle\\int_A \\mu(x,B)dF(x)\\) Se \\(\\mu(x,B)\\) é mensurável em \\(x\\) para cada \\(B\\in \\mathcal{B}(\\mathbb{R}),\\) então, pelo Teorema anterior, \\(P\\) é única. \\(~\\) Exemplo 1. Seja \\(X \\sim Beta(a,b)\\) e \\(Y|X=x \\sim Bin(n,x)\\) \\(~\\) Considere \\(\\left(\\Omega_1=[0,1],\\mathcal{A}_1=\\mathcal{B}([0,1]),P_X\\right),\\) de modo que, para \\(A \\in \\mathcal{A}_1~,\\) \\(P_X(A)\\) \\(=\\displaystyle\\int_{\\mathbb{R}}\\mathbb{I}_A dF_X(x)\\) \\(=\\displaystyle\\int_A f_X(x)dx\\) \\(=\\displaystyle\\int_A \\tfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}~x^{a-1}(1-x)^{b-1}dx\\). Além disso, considere \\(\\left(\\Omega_2=\\{0,1,\\ldots,n\\}, \\mathcal{A}_2=\\mathcal{P}(\\Omega_2)\\right)\\) e, para cada \\(x \\in [0,1]~,\\) defina \\(\\mu(x,B)=P(Y \\in B~|~ X=x).\\) Então, para \\(k=0,1,\\ldots,n,\\) \\(\\mu\\left(x,\\{k\\}\\right)\\) \\(=P(Y=k~|~ X=x)\\) \\(=\\displaystyle\\binom{n}{k}x^k(1-x)^{n-k}~\\) (que é mensurável em \\(x\\)). Tomando \\(\\Omega=\\Omega_1 \\times \\Omega_2~,~\\) \\(\\mathcal{A}=\\mathcal{A}_1 \\times \\mathcal{A}_2~,~\\) \\(P\\) é a única medida de probabilidade determinada por \\(P_X\\) (ou \\(F_X\\)) e \\(\\mu(x,\\cdot)~.\\) Assim, para \\(C \\in \\mathcal{A}~,\\) \\(P(C)\\) \\(=\\displaystyle\\int_{\\Omega_1}\\mu\\left(x,C(x)\\right)dP_X\\) \\(=\\displaystyle\\int_0^1 \\mu\\left(x,C(x)\\right)dF_X(x)\\) \\(=\\displaystyle\\int_0^1 \\mu\\left(x,C(x)\\right)f_X(x)dx~.\\) \\(~\\) Por exemplo, se \\(C=\\Omega_1 \\times \\{k\\},\\) temos \\(P\\left(\\Omega_1 \\times \\{k\\}\\right)\\) \\(=P\\left(\\left\\{X\\in[0,1]~,~Y=k\\right\\}\\right)\\) \\(=P_Y\\left(Y=k\\right)\\) \\(=\\displaystyle\\int_0^1P(Y=k|X=x)dF_X(x)\\) \\(=\\displaystyle\\int_0^1P(Y=k|X=x)f_X(x)dx\\) \\(=\\displaystyle\\int_0^1 \\binom{n}{k}x^k(1-x)^{n-k} ~\\tfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}~x^{a-1}(1-x)^{b-1}dx\\) \\(=\\displaystyle\\binom{n}{k}\\tfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\tfrac{\\Gamma(a+k)\\Gamma(b+n-k)}{\\Gamma(a+b+n)}\\int_0^1\\tfrac{\\Gamma(a+b+n)}{\\Gamma(a+k)\\Gamma(b+n-k)}~x^{(a+k)-1}(1-x)^{(b+n-k)-1}~dx\\) \\(=\\displaystyle\\binom{n}{k}\\tfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\tfrac{\\Gamma(a+k)\\Gamma(b+n-k)}{\\Gamma(a+b+n)}\\) \\(=\\displaystyle\\binom{n}{k}\\dfrac{\\beta(a+k,b+n-k)}{\\beta(a,b)}~.\\) \\(~\\) Nesse caso, diz-se que \\(Y \\sim \\text{Beta-Bin}(n,a,b)\\). \\(~\\) \\(~\\) Teorema Considere \\((\\Omega,\\mathcal{A},P)\\) e \\(X:\\Omega \\longrightarrow \\mathfrak{X}\\), \\(\\mathcal{F}\\) uma \\(\\sigma\\)-álgebra de \\(\\mathfrak{X}\\) e \\(B \\in \\mathcal{A}.\\) Então existe \\(g:\\mathfrak{X} \\longrightarrow \\mathbb{R}\\) tal que, para cada \\(A \\in \\mathcal{F},\\) \\(P(\\{X\\in A\\}\\cap B)\\) \\(=\\displaystyle\\int_Ag(x)dP_X(x).\\) Além disso, \\(g\\) é única \\([P_X]\\) q.c., isto é, \\(g(x)=P(B|X=x)\\) é única \\([P_X]\\) q.c. para um dado \\(B\\in\\mathcal{A}\\). Demo: segue diretamente do Teorema de Radon-Nikodin: se \\(\\mu(A)=\\) \\(P(\\{X\\in A\\}\\cap B)\\) então \\(\\mu\\) é medida finita em \\(\\mathcal{F}\\) com \\(\\mu &lt;&lt; P_X\\). \\(~\\) Exemplo 2. Seja \\(\\mathfrak{X}=\\{x_1,x_2,\\ldots\\}\\) com \\(p_i=P(\\{X=x_i\\})&gt;0.\\) Para \\(i=1,2,\\ldots\\), considere a função \\(g\\), uma proposta para \\(P\\left(B|\\{X=x_i\\}\\right)\\), definida por \\(g(x_i)\\) \\(=P\\left(B|\\{X=x_i\\}\\right)\\) \\(=\\dfrac{P\\left(B\\cap \\{X=x_i\\}\\right)}{P\\left(\\{X=x_i\\}\\right)}~.\\) Seja \\(A \\in \\mathcal{F}=\\mathcal{P}(\\mathfrak{X}),\\) então \\(\\displaystyle\\int_A g(x)~dP_X(x)\\) \\(=\\displaystyle\\int_{\\mathfrak{X}}g(x)~\\mathbb{I}_A(x)dP_X(x)\\) \\(=\\displaystyle\\sum_{i=1}^\\infty g(x_i)~\\mathbb{I}_A(x_i)P_X(X=x_i)\\) \\(=\\displaystyle\\sum_{x_i \\in A}g(x_i)P\\left(\\{X=x_i\\}\\right)\\) \\(=\\displaystyle\\sum_{x_i \\in A}\\dfrac{P\\left(B\\cap \\{X=x_i\\}\\right)}{P\\left(\\{X=x_i\\}\\right)}P\\left(\\{X=x_i\\}\\right)\\) \\(=\\displaystyle\\sum_{x_i \\in A}P\\left(B \\cap \\{X=x_i\\}\\right)\\) \\(=P\\left(\\{X\\in A\\}\\cap B\\right).\\) \\(~\\) Exemplo 3. Considere agora \\(\\Omega=\\mathbb{R}^2,\\) \\(\\mathcal{A}= \\mathcal{B}\\left(\\mathbb{R}^2\\right),\\) \\(X(x,y)=x,\\) \\(Y(x,y)=y\\) e \\(\\left(X,Y\\right)\\) vetor aleatório (absolutamente) contínuo com densidade conjunta \\(f\\), isto é, \\(P(A)=\\displaystyle\\int \\int_A f(x,y)~dxdy~,~~\\) \\(A \\in \\mathcal{A}~.\\) Nesse caso \\(P\\left(\\{X=x\\}\\right)=0~,~~ \\forall x~.\\) \\(~\\) Seja \\(f_1(x)=\\displaystyle\\int_{-\\infty}^{\\infty} f(x,y)~dy\\) a densidade marginal de \\(X\\) e defina \\(f(y|x)=\\dfrac{f(x,y)}{f_1(x)}\\) como a densidade condicional de \\(Y\\) dado \\(X=x.\\) \\(~\\) Note que \\(f(y|x)\\) só está definido quando \\(f_1(x) \\neq 0.\\) Contudo, se \\(S=\\{(x,y): f_1(x)=0\\}\\) então \\(P\\left(\\{(X,Y)\\in S\\}\\right)\\) \\(=\\displaystyle\\int \\int_S f(x,y)dxdy\\) \\(=\\displaystyle\\int_{\\{x:f_1(x)=0\\}}\\left[\\int_{-\\infty}^\\infty f(x,y)dy\\right]dx\\) \\(=\\displaystyle\\int_{\\{x:f_1(x)=0\\}} f_1(x)dx=0~,\\) de modo que \\(P\\left(\\{(X,Y)\\in S\\}\\right)=0\\) e podemos ignorar o conjunto onde \\(f(y|x)\\) não está definida. \\(~\\) Se \\(X=x,\\) \\(\\forall~ B \\in \\mathcal{A},\\) \\(B\\) ocorre se, e somente se, \\(Y \\in B(x)=\\left\\{y:(x,y) \\in B\\right\\}.\\) Assim, considere a proposta \\(g(x)\\) \\(=P\\left(\\left\\{Y \\in B(x)|X=x\\right\\}\\right)\\) \\(=\\displaystyle\\int_{B(x)}f(y|x)dy\\) \\(=\\displaystyle\\int_{-\\infty}^\\infty \\mathbb{I}_B(x,y)f(y|x)dy~.\\) \\(~\\) Então, se \\(A \\in \\mathcal{B}(\\mathbb{R}),\\) \\(P\\left(\\{X \\in A\\}\\cap B\\right)\\) \\(=\\displaystyle\\underset{\\left\\{x\\in A~;~(x,y)\\in B\\right\\}}{\\int\\int} f(x,y)dxdy\\) \\(=\\displaystyle\\int_{-\\infty}^{\\infty}\\left[\\int_{-\\infty}^{\\infty}\\mathbb{I}_B(x,y)f(y|x)dy\\right]~\\mathbb{I}_A(x)f_1(x)dx\\) \\(=\\displaystyle\\int_Af_1(x)dx\\underbrace{\\int_{B(x)}f(y|x)dy}_{g(x)}dx\\) \\(=\\displaystyle\\int_Ag(x)f_1(x)dx\\) \\(=\\displaystyle\\int_Ag(x)dP_X(x)~.\\) Portanto, \\(g(x)=P(B|X=x)~.\\) \\(~\\) No exemplo anterior, a relações entre as densidades \\(f(x,y)=f_1(x)f(y|x)\\) ou, equivalentemente, \\(f(x,y)=f_2(y)f(x|y)\\), podem ser usadas para obter a probabilidade condicional \\(\\displaystyle P(Y\\in C|X=y)=\\int_C f(y|x)dy\\), \\(C\\in\\mathcal{B}(\\mathbb{R})\\). Além disso, para \\(A,B\\in\\mathcal{B}(\\mathbb{R})\\), existe uma única medida \\(P\\) satisfazendo \\(P(X\\in A,Y \\in B)\\) \\(=\\displaystyle\\int_A P(B|X=x) f_1(x)dxdy\\) \\(=\\displaystyle\\int_A \\int_Bf(y|x)f_1(x)dydx\\) \\(=\\displaystyle\\int_A \\int_B f(x,y)dxdy\\) \\(=\\displaystyle\\int_B \\int_Af(x|y)f_2(y)dxdy~.\\) \\(~\\) \\(~\\) Exemplo 4. Esperança Condicional Seja \\((\\Omega=[0,1]^2, \\mathcal{A}=\\mathcal{B}([0,1]^2),P=\\lambda)\\) e considere as partições apresentados na figura a seguir. Defina as v.a. \\(X\\) e \\(Y\\) como \\(X(\\boldsymbol \\omega)=\\left\\{\\begin{array}{lll} x_2, &amp; \\omega_1 \\geq 1/2 &amp; (A)\\\\ x_1, &amp; \\omega_2 &lt; 1/2 &amp; (A^c)\\end{array}\\right.\\) \\(Y(\\boldsymbol \\omega)=\\left\\{\\begin{array}{lll} y_2, &amp; \\omega_1 \\leq \\omega_2 &amp; (B)\\\\ y_1, &amp; \\omega_1 &gt; \\omega_2&amp; (B^c)\\end{array}\\right.\\) \\(~\\) \\(P_X(x_2)\\) \\(=P\\left(X^{-1}\\left(\\{x_2\\}\\right)\\right)\\) \\(=P(\\boldsymbol\\omega \\in A)\\) \\(=\\lambda(A)=1/2\\) \\(P_Y(y_2)\\) \\(=P\\left(Y^{-1}\\left(\\{y_2\\}\\right)\\right)\\) \\(=P(\\boldsymbol\\omega \\in B)=\\) \\(\\lambda(B)=1/2\\) \\(~\\) \\(\\sigma_X\\) \\(=\\left\\{\\varnothing,A,A^c,\\Omega\\right\\} \\subseteq \\mathcal{B}\\left([0,1]^2\\right)\\) (é sub-\\(\\sigma\\)-álgebra de \\(\\mathcal{A}\\)) \\(\\sigma_Y\\) \\(=\\left\\{\\varnothing,B,B^c,\\Omega\\right\\} \\subseteq \\mathcal{B}\\left([0,1]^2\\right)\\) \\(~\\) Seja \\(\\boldsymbol Z(\\boldsymbol \\omega)=\\) \\(\\left(X(\\boldsymbol \\omega), Y(\\boldsymbol \\omega)\\right)\\) \\(=(X,Y)(\\boldsymbol \\omega)~.\\) Então, \\(Z: \\Omega\\longrightarrow \\mathbb{R}^2\\), de modo que \\(Z(\\boldsymbol \\omega)\\) \\(=\\displaystyle\\sum_{i=1}^4 \\boldsymbol z_i ~\\mathbb{I}_{C_i}(\\boldsymbol \\omega)\\) é uma função simples com \\(Z(\\boldsymbol \\omega)=\\left\\{\\begin{array}{ll} \\boldsymbol z_1=(x_1,y_1), &amp; \\boldsymbol \\omega \\in A^c \\cap B^c=C_1\\\\ \\boldsymbol z_2=(x_2,y_1), &amp; \\boldsymbol \\omega \\in A \\cap B^c=C_2\\\\ \\boldsymbol z_3=(x_1,y_2), &amp; \\boldsymbol \\omega \\in A^c \\cap B=C_3\\\\ \\boldsymbol z_4=(x_2,y_2), &amp; \\boldsymbol \\omega \\in A \\cap B=C_4 \\end{array}\\right.~,\\) onde \\(C_i=\\boldsymbol Z^{-1}\\left(\\{\\boldsymbol z_i\\}\\right)\\) \\(= \\left\\{\\boldsymbol\\omega\\in\\Omega:\\big(X(\\boldsymbol\\omega),Y(\\boldsymbol\\omega)\\big)=\\boldsymbol z_i\\right\\}~.\\) Então, \\(P_Z(\\boldsymbol z_1)\\) \\(=P_Z(\\boldsymbol z_4)\\) \\(=P_Z\\big((x_1,y_1)\\big)\\) \\(=P_Z\\big((x_2,y_2)\\big)\\) \\(=\\dfrac{1}{8}\\) \\(=\\lambda(A^c\\cap B^c)\\) \\(=\\lambda(A\\cap B)~,\\) \\(P_Z(\\boldsymbol z_2)\\) \\(=P_Z(\\boldsymbol z_3)\\) \\(=P_Z\\big((x_2,y_1)\\big)\\) \\(=P_Z\\big((x_1,y_2)\\big)\\) \\(=\\dfrac{3}{8}\\) \\(=\\lambda(A\\cap B^c)\\) \\(=\\lambda(A^c\\cap B)~.\\) \\(~\\) Pela que foi visto anteriormente, podemos definir \\(P_{Y|X=x_i}\\left(Y=y_j~|~X=x_i\\right)\\) \\(=\\dfrac{P\\left(\\left\\{Y=y_j~,~X=x_i\\right\\}\\right)}{P\\left(\\left\\{X=x_i\\right\\}\\right)}\\) \\(=\\left\\{\\begin{array}{ll}\\dfrac{1/8}{1/2}=\\dfrac{1}{4}~,&amp;i=j\\\\\\dfrac{3/8}{1/2}=\\dfrac{3}{4}~,&amp;i\\neq j\\end{array}\\right.~~,\\) e, assim, \\(E\\left[Y~|~X=x_i\\right]\\) \\(=\\displaystyle\\int y~dP_{Y|x_i}(y)\\) \\(=\\displaystyle\\sum_{j=1}^{2} y_j~P\\left(Y=y_j|X=x_i\\right)~.\\) \\(~\\) Considere, por exemplo, \\(x_1=y_1=1\\) e \\(x_2=y_2=2\\). Então, \\(E[Y|X=1]\\) \\(=1\\cdot\\dfrac{1}{4}+2\\cdot\\dfrac{3}{4}\\) \\(=\\dfrac{7}{4}~,\\) \\(E[Y|X=2]\\) \\(=1\\cdot\\dfrac{3}{4}+2\\cdot\\dfrac{1}{4}\\) \\(=\\dfrac{5}{4}~.\\) \\(~\\) Deste modo, podemos definir uma nova v.a. \\(E[Y|X](\\boldsymbol\\omega)=\\left\\{\\begin{array}{ll} 5/4~, &amp; \\left\\{\\boldsymbol\\omega:X(\\boldsymbol\\omega)=x_2\\right\\}=\\{\\boldsymbol\\omega \\in A\\}\\\\ 7/4~, &amp; \\left\\{\\boldsymbol\\omega:X(\\boldsymbol\\omega)=x_1\\right\\}=\\{\\boldsymbol\\omega \\in A^c\\} \\end{array}\\right.~.\\) \\(~\\) Note que a \\(\\sigma\\)-álgebra gerada pela v.a. \\(E[Y|X]\\) coincide com a gerada por \\(X\\), \\(\\sigma_X\\). Dessa forma, podemos definir, de forma equivalente para esse caso, o valor esperado de \\(Y\\) condicional à \\(\\sigma_X\\) por \\(E[Y|X]=E[Y|\\sigma_X]~.\\) \\(~\\) Referências "],["referências.html", "Referências", " Referências "]]
