<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Introdução à Inferência Bayesiana | Fundamentos de Inferência Bayesiana</title>
  <meta name="description" content="Notas de aula de Infência Bayesiana." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Introdução à Inferência Bayesiana | Fundamentos de Inferência Bayesiana" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas de aula de Infência Bayesiana." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Introdução à Inferência Bayesiana | Fundamentos de Inferência Bayesiana" />
  
  <meta name="twitter:description" content="Notas de aula de Infência Bayesiana." />
  

<meta name="author" content="Victor Fossaluza e Luís Gustavo Esteves" />


<meta name="date" content="2021-07-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ProbSubj.html"/>
<link rel="next" href="TeoDec.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notas de Aula de Inferência Bayesiana</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prefácio</a></li>
<li class="chapter" data-level="2" data-path="ProbSubj.html"><a href="ProbSubj.html"><i class="fa fa-check"></i><b>2</b> Probabilidade Subjetiva</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ProbSubj.html"><a href="ProbSubj.html#definição-axiomática"><i class="fa fa-check"></i><b>2.1</b> Definição Axiomática</a></li>
<li class="chapter" data-level="2.2" data-path="ProbSubj.html"><a href="ProbSubj.html#interpretações-de-probabilidade"><i class="fa fa-check"></i><b>2.2</b> Interpretações de Probabilidade</a></li>
<li class="chapter" data-level="2.3" data-path="ProbSubj.html"><a href="ProbSubj.html#relação-de-crença-precsim"><i class="fa fa-check"></i><b>2.3</b> Relação de Crença <span class="math inline">\(\precsim\)</span></a></li>
<li class="chapter" data-level="2.4" data-path="ProbSubj.html"><a href="ProbSubj.html#medida-de-probabilidade-que-representa-precsim"><i class="fa fa-check"></i><b>2.4</b> Medida de Probabilidade que “representa” <span class="math inline">\(\precsim\)</span></a></li>
<li class="chapter" data-level="2.5" data-path="ProbSubj.html"><a href="ProbSubj.html#medida-de-probabilidade-condicional"><i class="fa fa-check"></i><b>2.5</b> Medida de Probabilidade Condicional</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Bayes.html"><a href="Bayes.html"><i class="fa fa-check"></i><b>3</b> Introdução à Inferência Bayesiana</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Bayes.html"><a href="Bayes.html#BasBayes"><i class="fa fa-check"></i><b>3.1</b> Conceitos Básicos</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="Bayes.html"><a href="Bayes.html#inferência-frequentista-ou-clássica"><i class="fa fa-check"></i><b>3.1.1</b> Inferência Frequentista (ou Clássica)</a></li>
<li class="chapter" data-level="3.1.2" data-path="Bayes.html"><a href="Bayes.html#inferência-bayesiana"><i class="fa fa-check"></i><b>3.1.2</b> Inferência Bayesiana</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Bayes.html"><a href="Bayes.html#teorema-de-de-finetti"><i class="fa fa-check"></i><b>3.2</b> Teorema de De Finetti</a></li>
<li class="chapter" data-level="3.3" data-path="Bayes.html"><a href="Bayes.html#suficiência"><i class="fa fa-check"></i><b>3.3</b> Suficiência</a></li>
<li class="chapter" data-level="3.4" data-path="Bayes.html"><a href="Bayes.html#distribuição-a-priori"><i class="fa fa-check"></i><b>3.4</b> Distribuição a Priori</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="Bayes.html"><a href="Bayes.html#método-do-histograma"><i class="fa fa-check"></i><b>3.4.1</b> Método do Histograma</a></li>
<li class="chapter" data-level="3.4.2" data-path="Bayes.html"><a href="Bayes.html#elicitação-de-hiperparâmetros"><i class="fa fa-check"></i><b>3.4.2</b> Elicitação de Hiperparâmetros</a></li>
<li class="chapter" data-level="3.4.3" data-path="Bayes.html"><a href="Bayes.html#prioris-conjugadas"><i class="fa fa-check"></i><b>3.4.3</b> Prioris Conjugadas</a></li>
<li class="chapter" data-level="3.4.4" data-path="Bayes.html"><a href="Bayes.html#prioris-não-informativas"><i class="fa fa-check"></i><b>3.4.4</b> Prioris “Não-Informativas”</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="Bayes.html"><a href="Bayes.html#alguns-princípios-de-inferência"><i class="fa fa-check"></i><b>3.5</b> Alguns Princípios de Inferência</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="TeoDec.html"><a href="TeoDec.html"><i class="fa fa-check"></i><b>4</b> Introdução à Teoria da Decisão</a>
<ul>
<li class="chapter" data-level="4.1" data-path="TeoDec.html"><a href="TeoDec.html#BasDec"><i class="fa fa-check"></i><b>4.1</b> Conceitos Básicos</a></li>
<li class="chapter" data-level="4.2" data-path="TeoDec.html"><a href="TeoDec.html#Aleat"><i class="fa fa-check"></i><b>4.2</b> Aleatorização e Decisões Mistas</a></li>
<li class="chapter" data-level="4.3" data-path="TeoDec.html"><a href="TeoDec.html#DecDados"><i class="fa fa-check"></i><b>4.3</b> Problemas com Dados</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Estimacao.html"><a href="Estimacao.html"><i class="fa fa-check"></i><b>5</b> Estimação</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Estimacao.html"><a href="Estimacao.html#estimação-pontual"><i class="fa fa-check"></i><b>5.1</b> Estimação Pontual</a></li>
<li class="chapter" data-level="5.2" data-path="Estimacao.html"><a href="Estimacao.html#estimação-por-regiões"><i class="fa fa-check"></i><b>5.2</b> Estimação por Regiões</a></li>
<li class="chapter" data-level="5.3" data-path="Estimacao.html"><a href="Estimacao.html#custo-das-observações"><i class="fa fa-check"></i><b>5.3</b> Custo das Observações</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Test.html"><a href="Test.html"><i class="fa fa-check"></i><b>6</b> Testes de Hipóteses</a>
<ul>
<li class="chapter" data-level="6.1" data-path="Test.html"><a href="Test.html#BasTest"><i class="fa fa-check"></i><b>6.1</b> Conceitos Básicos</a></li>
<li class="chapter" data-level="6.2" data-path="Test.html"><a href="Test.html#revisão-abordagem-frequentista"><i class="fa fa-check"></i><b>6.2</b> Revisão: Abordagem Frequentista</a></li>
<li class="chapter" data-level="6.3" data-path="Test.html"><a href="Test.html#abordagem-bayesiana-via-teoria-da-decisão"><i class="fa fa-check"></i><b>6.3</b> Abordagem Bayesiana (via Teoria da Decisão)</a></li>
<li class="chapter" data-level="6.4" data-path="Test.html"><a href="Test.html#probabilidade-posterior-de-h_0"><i class="fa fa-check"></i><b>6.4</b> Probabilidade Posterior de <span class="math inline">\(H_0\)</span></a></li>
<li class="chapter" data-level="6.5" data-path="Test.html"><a href="Test.html#fator-de-bayes"><i class="fa fa-check"></i><b>6.5</b> Fator de Bayes</a></li>
<li class="chapter" data-level="6.6" data-path="Test.html"><a href="Test.html#teste-de-jeffreys"><i class="fa fa-check"></i><b>6.6</b> Teste de Jeffreys</a></li>
<li class="chapter" data-level="6.7" data-path="Test.html"><a href="Test.html#hipóteses-precisas"><i class="fa fa-check"></i><b>6.7</b> Hipóteses Precisas</a></li>
<li class="chapter" data-level="6.8" data-path="Test.html"><a href="Test.html#fbst---full-bayesian-significance-test"><i class="fa fa-check"></i><b>6.8</b> FBST - <em>Full Bayesian Significance Test</em></a></li>
<li class="chapter" data-level="6.9" data-path="Test.html"><a href="Test.html#p-value---nível-de-significância-adaptativo"><i class="fa fa-check"></i><b>6.9</b> P-value - Nível de Significância Adaptativo</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Comp.html"><a href="Comp.html"><i class="fa fa-check"></i><b>7</b> Métodos Computacionais</a>
<ul>
<li class="chapter" data-level="7.1" data-path="Comp.html"><a href="Comp.html#método-de-monte-carlo"><i class="fa fa-check"></i><b>7.1</b> Método de Monte Carlo</a></li>
<li class="chapter" data-level="7.2" data-path="Comp.html"><a href="Comp.html#monte-carlo-com-amostragem-de-importância"><i class="fa fa-check"></i><b>7.2</b> Monte Carlo com Amostragem de Importância</a></li>
<li class="chapter" data-level="7.3" data-path="Comp.html"><a href="Comp.html#método-de-rejeição"><i class="fa fa-check"></i><b>7.3</b> Método de Rejeição</a></li>
<li class="chapter" data-level="7.4" data-path="Comp.html"><a href="Comp.html#abc-aproximated-bayesian-computation"><i class="fa fa-check"></i><b>7.4</b> ABC (Aproximated Bayesian Computation)</a></li>
<li class="chapter" data-level="7.5" data-path="Comp.html"><a href="Comp.html#mcmc---monte-carlo-via-cadeias-de-markov"><i class="fa fa-check"></i><b>7.5</b> MCMC - Monte Carlo via Cadeias de Markov</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="Comp.html"><a href="Comp.html#pequena-introdução-às-cadeias-de-markov"><i class="fa fa-check"></i><b>7.5.1</b> Pequena Introdução às Cadeias de Markov</a></li>
<li class="chapter" data-level="7.5.2" data-path="Comp.html"><a href="Comp.html#o-algoritmo-de-metrópolis-hastings"><i class="fa fa-check"></i><b>7.5.2</b> O algoritmo de <strong>Metrópolis-Hastings</strong></a></li>
<li class="chapter" data-level="7.5.3" data-path="Comp.html"><a href="Comp.html#amostrador-de-gibbs"><i class="fa fa-check"></i><b>7.5.3</b> Amostrador de Gibbs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="R.html"><a href="R.html"><i class="fa fa-check"></i><b>8</b> Bibliotecas de R para Inferência Bayesiana</a>
<ul>
<li class="chapter" data-level="8.1" data-path="R.html"><a href="R.html#o-modelo-de-regressão-linear"><i class="fa fa-check"></i><b>8.1</b> O Modelo de Regressão Linear</a></li>
<li class="chapter" data-level="8.2" data-path="R.html"><a href="R.html#laplaces-demon"><i class="fa fa-check"></i><b>8.2</b> Laplace’s Demon</a></li>
<li class="chapter" data-level="8.3" data-path="R.html"><a href="R.html#stan"><i class="fa fa-check"></i><b>8.3</b> Stan</a></li>
<li class="chapter" data-level="8.4" data-path="R.html"><a href="R.html#mlg"><i class="fa fa-check"></i><b>8.4</b> MLG</a></li>
<li class="chapter" data-level="8.5" data-path="R.html"><a href="R.html#modelos-dinâmicos"><i class="fa fa-check"></i><b>8.5</b> Modelos Dinâmicos</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="R.html"><a href="R.html#dados-de-amoxicilina-fonte-cea"><i class="fa fa-check"></i><b>8.5.1</b> Dados de Amoxicilina (fonte: CEA)</a></li>
<li class="chapter" data-level="8.5.2" data-path="R.html"><a href="R.html#dados-de-covid-19"><i class="fa fa-check"></i><b>8.5.2</b> Dados de Covid-19</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Apêndice</b></span></li>
<li class="chapter" data-level="A" data-path="medprob.html"><a href="medprob.html"><i class="fa fa-check"></i><b>A</b> Breve Resumo de Medida e Probabilidade</a>
<ul>
<li class="chapter" data-level="A.1" data-path="medprob.html"><a href="medprob.html#basprob"><i class="fa fa-check"></i><b>A.1</b> Conceitos Básicos</a></li>
<li class="chapter" data-level="A.2" data-path="medprob.html"><a href="medprob.html#lebesgue"><i class="fa fa-check"></i><b>A.2</b> Valor Esperado de <span class="math inline">\(X\)</span> (OU uma ideia da tal Integral de Lebesgue)</a></li>
<li class="chapter" data-level="A.3" data-path="medprob.html"><a href="medprob.html#funções-de-variáveis-aleatórias"><i class="fa fa-check"></i><b>A.3</b> Funções de Variáveis Aleatórias</a></li>
<li class="chapter" data-level="A.4" data-path="medprob.html"><a href="medprob.html#função-de-distribuição"><i class="fa fa-check"></i><b>A.4</b> Função de Distribuição</a></li>
<li class="chapter" data-level="A.5" data-path="medprob.html"><a href="medprob.html#probabilidade-condicional"><i class="fa fa-check"></i><b>A.5</b> Probabilidade Condicional</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referências.html"><a href="referências.html"><i class="fa fa-check"></i>Referências</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de Inferência Bayesiana</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Bayes" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Introdução à Inferência Bayesiana</h1>
<div id="BasBayes" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Conceitos Básicos</h2>
<ul>
<li><p><strong>Inferência Estatística:</strong> fazer afirmações sobre quantidades não observáveis em um determinado contexto.</p></li>
<li><p><span class="math inline">\(\theta\)</span> : <strong>parâmetro</strong> - quantidade desconhecida de interesse (não-observável em determinado contexto).</p></li>
<li><p><span class="math inline">\(\Theta\)</span> : <strong>espaço paramétrico</strong> - conjunto onde <span class="math inline">\(\theta\)</span> toma valores (supostamente conhecido).</p></li>
<li><p><span class="math inline">\(E=\left(\boldsymbol X, \theta, \left\{f(\boldsymbol x|\theta)\right\}\right)\)</span>: <strong>experimento</strong> - “<em>tornar visível algo que antes era invisível</em>” ou, mais especificamente no nosso contexto, observar uma realização <span class="math inline">\(\boldsymbol x \in \mathfrak{X}\)</span> de um vetor aleatório <span class="math inline">\(\boldsymbol X\)</span> com alguma distribuição <span class="math inline">\(f(\boldsymbol x|\theta)\)</span>. Essa distribuição pertence, na maioria dos casos, à uma família de distribuições fixada mas que depende do parâmetro desconhecido de interesse <span class="math inline">\(\theta\)</span>. Note que na grande maioria dos problemas do dia a dia de um estatístico ele se utiliza de resultados experimentais para fazer afirmações sobre <span class="math inline">\(\theta\)</span> e este, por sua vez, é não-observável em geral.</p></li>
<li><p><span class="math inline">\(\mathfrak{X}\)</span> : <strong>espaço amostral</strong> - conjunto onde <span class="math inline">\(\boldsymbol X\)</span> toma valores (supostamente conhecido).</p></li>
<li><p><span class="math inline">\(\mathcal{F}\)</span> : <span class="math inline">\(\sigma\)</span>-álgebra de (sub)conjuntos de <span class="math inline">\(\mathfrak{X}\)</span>.</p></li>
<li><p>Neste espaço amostral, defini-se uma família <span class="math inline">\(\mathcal{P}=\{P(\cdot|\theta): \theta \in \Theta\}\)</span>, isto é, um conjunto de distribuições (condicionais) para <span class="math inline">\(\boldsymbol X\)</span> indexadas por <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><span class="math inline">\((\mathfrak{X},\mathcal{F},\mathcal{P})\)</span> : modelo estatístico (clássico).</p></li>
<li><p><span class="math inline">\(V_x(\theta)=f(\boldsymbol x |\theta)\)</span> : função de verossimilhança.</p></li>
</ul>
<p><span class="math inline">\(~\)</span></p>
<div id="inferência-frequentista-ou-clássica" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Inferência Frequentista (ou Clássica)</h3>
<ul>
<li><p><span class="math inline">\(\theta\)</span> é considerado fixo (apesar de desconhecido) e, portanto, não recebe uma distribuição de probabilidade.</p></li>
<li><p>Baseia-se no " princípio" da amostragem repetida (interpretação frequentista de probabilidade), isto é, supõe que é possivel realizar infinitas vezes o experimento. Assim, o <span class="math inline">\(\boldsymbol x\)</span> é apenas um dos possiveis resultados (hipóteticos) do experimento.</p></li>
<li><p>Probabilidade somente é definida em (uma <span class="math inline">\(\sigma\)</span>-álgebra de) <span class="math inline">\(\mathfrak{X}\)</span>.</p></li>
</ul>
</div>
<div id="inferência-bayesiana" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Inferência Bayesiana</h3>
<ul>
<li><p>Baseia-se na interpretação subjetivista de probabilidade, de modo que a <em>SUA</em> incerteza sobre algo desconhecido deve ser quantificada (traduzida) em termos de probabilidade.</p></li>
<li><p>Assim, <em>SUA</em> incerteza sobre o parâmetro (desconhecido) é representada por uma distribuição de probabilidade, <span class="math inline">\(\theta\)</span> é tratado como uma variável aleatória (v.a.) e <em>SUA</em> distribuição para <span class="math inline">\(\theta\)</span> antes da realização do experimento, <span class="math inline">\(f(\theta),\)</span> é chamada de <strong>distribuição a priori</strong>. Note que a atribuição de uma distribuição a prior para <span class="math inline">\(\theta\)</span> independe da natureza do parâmetro, ele pode ser a proporção de indivíduos que avalia positivamente o governo atual (quantidade essa que muda a todo instante) ou ainda a milésima casa do <span class="math inline">\(\pi\)</span> (algum número de 0 a 9, fixo porém desconhecido no momento dessa leitura).</p></li>
<li><p>A atualização de <em>SUA</em> incerteza sobre <span class="math inline">\(\theta,\)</span> incorporando uma nova informação trazida pelos dados <span class="math inline">\(\boldsymbol x\)</span> (representada por <span class="math inline">\(f(\boldsymbol x| \theta)\)</span>) é feita pelo <em>Teorema de Bayes</em>:</p></li>
<li><p><strong>Teorema de Bayes:</strong></p></li>
</ul>
<p><span class="math display">\[\underbrace{f(\theta| \boldsymbol x)}_{dist. posteriori}=~~\dfrac{f(\theta)f(\boldsymbol x|\theta)}{\displaystyle \int_{\Theta}f(\boldsymbol x|\theta)dP_\theta} ~\propto~ \underbrace{f(\theta)}_{priori}\overbrace{f(\boldsymbol x|\theta)}^{verossimilhança}.\]</span></p>
<!-- $$\underbrace{f(\theta| \boldsymbol x)}_{dist. posteriori} = \dfrac{f(\boldsymbol{x}| \theta)}{\int_{\Theta}f(\boldsymbol x|\theta)dP_\theta} \propto f(\boldsymbol x|\theta).$$ -->
<ul>
<li>Toda a inferência sobre <span class="math inline">\(\theta\)</span> será baseada exclusivamente em <span class="math inline">\(f(\theta| \boldsymbol x)\)</span>, não sendo necessário considerar pontos amostrais que poderiam mas não foram observados (como é feito na inferência frequentista).</li>
</ul>
<p><span class="math inline">\(~\)</span></p>
<ul>
<li><strong>Observação:</strong> será utilizada a notação geral para integral (de Lebesgue):</li>
</ul>
<p><span class="math display">\[\displaystyle \int_{\Theta}f(\boldsymbol x|\theta)dP_\theta
= \left\{ \begin{array}{ll} \displaystyle \int_{\Theta}f(\boldsymbol x|\theta) f(\theta) d\theta ~&amp;~ \text{(caso abs. contínuo)}\\ \displaystyle \sum_{\Theta}f(\boldsymbol x|\theta) f(\theta) ~&amp;~ \text{(caso discreto)} \end{array}\right.\]</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Exemplo 1a.</strong> Suponha que existem duas moedas, uma delas tem <span class="math inline">\(\theta =1/2\)</span> (honesta) e a outra <span class="math inline">\(\theta=3/4\)</span> (viesada). Uma moeda é escolhida e é feito um lançamento da moeda selecionada. Nesse experimento, tem-se <span class="math inline">\(X|\theta \sim Ber(\theta)\)</span>, com <span class="math inline">\(\Theta=\{1/2,3/4\}\)</span> e <span class="math inline">\(\mathfrak{X}=\{0,1\}\)</span>. Como “chutar” o valor de <span class="math inline">\(\theta\)</span>?</p>
<p>Considere que não existe razão para você acreditar que há algum tipo de preferência na escolha de uma ou outra moeda, isto é, considere que a priori <span class="math inline">\(f(\theta=1/2)\)</span> <span class="math inline">\(=f(\theta=3/4)\)</span> <span class="math inline">\(=1/2\)</span>. Suponha que o lançamento resultou em cara (<span class="math inline">\(x=1\)</span>). Então</p>
<p><span class="math inline">\(f(\theta = 3/4|X=1)\)</span> <span class="math inline">\(=\dfrac{f(X=1|\theta=3/4)f(\theta=3/4)}{\sum_\theta f(X=1|\theta)f(\theta)}\)</span> <span class="math inline">\(=\dfrac{\dfrac{3}{4}\dfrac{1}{2}}{\dfrac{3}{4}~\dfrac{1}{2}+\dfrac{1}{2}~\dfrac{1}{2}}=\)</span> <span class="math inline">\(\dfrac{3/4}{5/4}=\dfrac{3}{5}\)</span> <span class="math inline">\(= 1-\underbrace{f(\theta=1/2|X=1)}_{2/5}\)</span>.</p>
<p>Se, no entando, o resultado do lançamento da moeda fosse coroa (<span class="math inline">\(x=0\)</span>), teríamos</p>
<p><span class="math inline">\(P(\theta=3/4|X=0)\)</span> <span class="math inline">\(=\dfrac{\dfrac{1}{4}~\dfrac{1}{2}}{\dfrac{1}{4}~\dfrac{1}{2}+\dfrac{1}{2}~\dfrac{1}{2}}\)</span> <span class="math inline">\(=\dfrac{1/2}{1/2+2/2}=\dfrac{1}{3}\)</span>.</p>
<pre><code>## NULL</code></pre>
<p>Assim, se sua decisão for escolher o valor mais provável de <span class="math inline">\(\theta\)</span> após observar <span class="math inline">\(x\)</span>, a conclusão seria que a moeda é viesada <span class="math inline">\((\theta=3/4)\)</span> se for observado cara <span class="math inline">\((x=1)\)</span> e que a moeda é honesta <span class="math inline">\((\theta=1/2)\)</span> se o resultado for coroa <span class="math inline">\((x=0)\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Exemplo 1b.</strong> Considere agora que serão realizados <span class="math inline">\(n\)</span> lançamentos da moeda, de modo que agora tem-se <span class="math inline">\(X|\theta \sim Bin(n,\theta)\)</span>, <span class="math inline">\(\theta \in \{1/2,3/4\}\)</span>, <span class="math inline">\(x \in \{0,1,\ldots,n\}\)</span>. Suponha que observa-se <span class="math inline">\(X=x\)</span>.</p>
<p><span class="math inline">\(f(\theta=3/4|X=x)\)</span> <span class="math inline">\(=\dfrac{f(x|\theta=3/4)f(\theta=3/4)}{\displaystyle \sum_{\theta\in \{1/2,3/4\}}f(x|\theta)f(\theta)}\)</span> <span class="math inline">\(=\dfrac{\displaystyle \binom{n}{x}\left(\dfrac{3}{4}\right)^x\left(\dfrac{1}{4}\right)^{n-x}\dfrac{1}{2}}{\displaystyle \binom{n}{x}\left(\dfrac{3}{4}\right)^x\left(\dfrac{1}{4}\right)^{n-x}\dfrac{1}{2}+\displaystyle\binom{n}{x}\left(\dfrac{1}{2}\right)^x\left(\dfrac{1}{2}\right)^{n-x}\dfrac{1}{2}}\)</span> <span class="math inline">\(=\dfrac{1}{1+\left(\dfrac{2^n}{3^x}\right)}\)</span> <span class="math inline">\(=\dfrac{3^x}{3^x + 2^n}\)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="Bayes.html#cb2-1" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.5</span>,<span class="fl">0.75</span>)</span>
<span id="cb2-2"><a href="Bayes.html#cb2-2" aria-hidden="true" tabindex="-1"></a>prior<span class="ot">=</span><span class="fl">0.5</span> <span class="co"># priori P(theta[1]) = 1-P(theta[2])</span></span>
<span id="cb2-3"><a href="Bayes.html#cb2-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span><span class="dv">5</span>;</span>
<span id="cb2-4"><a href="Bayes.html#cb2-4" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="cf">function</span>(x){ </span>
<span id="cb2-5"><a href="Bayes.html#cb2-5" aria-hidden="true" tabindex="-1"></a>  (prior<span class="sc">*</span><span class="fu">dbinom</span>(x,n,theta)) <span class="sc">/</span> <span class="fu">sum</span>(prior <span class="sc">*</span> <span class="fu">dbinom</span>(x,n,theta)) }</span>
<span id="cb2-6"><a href="Bayes.html#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x=</span><span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n),<span class="at">each=</span><span class="fu">length</span>(theta))),</span>
<span id="cb2-7"><a href="Bayes.html#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">x1=</span><span class="fu">rep</span>(theta,(n<span class="sc">+</span><span class="dv">1</span>)),<span class="at">x2=</span><span class="fu">rep</span>(theta,(n<span class="sc">+</span><span class="dv">1</span>)),<span class="at">y1=</span><span class="dv">0</span>,</span>
<span id="cb2-8"><a href="Bayes.html#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y2=</span><span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n)),<span class="dv">1</span>,post))) <span class="sc">%&gt;%</span> </span>
<span id="cb2-9"><a href="Bayes.html#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="fl">0.5</span>,<span class="at">col=</span><span class="st">&quot;darkgrey&quot;</span>,<span class="at">lty=</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb2-10"><a href="Bayes.html#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x=</span>x1, <span class="at">xend=</span>x2, <span class="at">y=</span>y1,<span class="at">yend=</span>y2,<span class="at">colour=</span>x),<span class="at">lwd=</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb2-11"><a href="Bayes.html#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(theta)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;P(&quot;</span>,theta,<span class="st">&quot;|x)&quot;</span>))) <span class="sc">+</span>   </span>
<span id="cb2-12"><a href="Bayes.html#cb2-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()<span class="sc">+</span></span>
<span id="cb2-13"><a href="Bayes.html#cb2-13" aria-hidden="true" tabindex="-1"></a>  gganimate<span class="sc">::</span><span class="fu">transition_states</span>(x)</span></code></pre></div>
<pre><code>## NULL</code></pre>
<p><span class="math inline">\(~\)</span></p>
<p>Note que o Exemplo 1.a é um caso particular desse exemplo com <span class="math inline">\(n=1\)</span>. Se novamente sua decisão é baseada no valor mais provável de <span class="math inline">\(\theta\)</span>, deve-se escolher <span class="math inline">\(\theta=3/4\)</span> se</p>
<p><span class="math inline">\(f(\theta=3/4|X=x) &gt; f(\theta=1/2|X=x)\)</span> <span class="math inline">\(\Longleftrightarrow f(\theta=3/4|X=x) &gt; \dfrac{1}{2}\)</span> <span class="math inline">\(\Longleftrightarrow \dfrac{3^x}{3^x + 2^n} &gt; \dfrac{1}{2}\)</span> <span class="math inline">\(\Longleftrightarrow {3^x} &gt; {2^n}\)</span> <span class="math inline">\(\Longleftrightarrow \dfrac{x}{n} = \bar{x} &gt; \log_3{2}\approx 0,63\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Exemplo 1c.</strong> Considere que uma moeda será lançada <span class="math inline">\(n\)</span> vezes mas que <span class="math inline">\(\theta\)</span> é desconhecido, de modo que <span class="math inline">\(\Theta = [0,1]\)</span>. Para simplificar, vamos assumir <span class="math inline">\(f(\theta)=\mathbb{I}_{[0,1]}(\theta)\)</span>, isto é, <span class="math inline">\(\theta \sim Unif(0,1)\sim Beta(1,1)\)</span>. Essa priori corresponde ao caso em que você acredita que todos os valores possíveis para <span class="math inline">\(\theta\)</span> são igualmente “prováveis,” assim como nos exemplos anteriores. Novamente, <span class="math inline">\(X|\theta \sim Bin(n,\theta)\)</span></p>
<p><span class="math inline">\(f(\theta|x)\)</span>
<span class="math inline">\(=\dfrac{f(x|\theta)f(\theta)}{\displaystyle\int_0^1 f(x|\theta)f(\theta)d\theta}\)</span>
<span class="math inline">\(=\dfrac{\displaystyle\binom{n}{x}~\theta^x(1-\theta)^{n-x} ~~\mathbb{I}_{[0,1]}(\theta)}{\displaystyle\int_0^1\binom{n}{x}~\theta^x(1-\theta)^{n-x}d\theta}=\)</span> <span class="math inline">\(\dfrac{\tfrac{\Gamma(1+x+1+n-x)}{\Gamma(1+x)\Gamma(1+n-x)}~~\theta^x(1-\theta)^{n-x}~~\mathbb{I}_{[0,1]}(\theta)}{\underbrace{\displaystyle \int_0^1\tfrac{\Gamma(1+x+1+n-x)}{\Gamma(1+x)\Gamma(1+n-x)}~~\theta^x(1-\theta)^{n-x}d\theta}_{1}}\)</span>
<span class="math inline">\(=\tfrac{\Gamma(1+x+1+n-x)}{\Gamma(1+x)\Gamma(1+n-x)}~~\theta^x(1-\theta)^{n-x}~~\mathbb{I}_{[0,1]}(\theta)\)</span>.</p>
<p>Logo <span class="math inline">\(\theta|x \sim Beta(1+x,1+n-x)\)</span>. Nesse exemplo, o valor “mais provável” (com maior densidade a posteriori) para <span class="math inline">\(\theta\)</span> é a moda da distribuição, <span class="math inline">\(Moda(\theta|x)\)</span> <span class="math inline">\(= \dfrac{(1+x)-1}{(1+x)+(1+n-x)-2}\)</span> <span class="math inline">\(= \dfrac{x}{n}\)</span> <span class="math inline">\(=\bar{x}\)</span>. Suponha que foi observado <span class="math inline">\(n=5\)</span> e <span class="math inline">\(x=2\)</span>, a posteriori é <span class="math inline">\(\theta|x=2 \sim Beta(3,4)\)</span> e a moda é <span class="math inline">\(Moda(\theta|x)\)</span> <span class="math inline">\(=\frac{1+x-1}{1+1+n-2}\)</span> <span class="math inline">\(=\frac{2}{5}\)</span> <span class="math inline">\(=0,4\)</span>;</p>
<pre><code>## NULL</code></pre>
<p>Algumas medidas resumo da distribuição posterior para esse exemplo são</p>
<ul>
<li><p><span class="math inline">\(Moda(\theta|x)\)</span> <span class="math inline">\(=\dfrac{1+x-1}{1+1+n-2}\)</span> <span class="math inline">\(=\dfrac{2}{5}\)</span> <span class="math inline">\(=0,4\)</span>;</p></li>
<li><p><span class="math inline">\(E[\theta|x]\)</span> <span class="math inline">\(=\dfrac{1+x}{1+1+n}\)</span> <span class="math inline">\(=\dfrac{3}{7}\)</span> <span class="math inline">\(=0,43\)</span>;</p></li>
<li><p><span class="math inline">\(Med(\theta|x)\)</span> <span class="math inline">\(\approx \dfrac{1+x-1/3}{1+1+n-2/3}\)</span> <span class="math inline">\(=\dfrac{8/3}{19/3}\)</span> <span class="math inline">\(\approx 0,42\)</span>;</p></li>
<li><p><span class="math inline">\(Var(\theta|x)\)</span> <span class="math inline">\(=\dfrac{(1+x)(1+n-x)}{(1+1+n)^2(1+1+n+1)}\)</span> <span class="math inline">\(=\dfrac{12}{392}\)</span> <span class="math inline">\(\approx 0,031\)</span>.</p></li>
</ul>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Exemplo 1d.</strong> Por fim, suponha que no exemplo anterior, sua opinião a priori é representada por uma distribuição beta qualquer com parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>, <span class="math inline">\(a,b &gt; 0\)</span>. Desta forma, <span class="math inline">\(X|\theta \sim Bin(n,\theta)\)</span> e <span class="math inline">\(\theta\sim Beta(a,b)\)</span>. Calculando a distribuição a posteriori de forma similar ao exemplo anterior, temos que <span class="math inline">\(\theta|X=x \sim Beta(a+x,b+n-x)\)</span>. Note que o exemplo anterior é o caso particular em que <span class="math inline">\(a=b=1~.\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="Bayes.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(transformr)</span>
<span id="cb5-2"><a href="Bayes.html#cb5-2" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)</span>
<span id="cb5-3"><a href="Bayes.html#cb5-3" aria-hidden="true" tabindex="-1"></a>a<span class="ot">=</span><span class="dv">2</span>; b<span class="ot">=</span><span class="dv">2</span>;</span>
<span id="cb5-4"><a href="Bayes.html#cb5-4" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span><span class="dv">5</span></span>
<span id="cb5-5"><a href="Bayes.html#cb5-5" aria-hidden="true" tabindex="-1"></a>vero1 <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n)),<span class="dv">1</span>,</span>
<span id="cb5-6"><a href="Bayes.html#cb5-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">function</span>(x){<span class="fu">dbeta</span>(theta,<span class="dv">1</span><span class="sc">+</span>x,<span class="dv">1</span><span class="sc">+</span>n<span class="sc">-</span>x)}))</span>
<span id="cb5-7"><a href="Bayes.html#cb5-7" aria-hidden="true" tabindex="-1"></a>post1 <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n)),<span class="dv">1</span>,</span>
<span id="cb5-8"><a href="Bayes.html#cb5-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">function</span>(x){<span class="fu">dbeta</span>(theta,a<span class="sc">+</span>x,b<span class="sc">+</span>n<span class="sc">-</span>x)}))</span>
<span id="cb5-9"><a href="Bayes.html#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x=</span><span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n),<span class="at">each=</span><span class="fu">length</span>(theta))),</span>
<span id="cb5-10"><a href="Bayes.html#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">theta=</span><span class="fu">rep</span>(theta,(n<span class="sc">+</span><span class="dv">1</span>)),<span class="at">post=</span>post1,<span class="at">vero=</span>vero1) <span class="sc">%&gt;%</span> </span>
<span id="cb5-11"><a href="Bayes.html#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb5-12"><a href="Bayes.html#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span><span class="fu">dbeta</span>(theta,a,b),<span class="at">linetype=</span><span class="st">&quot;Prior&quot;</span>,<span class="at">colour=</span><span class="st">&quot;Prior&quot;</span>),<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb5-13"><a href="Bayes.html#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>post,<span class="at">linetype=</span><span class="st">&quot;Posterior&quot;</span>,<span class="at">colour=</span>x),<span class="at">lwd=</span><span class="fl">1.3</span>) <span class="sc">+</span></span>
<span id="cb5-14"><a href="Bayes.html#cb5-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>vero,<span class="at">linetype=</span><span class="st">&quot;Verossimilhança&quot;</span>,<span class="at">colour=</span>x),<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb5-15"><a href="Bayes.html#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(theta)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;f(&quot;</span>,theta,<span class="st">&quot;|x)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb5-16"><a href="Bayes.html#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()<span class="sc">+</span><span class="fu">labs</span>(<span class="at">linetype=</span><span class="st">&quot;&quot;</span>)<span class="sc">+</span></span>
<span id="cb5-17"><a href="Bayes.html#cb5-17" aria-hidden="true" tabindex="-1"></a>  gganimate<span class="sc">::</span><span class="fu">transition_states</span>(x)</span></code></pre></div>
<pre><code>## NULL</code></pre>
<p><span class="math inline">\(~\)</span></p>
<p>Suponha agora que <span class="math inline">\(a=b=2\)</span>, <span class="math inline">\(n=5\)</span> e <span class="math inline">\(x=2\)</span>, de modo que <span class="math inline">\(\theta|x=2 \sim Beta(4,5)\)</span>. Algumas medidas resumo da distribuição posterior para esse exemplo são</p>
<ul>
<li><p><span class="math inline">\(Moda(\theta|x)\)</span> <span class="math inline">\(=\dfrac{a+x-1}{a+b+n-2}\)</span> <span class="math inline">\(=\dfrac{3}{7}\)</span> <span class="math inline">\(\approx 0,428\)</span>;</p></li>
<li><p><span class="math inline">\(E[\theta|x]\)</span> <span class="math inline">\(=\dfrac{a+x}{a+b+n}\)</span> <span class="math inline">\(=\dfrac{4}{9}\)</span> <span class="math inline">\(\approx 0,444\)</span>;</p></li>
<li><p><span class="math inline">\(Med(\theta|x)\)</span> <span class="math inline">\(\approx \dfrac{a+x-1/3}{a+b+n-2/3}\)</span> <span class="math inline">\(=\dfrac{11/3}{25/3}\)</span> <span class="math inline">\(\approx 0,440\)</span>;</p></li>
<li><p><span class="math inline">\(Var(\theta|x)\)</span> <span class="math inline">\(=\dfrac{(a+x)(b+n-x)}{(a+b+n)^2(a+b+n+1)}\)</span> <span class="math inline">\(=\dfrac{20}{810}\)</span> <span class="math inline">\(\approx 0,0247\)</span>.</p></li>
</ul>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
</div>
</div>
<div id="teorema-de-de-finetti" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Teorema de De Finetti</h2>
<p><strong>Definição.</strong> Uma coleção finita <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> de quantidades aleatórias é dita <em>permutável</em> se a distribuição de <span class="math inline">\(\left(X_{\pi_1},\ldots,X_{\pi_n}\right)\)</span> é a mesma para toda permutação <span class="math inline">\(\boldsymbol\pi=(\pi_1,\ldots,\pi_n)\)</span> dos índices <span class="math inline">\((1,\ldots,n)\)</span>. Uma coleção infinita de quantidades aleatórias é <em>permutável</em> se toda subcoleção é permutável.</p>
<p><span class="math inline">\(~\)</span></p>
<ul>
<li>Segue da definição que cada uma das variáveis <span class="math inline">\(X_1,\ldots,X_n\)</span> tem a mesma distribuição marginal. Além disso, <span class="math inline">\((X_i,X_j)\)</span> têm mesma distribuição que <span class="math inline">\((X_k,X_l)\)</span>, <span class="math inline">\(\forall i\neq j\)</span> e <span class="math inline">\(k\neq l\)</span>, e assim por diante.</li>
</ul>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Proposição.</strong> Uma coleção <span class="math inline">\(C\)</span> de variáveis aleatórias é permitável se, e somente se, para todo <span class="math inline">\(n\)</span> finito menor ou igual ao tamanho da coleção <span class="math inline">\(C\)</span>, toda <span class="math inline">\(n\)</span>-upla (sequência ordenada de <span class="math inline">\(n\)</span> elementos) de elementos distintos de <span class="math inline">\(C\)</span> têm a mesma distribuição que toda outra <span class="math inline">\(n\)</span>-upla.</p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 1.</strong> Considere uma coleção <span class="math inline">\(X_1,X_2,\ldots\)</span> uma sequência (finita ou infinita) de variáveis aleatórias independentes e identicamente distribuidas (v.a. i.i.d). Note que <span class="math inline">\(f(x_1,\ldots,x_n)=\displaystyle\prod_{i=1}^nf(x_i)~,\)</span> <span class="math inline">\(\forall n~,\)</span> de modo que <span class="math inline">\(\left(X_{i_1},\ldots,X_{i_n}\right)\)</span> têm a mesma distribuição de <span class="math inline">\(\left(X_{j_1},\ldots,X_{j_n}\right)\)</span>, para <span class="math inline">\(i_1\neq\ldots\neq i_n\)</span> e <span class="math inline">\(j_1\neq\ldots\neq j_n\)</span>. Então, toda coleção de v.a. i.i.d é permutável.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<strong>Exemplo 2:</strong> Foi visto no exemplo anterior que a suposição que uma sequência de v.a. é i.i.d. implica que tal sequência é também permutável. Sabe-se também que independência implica em correlação nula, <span class="math inline">\(\rho=0\)</span>. Será então que v.a. identicamente distribuídas e não correlacionadas são também permutáveis?<br />
<span class="math inline">\(~\)</span><br />

<table>
<thead>
<tr>
<th style="text-align:center;">
<span class="math inline">\(X_1~\)</span> / <span class="math inline">\(~X_2\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(-1\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(0\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(+1\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(f(x_1)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
<span class="math inline">\(-1\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.10\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.05\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.15\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.3\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(0\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.15\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.20\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.05\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.4\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(+1\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.05\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.15\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.10\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.3\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(f(x_2)\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.3\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.4\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(0.3\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(1.0\)</span>
</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(~\)</span><br />
<span class="math inline">\(cor(X_1,X_2)\)</span>
<span class="math inline">\(=\frac{\text{Cov}\left(X_1,X_2\right)}{\sqrt{\text{Var}(X_1)\text{Var}(X_2)}}\)</span>
<span class="math inline">\(=\frac{\text{E}\left[\left(X_1-\text{E}[X_1]\right)\left(X_2-\text{E}[X_2]\right)\right]}{\sqrt{\text{Var}(X_1)\text{Var}(X_2)}}\)</span>
<span class="math inline">\(=\frac{\text{E}\left[X_1X_2\right]-\text{E}[X_1]\text{E}[X_2]}{\sqrt{\text{Var}(X_1)\text{Var}(X_2)}}\)</span><br />
<span class="math inline">\(E(X_1)=E(X_2)=0\)</span><br />
<span class="math inline">\(E(X_1X_2)=-1\cdot0,2+0+1\cdot0,2=0\)</span> <span class="math inline">\(\Rightarrow cor(X_1,X_2)=0\)</span><br />
<span class="math inline">\((X_1,X_2)\)</span> são identicamente distribuídas e não correlacionadas mas não são permutáveis pois, por exemplo, <span class="math inline">\(P\big((X_1,X_2)=(1,-1)\big)~\neq~P\big((X_2,X_1)=(1,-1)\big)~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 3:</strong> Suponha que <span class="math inline">\(X_1,X_2,\ldots\)</span> são condicionalmente i.i.d. dado <span class="math inline">\(Y=y\)</span> com densidade <span class="math inline">\(f(x_i|y),\;i=1,2,\ldots\)</span> e <span class="math inline">\(Y\)</span> tem densidade <span class="math inline">\(h(y)\)</span>. Então <span class="math inline">\(X_1,X_2,\ldots\)</span> são permutaveis.<br />
<span class="math inline">\(f_{X_{i_1},\ldots,X_{i_n}}(x_1,\ldots,x_n)\)</span> <span class="math inline">\(=\displaystyle\int\prod_{j=1}^nf(x_j|y)h(y)dy,\)</span> para qualquer <span class="math inline">\(n\)</span>-upla <span class="math inline">\(X_{i_1},\ldots,X_{i_n}\)</span>. Note que o lado direito não depende dos rótulos <span class="math inline">\(i_1,\ldots,i_n\)</span>.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Teorema de Representação de De Finetti.</strong> (para v.a. Bernoulli)</p>
<p>Uma sequência infinita <span class="math inline">\(\left(X_n\right)_{n\geq 1}\)</span> de v.a. Bernoulli é permutável se, e somente se, existe uma v.a <span class="math inline">\(\theta\)</span> em <span class="math inline">\([0,1]\)</span> tal que, condicional a <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\left(X_i\right)_{n\geq 1}\)</span> são i.i.d. <span class="math inline">\(Ber(\theta)\)</span>. Além disso, se a sequência é permutável, então a distribuição de <span class="math inline">\(\theta\)</span> é única e <span class="math inline">\(\displaystyle\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^\infty X_i ~\underset{n\uparrow\infty}{\overset{q.c.}{\longrightarrow}}~\theta~.\)</span></p>
<p><span class="math inline">\(P(X_1=x_1,\ldots,X_n=x_n)\)</span> <span class="math inline">\(=\displaystyle\int_0^1\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}~dF(\theta)\)</span> <span class="math inline">\(=\displaystyle\int_0^1\prod_{i=1}^n\underbrace{\theta^{x_i}(1-\theta)^{1-x_i}}_{f(x_i|\theta)}~f(\theta)~d\theta~,\)</span></p>
<p>onde <span class="math inline">\(F(\theta)=\displaystyle\lim_{n\uparrow\infty}~\text{P}\left(\dfrac{\sum_iX_i}{n}\leq \theta\right)~.\)</span></p>
<!-- $~$ -->
<!-- $~$ -->
<!-- A suposição de permutabilidade permite -->
<!-- * Expresar crenças de simetria sobre $X_1,X_2,\ldots$ de maneira "mais fraca" que i.i.d. -->
<!-- * Não é necessário supor independência ou que o limite de frequência relativa existe. Em outras palavras, não é necessário que o parâmetro $\theta$ do modelo estatístico exista no sentido físico. -->
<!-- * "Rótulo" das variáveis aleatórias é irrelevante. -->
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 4:</strong> (1.19/1.20 - Schervish)<br />
Seja <span class="math inline">\(\left(X_n\right)_{n\geq 1}\)</span> v.a. Bernoulli.<br />
<span class="math inline">\(~\)</span><br />
Considere que o <em>Estatístico 1</em> acredita que <span class="math inline">\(P_1(X_1=x_1,\ldots,X_n=x_n)\)</span> <span class="math inline">\(=\dfrac{12}{x+2}\dfrac{1}{\binom{n+4}{x+2}}~,\)</span> de modo que <span class="math inline">\(P_1(X_1=1)\)</span> <span class="math inline">\(=\dfrac{12}{3}\dfrac{3!~2!}{5!}\)</span> <span class="math inline">\(=\dfrac{4}{10}\)</span> <span class="math inline">\(=0,4~.\)</span><br />
Por outro lado, o <em>Estatístico 2</em> acredita que <span class="math inline">\(P_2(X_1=x_1,\ldots,X_n=x_n)\)</span> <span class="math inline">\(=\dfrac{1}{(n+1)\binom{n}{x}}\)</span> e, então, <span class="math inline">\(P_2(X_1=1)=\dfrac{1}{2}=0,5~.\)</span><br />
<span class="math inline">\(~\)</span><br />
Contudo, pelo Teorema de Finetti, ambos acreditam que o limite <span class="math inline">\(\theta=\displaystyle\lim_{n\uparrow\infty}\frac{1}{n}\sum_{i=1}^nX_i\)</span> existe com probabilidade 1 e que <span class="math inline">\(P(X_1=1|\theta)=\theta\)</span>, mas não tem opiniões diferentes sobre <span class="math inline">\(\theta\)</span>.<br />
<span class="math inline">\(~\)</span><br />
Suponha agora que foi observado <span class="math inline">\(\boldsymbol{x}=(x_1,\ldots,x_{20})\)</span> com <span class="math inline">\(\displaystyle\sum_{i=1}^{20}x_i=14\)</span>. Então,<br />
<span class="math inline">\(P_i(X_{21}=1|X_1=x_1,\ldots,X_{20}=x_{20})\)</span> <span class="math inline">\(=\dfrac{P_i(X_1=x_1,\ldots,X_{20}=x_{20},X_{21}=1)}{P_i(X_1=x_1,\ldots,X_{20}=x_{20})}\)</span><br />
de modo que,<br />
<span class="math inline">\(P_1(X_{21}=1|\mathbf X=\mathbf x)=\)</span> <span class="math inline">\(\dfrac{\dfrac{12}{17}\dfrac{1}{\binom{25}{17}}}{\dfrac{12}{16}\dfrac{1}{\binom{24}{16}}}\)</span> <span class="math inline">\(=\dfrac{16}{17}\dfrac{\dfrac{24!}{16!8!}}{\dfrac{25!}{17!8!}}\)</span> <span class="math inline">\(=\dfrac{16}{17}\dfrac{17}{25}\)</span> <span class="math inline">\(=\dfrac{16}{25}=0,64\)</span><br />
<span class="math inline">\(P_2(X_{21}=1|\mathbf X=\mathbf x)\)</span> <span class="math inline">\(=\dfrac{\dfrac{1}{22\binom{21}{15}}}{\dfrac{1}{21\binom{20}{14}}}\)</span> <span class="math inline">\(=\dfrac{21}{22}\dfrac{\dfrac{20!}{14!6!}}{\dfrac{21\cdot20!}{15\cdot 14!6!}}\)</span> <span class="math inline">\(=\dfrac{21}{22}\dfrac{15}{21}\)</span> <span class="math inline">\(=\dfrac{15}{22}=0,68\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Definição.</strong> Seja <span class="math inline">\(X_1,\ldots,X_n\)</span> uma sequência de variáveis aleatórias permutáveis. A <em>função de distribuição empírica</em> é definida como</p>
<p><span class="math inline">\(F_n(x) = \displaystyle\dfrac{1}{n}\sum_{i=1}^{n} \mathbb{I}(x_i\leq x)~.\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Teorema de Representação de De Finetti</strong></p>
<p>Uma sequência de v.a.s <span class="math inline">\(\{X_n\}_{n\geq 1}\)</span> assumindo valores em (um subconjunto de) <span class="math inline">\(\mathbb R\)</span> é permutável se, e somente se, existe uma medida de probabilidade sobre (uma <span class="math inline">\(\sigma\)</span>-álgebra do) conjunto de funções de distribuições que “sorteia” uma <span class="math inline">\(F\)</span> e, dada esta <span class="math inline">\(F\)</span>, os elementos da sequência <span class="math inline">\(\{X_n\}_{n\geq 1}\)</span> são i.i.d. com distribuição <span class="math inline">\(F\)</span>. Isto é,</p>
<p><span class="math inline">\(F_{\mathbf X}(x_1,\ldots,x_n)=\displaystyle\int\prod_{i=1}^n F(x_i)d\mu(F)\)</span>, <span class="math inline">\(\forall n\)</span>.</p>
<p>Além disso, <span class="math inline">\(F_n\underset{n\uparrow\infty}{\longrightarrow}F\)</span> e a distribuição de <span class="math inline">\(F=\underset{n\uparrow\infty}{lim}F_n\)</span> é única e é <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
</div>
<div id="suficiência" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Suficiência</h2>
<p>Muitas vezes, a quantidade de dados é muito grande e desejamos “resumir” a informação trazida pelos dados. Uma forma de fazê-lo sem perder informação sobre o parâmetro de interesse é usar uma <em>estatística suficiente</em>.</p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Definição.</strong> Dizemos que uma função da amostra <span class="math inline">\(T:\mathfrak{X} \rightarrow \mathbb{R}^p\)</span> é uma <em>estatística suficiente</em> (do ponto de vista <em>frequentista</em>) se <span class="math inline">\(f\left(\boldsymbol x | T(\boldsymbol x),\theta\right) = f\left(\boldsymbol x | T(\boldsymbol x)\right)\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<p>Em palavras, conhecendo o valor da estatística suficiente, a distribuição da amostra (do v.a. <span class="math inline">\(\boldsymbol X\)</span>) não depende mais do parâmetro <span class="math inline">\(\theta\)</span>. Isso quer dizer que a informação disponível na amostra <span class="math inline">\(\boldsymbol X\)</span> sobre <span class="math inline">\(\theta\)</span> está contida em <span class="math inline">\(T(\boldsymbol X)\)</span>. Obter uma estatística suficiente nem sempre é uma tarefa fácil mas o resultado a seguir, conhecido como <em>critério da fatoração</em> permite identificar estatísticas suficientes.</p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Teorema.</strong> A estatística <span class="math inline">\(T:\mathfrak{X} \rightarrow \mathbb{R}^p\)</span> é suficiente para a família de distribuições <span class="math inline">\(\left\{f(\cdot|\theta):\theta \in \Theta\right\}\)</span> se, e somente se, para todo <span class="math inline">\(x \in \mathfrak{X}\)</span> e para todo <span class="math inline">\(\theta \in \Theta\)</span>, podemos escrever <span class="math inline">\(f\left(\boldsymbol x | \theta\right)\)</span> <span class="math inline">\(= u(\boldsymbol x) v\left(T(\boldsymbol x),\theta\right)\)</span>, onde <span class="math inline">\(u\)</span> é uma função positiva que não depende de <span class="math inline">\(\theta\)</span> e <span class="math inline">\(v\)</span> é uma função não-negativa e depende de <span class="math inline">\(\boldsymbol x\)</span> somente através de <span class="math inline">\(T(\boldsymbol x)\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo.</strong> Seja <span class="math inline">\(X_1,\ldots,X_n\)</span> v.a. tais que, condicional ao conhecimento de <span class="math inline">\(\theta\)</span>, são c.i.i.d. com <span class="math inline">\(X_1|\theta \sim Exp(\theta)\)</span>. Então,<br />
<span class="math inline">\(f(\boldsymbol x|\theta)\)</span> <span class="math inline">\(=\prod f(x_i|\theta)\)</span> <span class="math inline">\(=\prod \theta e^{-\theta x_i} ~\mathbb{I}_{\mathbb{R+}}(x_i)\)</span> <span class="math inline">\(=\theta^n e^{-\theta \sum x_i} ~\prod ~\mathbb{I}_{\mathbb{R+}}(x_i)\)</span> <span class="math inline">\(= v\left(\sum x_i, \theta\right) u(\boldsymbol x)\)</span>.<br />
Portanto, <span class="math inline">\(T(\boldsymbol x) = \sum x_i\)</span> é estatística suficiente para <span class="math inline">\(\theta\)</span>. De fato, como <span class="math inline">\(T(\boldsymbol X)\)</span> <span class="math inline">\(= \sum X_i | \theta\)</span> <span class="math inline">\(\sim Gama(n,\theta)\)</span> e <span class="math inline">\(\left\{X_1=x_1,\ldots,X_n=x_n\right\}\)</span> <span class="math inline">\(\subseteq \left\{T(\boldsymbol X) = \sum X_i = \sum x_i = t\right\}~,\)</span><br />
<span class="math inline">\(f\left(\boldsymbol x| T(\boldsymbol x),\theta\right)\)</span> <span class="math inline">\(=\dfrac{f\left(\boldsymbol{x},T(\boldsymbol{x})|\theta\right)}{f\left(T(\boldsymbol{x})|\theta\right)}\)</span> <span class="math inline">\(=\dfrac{f\left(\boldsymbol{x}|\theta\right)}{f\left(t|\theta\right)}\)</span> <span class="math inline">\(=\dfrac{\theta^n e^{\theta \sum x_i} ~\prod ~\mathbb{I}_{\mathbb{R+}}(x_i)}{\frac{\theta^n}{\Gamma(n)}t^{n-1} e^{\theta t} ~\prod ~\mathbb{I}_{\mathbb{R+}}(x_i)}\)</span> <span class="math inline">\(= \dfrac{\Gamma(n)}{t^{n-1}} ~\mathbb{I}_{\mathbb{R}_+}\left(t\right)~,\)</span><br />
que não depende de <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p>Sob o enfoque bayesiano, a definição de suficiência é um pouco mais intuitiva que a frequentista.</p>
<p><strong>Definição:</strong> Dizemos que uma função da amostra <span class="math inline">\(T:\mathfrak{X} \rightarrow \mathbb{R}^p\)</span> é uma <em>estatística suficiente</em> (no sentido <em>bayesiano</em>) se <span class="math inline">\(f\left(\theta | T(\boldsymbol x)\right) = f\left(\theta | \boldsymbol x\right)\)</span>, para todo <span class="math inline">\(x \in \mathfrak{X}\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Voltando ao exemplo</strong>, suponha agora que, a priori, <span class="math inline">\(\theta \sim Gama(a,b)\)</span>. Então,<br />
<span class="math inline">\(f(\theta| \boldsymbol x)\)</span> <span class="math inline">\(\propto f(\boldsymbol x|\theta)f(\theta)\)</span> <span class="math inline">\(\propto \theta^n e^{-\theta \sum x_i} ~~\theta^{a-1}e^{-b\theta}\)</span> <span class="math inline">\(\propto \theta^{a+n-1} e^{-(b+\sum x_i)\theta}\)</span><br />
Seja <span class="math inline">\(T = T(\boldsymbol X) = \sum X_i\)</span>, temos que <span class="math inline">\(T|\theta\sim Gamma(n,\theta)\)</span>, de modo que<br />
<span class="math inline">\(f\left(\theta| T(\boldsymbol x)=t\right)\)</span> <span class="math inline">\(\propto f(t|\theta)f(\theta)\)</span> <span class="math inline">\(\propto \theta^n t^{n-1} e^{\theta t} ~~\theta^{a-1}e^{-b\theta}\)</span> <span class="math inline">\(\propto \theta^{a+n-1} e^{-(b+t)\theta}\)</span> , com <span class="math inline">\(t=\sum x_i\)</span>.<br />
Assim, <span class="math inline">\(\theta|\boldsymbol x\)</span> <span class="math inline">\(\sim \theta|T(\boldsymbol x)\)</span> <span class="math inline">\(\sim Gamma\left(a+n,b+\sum x_i\right)\)</span> e, portanto, <span class="math inline">\(T(\boldsymbol X) = \sum X_i\)</span> é estatística suficiente para <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p>Pelo teorema da fatoração, temos que <span class="math inline">\(f\left(\boldsymbol x | \theta\right)\)</span> <span class="math inline">\(= u(\boldsymbol x) v\left(T(\boldsymbol x),\theta\right)\)</span> e, portanto <span class="math inline">\(f(\theta|\boldsymbol x)\)</span> <span class="math inline">\(\propto f(\theta) f\left(\boldsymbol x | \theta\right)\)</span> <span class="math inline">\(\propto f(\theta) v\left(T(\boldsymbol x),\theta\right)~,\)</span> que só depende de <span class="math inline">\(\boldsymbol x\)</span> por meio de <span class="math inline">\(T(\boldsymbol x)\)</span>. Para os casos mais comuns, as definições são equivalentes <span class="citation">(<a href="#ref-Schervish12" role="doc-biblioref">Schervish 2012</a>)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<p>Um dos princípios de inferência estatística é o <em>princípio da suficiência</em>. Segundo este, se <span class="math inline">\(T\)</span> é uma estatística suficiente para <span class="math inline">\(\theta\)</span> e se dois pontos amostrais <span class="math inline">\(\boldsymbol x, \boldsymbol y \in \mathfrak{X}\)</span> são tais que <span class="math inline">\(T(\boldsymbol x)=T(\boldsymbol y)\)</span> então as inferências baseadas nesses pontos devem ser as mesmas. Adiante, retomaremos esse princípio de forma mais formal.</p>
<p><span class="math inline">\(~\)</span></p>
<!-- ## Aula 09  -->
</div>
<div id="distribuição-a-priori" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Distribuição a Priori</h2>
<ul>
<li>A priori é sempre subjetiva (assim como a escolha do modelo estatístico)!
<ul>
<li>Por exemplo, dizer que os dados seguem uma distribuição normal, é uma escolha subjetiva, muitas vezes baseadas nas facilidades matemáticas que essa distribuição proporciona.<br />
</li>
<li>Do mesmo modo, suponha que dois indivíduos que consideram que a distribuição do parêmetro é simétrica, com mesmas suposições sobre média e variância. O primeiro pode optar por representar sua distribuição usando uma distribuição Normal, enquanto o segundo pode utilizar uma distribuição T ou Cauchy.<br />
</li>
</ul></li>
<li>Não existe “opinião errada,” existem opiniões diferentes, dado o nível de conhecimento e as experiências prévias do indivíduo. Contudo, algumas “boas práticas” devem ser consideradas como, por exemplo, tomar cuidado para não atribuir probabilidade nula a pontos “possíveis” do espaço paramétrico.<br />
</li>
<li>A priori deve ser sua opinião apenas sobre o parâmetro <span class="math inline">\(\theta\)</span> e não deve depender de fatores como o desenho do experimento ou o objetivo do estudo.</li>
</ul>
<!-- ### Prioris Baseada na Opinião de um Especialista -->
<div id="método-do-histograma" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Método do Histograma</h3>
<ul>
<li><p>Muitas vezes, para “extrair” o conhecimento de um especialista, podemos dividir o espaço paramétrico em regiões e pedir para o especialista “ordenar” esses conjuntos, utilizando “pesos” que refletem a crença que o parâmetro esteja em cada uma daquelas regiões.</p></li>
<li><p><strong>Exemplo 1.</strong> (<span class="citation"><a href="#ref-Albert09" role="doc-biblioref">Albert</a> (<a href="#ref-Albert09" role="doc-biblioref">2009</a>)</span>, pág 27)</p>
<ul>
<li>Seja <span class="math inline">\(\theta\)</span> uma proporção desconhecida <span class="math inline">\((\Theta=[0,1])\)</span>;<br />
</li>
<li>Considere a partição <span class="math inline">\(T = \left\{[0,0.1), [0.1,0.2), \ldots, [0.9,1] \right\}\)</span>;</li>
<li>Suponha que um especialistas atribui pesos <span class="math inline">\(p=(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0)\)</span> a esse intervalos;<br />
</li>
<li>A piori, nesse caso, é o histograma apresentado a seguir.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="Bayes.html#cb7-1" aria-hidden="true" tabindex="-1"></a>p<span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">5.2</span>, <span class="dv">8</span>, <span class="fl">7.2</span>, <span class="fl">4.6</span>, <span class="fl">2.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb7-2"><a href="Bayes.html#cb7-2" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,p<span class="sc">/</span>(<span class="fu">sum</span>(p)))</span>
<span id="cb7-3"><a href="Bayes.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">theta=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>), prior) <span class="sc">%&gt;%</span> </span>
<span id="cb7-4"><a href="Bayes.html#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="at">data=</span>.) <span class="sc">+</span></span>
<span id="cb7-5"><a href="Bayes.html#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>prior),<span class="at">direction=</span><span class="st">&quot;vh&quot;</span>,<span class="at">color=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="InfBayes_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /></p>
<ul>
<li>Voltando ao exemplo da moeda, suponha novamente que foram observados <span class="math inline">\(x=2\)</span> sucessos em <span class="math inline">\(n=5\)</span> lançamentos. A posteriori nesse caso pode ser obtida multiplicando a distribuição a priori pela verossimilhança e “padronizando” a função obtida. Assim:</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="Bayes.html#cb8-1" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span><span class="dv">5</span></span>
<span id="cb8-2"><a href="Bayes.html#cb8-2" aria-hidden="true" tabindex="-1"></a>x<span class="ot">=</span><span class="dv">2</span></span>
<span id="cb8-3"><a href="Bayes.html#cb8-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">5.2</span>, <span class="dv">8</span>, <span class="fl">7.2</span>, <span class="fl">4.6</span>, <span class="fl">2.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb8-4"><a href="Bayes.html#cb8-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> p<span class="sc">/</span>(<span class="fu">sum</span>(p))</span>
<span id="cb8-5"><a href="Bayes.html#cb8-5" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)</span>
<span id="cb8-6"><a href="Bayes.html#cb8-6" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(p,<span class="at">each=</span><span class="dv">10</span>),<span class="dv">0</span>)<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">c</span>(<span class="fu">rep</span>(p,<span class="at">each=</span><span class="dv">10</span>),<span class="dv">0</span>))</span>
<span id="cb8-7"><a href="Bayes.html#cb8-7" aria-hidden="true" tabindex="-1"></a>vero <span class="ot">=</span> <span class="fu">dbinom</span>(x,n,theta)<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">dbinom</span>(x,n,theta))</span>
<span id="cb8-8"><a href="Bayes.html#cb8-8" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> (prior <span class="sc">*</span> vero)<span class="sc">/</span><span class="fu">sum</span>(prior <span class="sc">*</span> vero)</span>
<span id="cb8-9"><a href="Bayes.html#cb8-9" aria-hidden="true" tabindex="-1"></a>pH <span class="ot">=</span> <span class="fu">tibble</span>(<span class="at">theta=</span><span class="fu">rep</span>(theta,<span class="dv">3</span>),<span class="at">dens=</span><span class="fu">c</span>(prior,vero,post),<span class="at">Dist=</span><span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&#39;1.priori&#39;</span>,<span class="st">&#39;2.verossimilhança&#39;</span>,<span class="st">&#39;3.posteriori&#39;</span>),<span class="at">each=</span><span class="dv">101</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb8-10"><a href="Bayes.html#cb8-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="at">data=</span>.) <span class="sc">+</span></span>
<span id="cb8-11"><a href="Bayes.html#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>dens,<span class="at">colour=</span>Dist),<span class="at">lwd=</span><span class="fl">1.5</span>)</span>
<span id="cb8-12"><a href="Bayes.html#cb8-12" aria-hidden="true" tabindex="-1"></a>pH</span></code></pre></div>
<p><img src="InfBayes_files/figure-html/unnamed-chunk-12-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p><span class="math inline">\(~\)</span></p>
</div>
<div id="elicitação-de-hiperparâmetros" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Elicitação de Hiperparâmetros</h3>
<ul>
<li>Nessa abordagem, a priori é obtida da seguinte maneira:
<ol style="list-style-type: decimal">
<li>Escolha uma família de distribuições conveniente. O conceito de “conveniência” aqui pode levar em conta, por exemplo, o suporte da distribuição, se é flexível o suficiente para acomodar diversos tipos de opinião, se permite a obtenção analítica da posteriori e assim por diante;<br />
</li>
<li>Obtenha um conjunto de medidas resumo (como média, variância, quantis, etc.);<br />
</li>
<li>Utilize as medidas resumo para calcular hiperparâmetros da distribuição escolhida.</li>
</ol></li>
</ul>
<p><span class="math inline">\(~\)</span></p>
<ul>
<li><p><strong>Exemplo:</strong> Na seção anterior, a priori dada pelo histograma tem média <span class="math inline">\(m=0.31\)</span> e variância aproximadamente <span class="math inline">\(v=0.02\)</span>. Podemos utilizar como priori, por exemplo, uma distribuição beta com essa média e variância, já que a beta tem um suporte conveniente e facilita as contas, como também já vimos. Assim, vamos considerar uma distribuição <span class="math inline">\(Beta(a,b)\)</span> e escolher <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> satisfazendo:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(E[\theta]\)</span> <span class="math inline">\(=\dfrac{a}{a+b}\)</span> <span class="math inline">\(=m\)</span> <span class="math inline">\(\Longleftrightarrow b=\left(\dfrac{1-m}{m}\right)a\)</span></li>
<li><span class="math inline">\(Var(\theta)\)</span> <span class="math inline">\(=\dfrac{ab}{(a+b)^2(a+b+1)}\)</span> <span class="math inline">\(=0.02\)</span> <span class="math inline">\(\Longleftrightarrow a=\dfrac{m(m-m^2-v)}{v}\)</span></li>
</ol></li>
</ul>
<p>Resolvendo o sistema temos, de forma geral, que <span class="math inline">\(a=\dfrac{m(m-m^2-v)}{v}\)</span> e <span class="math inline">\(b=\dfrac{(1-m)(m-m^2-v)}{v}\)</span>.</p>
<p>Assim, no nosso exemplo, teríamos uma <span class="math inline">\(Beta(3,6.7)\)</span>. Além disso, já vimos que, nesse caso, a distribuição a posteriori é <span class="math inline">\(Beta(3+x,6.7+n-x)\)</span>. Considerando novamente <span class="math inline">\(n=5\)</span> e <span class="math inline">\(x=2\)</span>, temos:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="Bayes.html#cb9-1" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span><span class="dv">5</span>; x<span class="ot">=</span><span class="dv">2</span></span>
<span id="cb9-2"><a href="Bayes.html#cb9-2" aria-hidden="true" tabindex="-1"></a>m<span class="ot">=</span><span class="fl">0.31</span>; v<span class="ot">=</span><span class="fl">0.02</span></span>
<span id="cb9-3"><a href="Bayes.html#cb9-3" aria-hidden="true" tabindex="-1"></a>a<span class="ot">=</span>m<span class="sc">*</span>(m<span class="sc">-</span>m<span class="sc">^</span><span class="dv">2</span><span class="sc">-</span>v)<span class="sc">/</span>v; b<span class="ot">=</span>(<span class="dv">1</span><span class="sc">-</span>m)<span class="sc">*</span>(m<span class="sc">-</span>m<span class="sc">^</span><span class="dv">2</span><span class="sc">-</span>v)<span class="sc">/</span>v</span>
<span id="cb9-4"><a href="Bayes.html#cb9-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">5.2</span>, <span class="dv">8</span>, <span class="fl">7.2</span>, <span class="fl">4.6</span>, <span class="fl">2.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb9-5"><a href="Bayes.html#cb9-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> p<span class="sc">/</span>(<span class="fu">sum</span>(p))</span>
<span id="cb9-6"><a href="Bayes.html#cb9-6" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)</span>
<span id="cb9-7"><a href="Bayes.html#cb9-7" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">dbeta</span>(theta,a,b)<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">dbeta</span>(theta,a,b))</span>
<span id="cb9-8"><a href="Bayes.html#cb9-8" aria-hidden="true" tabindex="-1"></a>vero <span class="ot">=</span> <span class="fu">dbinom</span>(x,n,theta)<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">dbinom</span>(x,n,theta))</span>
<span id="cb9-9"><a href="Bayes.html#cb9-9" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">dbeta</span>(theta,a<span class="sc">+</span>x,b<span class="sc">+</span>n<span class="sc">-</span>x)<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">dbeta</span>(theta,a<span class="sc">+</span>x,b<span class="sc">+</span>n<span class="sc">-</span>x))</span>
<span id="cb9-10"><a href="Bayes.html#cb9-10" aria-hidden="true" tabindex="-1"></a>priorH <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(p,<span class="at">each=</span><span class="dv">10</span>),<span class="dv">0</span>)<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">c</span>(<span class="fu">rep</span>(p,<span class="at">each=</span><span class="dv">10</span>),<span class="dv">0</span>))</span>
<span id="cb9-11"><a href="Bayes.html#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">theta=</span><span class="fu">rep</span>(theta,<span class="dv">4</span>),<span class="at">dens=</span><span class="fu">c</span>(prior,vero,post,priorH),</span>
<span id="cb9-12"><a href="Bayes.html#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">Dist=</span><span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&#39;1.Priori Beta&#39;</span>,<span class="st">&#39;2.Verossimilhança&#39;</span>,<span class="st">&#39;3.Posteriori&#39;</span>,<span class="st">&#39;0.Priori Histograma&#39;</span>),<span class="at">each=</span><span class="dv">101</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb9-13"><a href="Bayes.html#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="at">data=</span>.) <span class="sc">+</span></span>
<span id="cb9-14"><a href="Bayes.html#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>dens,<span class="at">colour=</span>Dist),<span class="at">lwd=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="InfBayes_files/figure-html/unnamed-chunk-13-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p><span class="math inline">\(~\)</span></p>
</div>
<div id="prioris-conjugadas" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Prioris Conjugadas</h3>
<p>Como visto no exemplo da moeda, em que a distribuição a priori era <span class="math inline">\(Beta(a,b)\)</span>, a posteriori era facilmente obtida e também estava na classe das distribuições <span class="math inline">\(Beta\)</span>. Em particular, quando observa-se <span class="math inline">\(x\)</span> sucessos em <span class="math inline">\(n\)</span> realizações de ensaios de Bernoulli, a distribuição a posteriori é <span class="math inline">\(Beta(a+x,b+n-x)\)</span>. Isso ocorre pois essa distribuição pertence à uma classe bastante espefícica de distribuições a priori, chamadas distribuições conjugadas.</p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Definição</strong> Seja <span class="math inline">\(\mathcal{P}=\{f(x|\theta):\;\theta \in \Theta\}\)</span> uma família de distribuições (condicionais) para <span class="math inline">\(\boldsymbol{X}\)</span> e considere <span class="math inline">\(\mathcal{C}=\{h(\theta|a):\;a\in A\}\)</span> uma família de distribuições para <span class="math inline">\(\theta\)</span>. Dizemos que (a família) <span class="math inline">\(\mathcal{C}\)</span> é <strong>conjugada</strong> para <span class="math inline">\(\mathcal{P}\)</span> se, <span class="math inline">\(\forall \;h(\theta)\in \mathcal{C},\)</span> <span class="math inline">\(h(\theta|\boldsymbol{x})\propto f(\boldsymbol x|\theta)h(\theta) \in \mathcal{C},\forall \boldsymbol x \in \mathfrak{X}.\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Resultado 1.</strong> Seja <span class="math inline">\(X\)</span> v.a. tal que, condicional ao conhecimento de <span class="math inline">\(\theta,\)</span> <span class="math inline">\(X|\theta \sim Bin(n,\theta).\)</span> Considere que, a priori, <span class="math inline">\(\theta \sim Beta(a,b).\)</span> Então, <span class="math inline">\(\theta|X=x \sim Beta(a+x,b+n-x).\)</span> Portanto, a família <span class="math inline">\(\mathcal{C}=\{Beta(a_1,a_2):\;(a_1,a_2)\in \mathbb{R}^2_+\}\)</span> é conjugada para <span class="math inline">\(\mathcal{P}=\{Bin(n,\theta):\;\theta \in [0,1]\}.\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<ul>
<li>Esse resultado também vale se
<ol style="list-style-type: decimal">
<li><span class="math inline">\(X_1,\ldots,X_n\)</span> são v.a.s <em>condicionalmente independentes e identicamente distribuidas</em> (c.i.i.d.) com <span class="math inline">\(X_i|\theta \sim Ber(\theta)\)</span><br />
</li>
<li><span class="math inline">\(X_i|\theta\sim Geo(\theta),\)</span> <span class="math inline">\(i=1,\ldots,n \; c.i.i.d.\)</span><br />
</li>
<li><span class="math inline">\(X_i|\theta \sim BinNeg(k,\theta)\)</span><br />
<span class="math inline">\(\theta\sim Beta(a,b)\Rightarrow\)</span> <span class="math inline">\(\theta|\boldsymbol X=\boldsymbol x \sim Beta(a+s,b+f)\)</span> em que <span class="math inline">\(s\)</span> é o número de sucessos e <span class="math inline">\(f\)</span> é o número de fracassos.</li>
</ol></li>
</ul>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Resultado 2.</strong> (<em>generalização do resultado anterior para o caso em que o número de categorias é maior que 2</em>)</p>
<p>Seja <span class="math inline">\(\boldsymbol X | \boldsymbol \theta \sim Multinomial(n,\boldsymbol \theta)\)</span>, isto é, sua função de probabilidade é dada por</p>
<p><span class="math display">\[f(\boldsymbol x| \boldsymbol \theta)= \binom{n}{x_1,x_2,\ldots,x_k}~\prod_{i=1}^{k-1}\theta^i~\underbrace{\left(1-\sum_{i=1}^{k-1}\theta_i\right)^{\displaystyle n-\sum_{i=1}^{k-1}x_i}}_{\displaystyle \theta_k^{~~x_k}}\]</span></p>
<p>em que <span class="math inline">\(\theta_i\in [0,1]\)</span> com <span class="math inline">\(\sum_{i=1}^K\theta_i=1\)</span>, <span class="math inline">\(x_i \in \{0,1,\ldots,n\}\)</span> com <span class="math inline">\(\sum_{i=1}^nx_i=n\)</span> e <span class="math inline">\(\displaystyle \binom{n}{x_1,x_2,\ldots,x_k}=\dfrac{n!}{x_1!x_2!\ldots x_k!}\)</span>.</p>
<p>Considere que, a priori, <span class="math inline">\(\boldsymbol \theta \sim Dirichlet(a_1,\ldots,a_k),\)</span> <span class="math inline">\(a_i &gt; 0, i=1,\ldots,k\)</span>, isto é, a f.d.p. a priori para <span class="math inline">\(\boldsymbol \theta\)</span> é dada por</p>
<p><span class="math display">\[f(\boldsymbol \theta) = \dfrac{\Gamma(\sum_{i=1}^K a_i)}{\Gamma(a_1)\Gamma(a_2)\ldots\Gamma(a_k)}\prod_{i=1}^{k-1}\theta_i^{a_i-1}\bigg(\underbrace{1-\sum_{i=1}^{k-1}\theta_i}_{\theta_k}\bigg)^{a_k-1}.\]</span></p>
<p>Então, a distribuição a posteriori para <span class="math inline">\(\boldsymbol \theta\)</span> é
<span class="math inline">\(\boldsymbol \theta|\boldsymbol X = \boldsymbol x \sim Dirichlet (a_1+x_1,\ldots,a_k+x_k)\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Demo:</strong> Para verificar o resultado, basta ver que<br />
<span class="math inline">\(f(\boldsymbol\theta|\boldsymbol x)\)</span> <span class="math inline">\(=\dfrac{f(\boldsymbol x| \boldsymbol \theta)f(\boldsymbol \theta)}{\int_\Theta f(\boldsymbol x| \boldsymbol \theta)f(\boldsymbol \theta)d\boldsymbol \theta}\)</span> <span class="math inline">\(\propto f(\boldsymbol x| \boldsymbol \theta)f(\boldsymbol \theta)\)</span> <span class="math inline">\(\propto \prod_{i=1}^{k-1}\theta_i^{(a_i+x_i-1)}\left(1-\sum_{i=1}^{k-1}\theta_i\right)^{(a_k+x_k)-1}\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Resultado 3.</strong> Seja <span class="math inline">\(X_1,\ldots,X_n\)</span> v.a. c.i.i.d tais que <span class="math inline">\(X_i|\theta \sim Unif(0,\theta)\)</span> e considere que, a priori,<span class="math inline">\(\theta \sim Pareto(a,b)\)</span>. Então <span class="math inline">\(\theta|\boldsymbol X = \boldsymbol x \sim Pareto\left(a+n,max\{b,x_{(n)}\}\right)\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Demo:</strong><br />
<span class="math inline">\(f(\boldsymbol x|\theta)\)</span> <span class="math inline">\(\overset{ci}{=}\prod_{i=1}^nf(x_i|\theta)\)</span> <span class="math inline">\(\overset{id}{=}\prod_{i=1}^n\dfrac{1}{\theta}\mathbb{I}_{[0,\theta]}(x_i)\)</span> <span class="math inline">\(=\dfrac{1}{\theta^n}\mathbb{I}_{[0,\theta]}(x_{(n)})\)</span> <span class="math inline">\(=\dfrac{1}{\theta^n}\mathbb{I}_{[x_{(n)},+\infty)}(\theta)\)</span><br />
em que <span class="math inline">\(x_{(n)}=max\{x_1,\ldots,x_n\}\)</span>.<br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(f(\theta)=\dfrac{ab^a}{\theta^{a+1}}\mathbb{I}_{[b,+\infty]}(\theta)\)</span>.<br />
Então<br />
<span class="math inline">\(f(\theta| \boldsymbol x)\)</span> <span class="math inline">\(\propto f(\boldsymbol x|\theta)f(\theta)\)</span> <span class="math inline">\(=\dfrac{1}{\theta^{a+n+1}}\mathbb{I}_{[x_{(n)},+\infty)}(\theta)\mathbb{I}_{[b,+\infty)}(\theta)\)</span> <span class="math inline">\(=\dfrac{1}{\theta^{a+n+1}}\mathbb{I}_{[max\{b,x_{(n)}\},+\infty)}(\theta)\)</span><br />
<span class="math inline">\(~\)</span>
<span class="math inline">\(\Rightarrow \theta|\boldsymbol X = \boldsymbol x \sim Pareto(a+n,max\{b,x_{(n)}\})\)</span>.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Resultado 4.</strong> Seja <span class="math inline">\(X_1,\ldots,X_n,Y_1,\ldots,Y_m\)</span> v.a. condicionalmente independentes tais que <span class="math inline">\(X_i|\theta\sim Exp(\theta),i=1,\ldots,n\)</span> e <span class="math inline">\(Y_j|\theta \sim Poisson(\theta),j=1,\ldots,m\)</span>. Considere que, a priori, <span class="math inline">\(\theta \sim Gama(a,b)\)</span>. Então <span class="math inline">\(\theta| \boldsymbol x,\boldsymbol y \sim Gama(a+n+\sum_jy_j~,~b+m+\sum_ix_i)\)</span>.</p>
<blockquote>
<p><strong>Demo:</strong><br />
<span class="math inline">\(f(\boldsymbol x, \boldsymbol y|\theta)\overset{ci}{=}f(\boldsymbol x|\theta)f(\boldsymbol y|\theta)\overset{ci}{=}\)</span> <span class="math inline">\(\prod_{i=1}^nf(x_i|\theta)\prod_{j=1}^mf(y_i|\theta)=\)</span> <span class="math inline">\(\prod_{i=1}^n\theta e^{-\theta x_i}\prod_{j=1}^m\dfrac{\theta^{y_j}e^{-\theta}}{y_j!}=\)</span> <span class="math inline">\(\dfrac{1}{\prod_{j=1}^my_j!}\theta^{n+\sum_j y_j}e^{-(m+\sum_ix_i)\theta}\)</span><br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(f(\theta)=\dfrac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b\theta}\)</span>
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(f(\theta| \boldsymbol{x,y})\propto f(\boldsymbol x, \boldsymbol y|\theta)f(\theta)\propto\)</span> <span class="math inline">\(\theta^{[a+n+\sum_jy_j]-1}e^{-[b+m+\sum_ix_i]\theta}\)</span>
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(\Rightarrow \theta| \boldsymbol x,\boldsymbol y \sim Gama(a+n+\sum_jy_j,b+m+\sum_ix_i)\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Resultado 5.</strong> Seja <span class="math inline">\(~\mathcal{P}=\{f(\boldsymbol x|\theta):\; \theta \in \Theta\}~\)</span> e <span class="math inline">\(~\mathcal{C}=\{h(\theta|a):\;a\in A\}~\)</span> uma <em>família conjugada</em> para <span class="math inline">\(\mathcal{P}\)</span>. Considere <span class="math inline">\(\mathcal{M}=\{h(\theta)=\sum_{i=1}^mw_ih_i(\theta):\)</span> <span class="math inline">\(h_i \in \mathcal{C} \; e \; w_i&gt;0,\; \sum_{i=1}^m w_i=1\}\)</span>. Então <span class="math inline">\(\mathcal{M}\)</span> é <em>família conjugada</em> para <span class="math inline">\(\mathcal{P}\)</span>.</p>
<blockquote>
<p><strong>Demo:</strong> Como <span class="math inline">\(\mathcal{C}\)</span> é conjugada para <span class="math inline">\(\mathcal{P}\)</span>, para toda função <span class="math inline">\(h_i \in \mathcal{C}\)</span>, temos que <span class="math inline">\(f_i(\theta|\boldsymbol x)\propto h_i(\theta)f(\boldsymbol x|\theta)\in \mathcal{C}\)</span>. Então<br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(h\in \mathcal{M}\)</span> <span class="math inline">\(~\Rightarrow~ f(\theta|\boldsymbol x)\)</span> <span class="math inline">\(~\propto~ h(\theta)f(\boldsymbol x|\theta)\)</span> <span class="math inline">\(~\propto~\sum_{i=1}^m w_i\underbrace{h_i(\theta)f(\boldsymbol x|\theta)}_{\in \mathcal{C}}\)</span> <span class="math inline">\(~\propto~\sum_{i=1}^m w_i^*f_i(\theta|\boldsymbol x)\in \mathcal{M}\)</span>.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Exemplo.</strong> Seja <span class="math inline">\(X|\theta \sim Bin(n,\theta)\)</span> e <span class="math inline">\(f(\theta)\)</span> <span class="math inline">\(=wf_1(\theta)+(1-w)f_2(\theta)\)</span>, com <span class="math inline">\(f_1\sim Beta(a_1,b_1)\)</span> e <span class="math inline">\(f_2\sim Beta(a_2,b_2)\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(f(\theta|x)\)</span> <span class="math inline">\(=\dfrac{f(x|\theta)f(\theta)}{\int_0^1f(x|\theta)f(\theta)}\)</span> <span class="math inline">\(=\dfrac{f(x|\theta)[wf_1(\theta)+(1-w)f_2(\theta)]}{w\int_0^1f_1(\theta)f(x|\theta)d\theta+(1-w)\int_0^1f_2(\theta)f(x|\theta)d\theta}\)</span></p>
<p><span class="math inline">\(\propto\dfrac{w\binom{n}{x}\frac{\Gamma(a_1+b_1)}{\Gamma(a_1)\Gamma(b_1)}\theta^{a_1+x-1}(1-\theta)^{b_1+n-x-1}+(1-w)\binom{n}{x}\frac{\Gamma(a_2+b_2)}{\Gamma(a_2)\Gamma(b_2)}\theta^{a_2+x-1}(1-\theta)^{b_2+n-x-1}}{\underbrace{w\binom{n}{x}\frac{\Gamma(a_1+b_1)}{\Gamma(a_1)\Gamma(b_1)}\frac{\Gamma(a_1+x)\Gamma(b_1+n-x)}{\Gamma(a_1+b_1+n)}}_{A}+\underbrace{(1-w)\binom{n}{x}\frac{\Gamma(a_2+b_2)}{\Gamma(a_2)\Gamma(b_2)}\frac{\Gamma(a_2+x)\Gamma(b_2+n-x)}{\Gamma(a_2+b_2+n)}}_{B}}\)</span></p>
<p><span class="math inline">\(\propto~\underbrace{\dfrac{A}{A+B}}_{w^*}Beta(a_1+x,b_1+n-x)+\underbrace{\dfrac{B}{A+B}}_{1-w^*}Beta(a_2+x,b_2+n-x)\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<p>Primeiramente, suponha que <span class="math inline">\(n=5\)</span>, e temos uma mistura das distribuições <span class="math inline">\(Beta(5,12)\)</span> e <span class="math inline">\(Beta(10,3)\)</span>, com <span class="math inline">\(w=0.5\)</span>. O gráfico a seguir apresenta as distribuições a priori, a verossimilhança e a posteriori para cada possível valor de <span class="math inline">\(x\)</span> em <span class="math inline">\(\left\{0,1,\ldots,5\right\}\)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="Bayes.html#cb10-1" aria-hidden="true" tabindex="-1"></a>a1<span class="ot">=</span><span class="dv">5</span>; b1<span class="ot">=</span><span class="dv">12</span></span>
<span id="cb10-2"><a href="Bayes.html#cb10-2" aria-hidden="true" tabindex="-1"></a>a2<span class="ot">=</span><span class="dv">10</span>; b2<span class="ot">=</span><span class="dv">3</span> </span>
<span id="cb10-3"><a href="Bayes.html#cb10-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span><span class="dv">5</span></span>
<span id="cb10-4"><a href="Bayes.html#cb10-4" aria-hidden="true" tabindex="-1"></a>w<span class="ot">=</span><span class="fl">0.5</span></span>
<span id="cb10-5"><a href="Bayes.html#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="Bayes.html#cb10-6" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)</span>
<span id="cb10-7"><a href="Bayes.html#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="Bayes.html#cb10-8" aria-hidden="true" tabindex="-1"></a>A <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n)),<span class="dv">1</span>,</span>
<span id="cb10-9"><a href="Bayes.html#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(x){w<span class="sc">*</span><span class="fu">choose</span>(n,x)<span class="sc">*</span><span class="fu">gamma</span>(a1<span class="sc">+</span>b1)<span class="sc">/</span>(<span class="fu">gamma</span>(a1)<span class="sc">*</span><span class="fu">gamma</span>(b1))<span class="sc">*</span></span>
<span id="cb10-10"><a href="Bayes.html#cb10-10" aria-hidden="true" tabindex="-1"></a>    (<span class="fu">gamma</span>(a1<span class="sc">+</span>x)<span class="sc">*</span><span class="fu">gamma</span>(b1<span class="sc">+</span>n<span class="sc">-</span>x))<span class="sc">/</span><span class="fu">gamma</span>(a1<span class="sc">+</span>b1<span class="sc">+</span>n)}))</span>
<span id="cb10-11"><a href="Bayes.html#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="Bayes.html#cb10-12" aria-hidden="true" tabindex="-1"></a>B <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n)),<span class="dv">1</span>,</span>
<span id="cb10-13"><a href="Bayes.html#cb10-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(x){(<span class="dv">1</span><span class="sc">-</span>w)<span class="sc">*</span><span class="fu">choose</span>(n,x)<span class="sc">*</span><span class="fu">gamma</span>(a2<span class="sc">+</span>b2)<span class="sc">/</span>(<span class="fu">gamma</span>(a2)<span class="sc">*</span><span class="fu">gamma</span>(b2))<span class="sc">*</span></span>
<span id="cb10-14"><a href="Bayes.html#cb10-14" aria-hidden="true" tabindex="-1"></a>    (<span class="fu">gamma</span>(a2<span class="sc">+</span>x)<span class="sc">*</span><span class="fu">gamma</span>(b2<span class="sc">+</span>n<span class="sc">-</span>x))<span class="sc">/</span><span class="fu">gamma</span>(a2<span class="sc">+</span>b2<span class="sc">+</span>n)}))</span>
<span id="cb10-15"><a href="Bayes.html#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="Bayes.html#cb10-16" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">=</span> A<span class="sc">/</span>(A<span class="sc">+</span>B)</span>
<span id="cb10-17"><a href="Bayes.html#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="Bayes.html#cb10-18" aria-hidden="true" tabindex="-1"></a>prior2 <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n)),<span class="dv">1</span>,</span>
<span id="cb10-19"><a href="Bayes.html#cb10-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(x){w<span class="sc">*</span><span class="fu">dbeta</span>(theta,a1,b1)<span class="sc">+</span></span>
<span id="cb10-20"><a href="Bayes.html#cb10-20" aria-hidden="true" tabindex="-1"></a>              (<span class="dv">1</span><span class="sc">-</span>w)<span class="sc">*</span><span class="fu">dbeta</span>(theta,a2,b2)}))</span>
<span id="cb10-21"><a href="Bayes.html#cb10-21" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb10-22"><a href="Bayes.html#cb10-22" aria-hidden="true" tabindex="-1"></a>post2 <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">as.matrix</span>(<span class="fu">mapply</span>(<span class="cf">function</span>(x,w2){</span>
<span id="cb10-23"><a href="Bayes.html#cb10-23" aria-hidden="true" tabindex="-1"></a>  w2<span class="sc">*</span><span class="fu">dbeta</span>(theta,a1<span class="sc">+</span>x,b1<span class="sc">+</span>n<span class="sc">-</span>x)<span class="sc">+</span></span>
<span id="cb10-24"><a href="Bayes.html#cb10-24" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">-</span>w2)<span class="sc">*</span><span class="fu">dbeta</span>(theta,a2<span class="sc">+</span>x,b2<span class="sc">+</span>n<span class="sc">-</span>x)},<span class="fu">seq</span>(<span class="dv">0</span>,n),w2)))</span>
<span id="cb10-25"><a href="Bayes.html#cb10-25" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-26"><a href="Bayes.html#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="co">#vero = as.vector(apply(matrix(seq(0,n)),1,</span></span>
<span id="cb10-27"><a href="Bayes.html#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co"># function(x){dbinom(x,prob=theta,size=n)}))</span></span>
<span id="cb10-28"><a href="Bayes.html#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="Bayes.html#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Verossimilhança proporcional visualmente melhor</span></span>
<span id="cb10-30"><a href="Bayes.html#cb10-30" aria-hidden="true" tabindex="-1"></a>vero <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n)),<span class="dv">1</span>,</span>
<span id="cb10-31"><a href="Bayes.html#cb10-31" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(x){<span class="fu">dbeta</span>(theta,x<span class="sc">+</span><span class="dv">1</span>,n<span class="sc">-</span>x<span class="sc">+</span><span class="dv">1</span>)}))</span>
<span id="cb10-32"><a href="Bayes.html#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="Bayes.html#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x=</span><span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="fu">seq</span>(<span class="dv">0</span>,n),<span class="at">each=</span><span class="fu">length</span>(theta))),</span>
<span id="cb10-34"><a href="Bayes.html#cb10-34" aria-hidden="true" tabindex="-1"></a>    <span class="at">w2=</span><span class="fu">rep</span>(w2,<span class="at">each=</span><span class="fu">length</span>(theta)),</span>
<span id="cb10-35"><a href="Bayes.html#cb10-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">theta=</span><span class="fu">rep</span>(theta,(n<span class="sc">+</span><span class="dv">1</span>)),<span class="at">vero=</span>vero,<span class="at">prior=</span>prior2,<span class="at">post=</span>post2) <span class="sc">%&gt;%</span> </span>
<span id="cb10-36"><a href="Bayes.html#cb10-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb10-37"><a href="Bayes.html#cb10-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>post, <span class="at">colour=</span>x),<span class="at">lwd=</span><span class="fl">1.5</span>) <span class="sc">+</span> </span>
<span id="cb10-38"><a href="Bayes.html#cb10-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>prior,<span class="at">colour=</span><span class="st">&quot;Prior&quot;</span>),<span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb10-39"><a href="Bayes.html#cb10-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>vero,<span class="at">colour=</span><span class="st">&quot;Verossimilhança&quot;</span>),<span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">2</span>)<span class="sc">+</span></span>
<span id="cb10-40"><a href="Bayes.html#cb10-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(theta)) <span class="sc">+</span>   </span>
<span id="cb10-41"><a href="Bayes.html#cb10-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;f(&quot;</span>,theta,<span class="st">&quot;|x)&quot;</span>)))<span class="sc">+</span></span>
<span id="cb10-42"><a href="Bayes.html#cb10-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb10-43"><a href="Bayes.html#cb10-43" aria-hidden="true" tabindex="-1"></a>  gganimate<span class="sc">::</span><span class="fu">transition_states</span>(x)</span></code></pre></div>
<pre><code>## NULL</code></pre>
<p><span class="math inline">\(~\)</span></p>
<p>Agora, suponha que <span class="math inline">\(n=5\)</span> e foi observado <span class="math inline">\(x=2\)</span>. Novamente, considere a mistura das distribuições <span class="math inline">\(Beta(5,12)\)</span> e <span class="math inline">\(Beta(10,3)\)</span> mas agora com pesos <span class="math inline">\(w\)</span> variando no conjunto <span class="math inline">\(\left\{0,0.1,\ldots,0.9,1\right\}\)</span>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="Bayes.html#cb12-1" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span><span class="dv">5</span>; x<span class="ot">=</span><span class="dv">2</span></span>
<span id="cb12-2"><a href="Bayes.html#cb12-2" aria-hidden="true" tabindex="-1"></a>w <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>)</span>
<span id="cb12-3"><a href="Bayes.html#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="Bayes.html#cb12-4" aria-hidden="true" tabindex="-1"></a>A <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(w),<span class="dv">1</span>,</span>
<span id="cb12-5"><a href="Bayes.html#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(w){w<span class="sc">*</span><span class="fu">choose</span>(n,x)<span class="sc">*</span><span class="fu">gamma</span>(a1<span class="sc">+</span>b1)<span class="sc">/</span>(<span class="fu">gamma</span>(a1)<span class="sc">*</span></span>
<span id="cb12-6"><a href="Bayes.html#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">gamma</span>(b1))<span class="sc">*</span>(<span class="fu">gamma</span>(a1<span class="sc">+</span>x)<span class="sc">*</span><span class="fu">gamma</span>(b1<span class="sc">+</span>n<span class="sc">-</span>x))<span class="sc">/</span><span class="fu">gamma</span>(a1<span class="sc">+</span>b1<span class="sc">+</span>n)}))</span>
<span id="cb12-7"><a href="Bayes.html#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="Bayes.html#cb12-8" aria-hidden="true" tabindex="-1"></a>B <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(w),<span class="dv">1</span>,</span>
<span id="cb12-9"><a href="Bayes.html#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(w){(<span class="dv">1</span><span class="sc">-</span>w)<span class="sc">*</span><span class="fu">choose</span>(n,x)<span class="sc">*</span><span class="fu">gamma</span>(a2<span class="sc">+</span>b2)<span class="sc">/</span>(<span class="fu">gamma</span>(a2)<span class="sc">*</span></span>
<span id="cb12-10"><a href="Bayes.html#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">gamma</span>(b2))<span class="sc">*</span>(<span class="fu">gamma</span>(a2<span class="sc">+</span>x)<span class="sc">*</span><span class="fu">gamma</span>(b2<span class="sc">+</span>n<span class="sc">-</span>x))<span class="sc">/</span><span class="fu">gamma</span>(a2<span class="sc">+</span>b2<span class="sc">+</span>n)}))</span>
<span id="cb12-11"><a href="Bayes.html#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="Bayes.html#cb12-12" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">=</span> A<span class="sc">/</span>(A<span class="sc">+</span>B)</span>
<span id="cb12-13"><a href="Bayes.html#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="Bayes.html#cb12-14" aria-hidden="true" tabindex="-1"></a>prior2 <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(w),<span class="dv">1</span>,<span class="cf">function</span>(w){</span>
<span id="cb12-15"><a href="Bayes.html#cb12-15" aria-hidden="true" tabindex="-1"></a>  w<span class="sc">*</span><span class="fu">dbeta</span>(theta,a1,b1)<span class="sc">+</span>(<span class="dv">1</span><span class="sc">-</span>w)<span class="sc">*</span><span class="fu">dbeta</span>(theta,a2,b2)}))</span>
<span id="cb12-16"><a href="Bayes.html#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="Bayes.html#cb12-17" aria-hidden="true" tabindex="-1"></a>post2 <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">as.matrix</span>(<span class="fu">mapply</span>(<span class="cf">function</span>(w,w2){</span>
<span id="cb12-18"><a href="Bayes.html#cb12-18" aria-hidden="true" tabindex="-1"></a>  w2<span class="sc">*</span><span class="fu">dbeta</span>(theta,a1<span class="sc">+</span>x,b1<span class="sc">+</span>n<span class="sc">-</span>x)<span class="sc">+</span></span>
<span id="cb12-19"><a href="Bayes.html#cb12-19" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">-</span>w2)<span class="sc">*</span><span class="fu">dbeta</span>(theta,a2<span class="sc">+</span>x,b2<span class="sc">+</span>n<span class="sc">-</span>x)},w,w2)))</span>
<span id="cb12-20"><a href="Bayes.html#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="Bayes.html#cb12-21" aria-hidden="true" tabindex="-1"></a>vero <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">rep</span>(x,<span class="dv">2</span><span class="sc">*</span>n<span class="sc">+</span><span class="dv">1</span>)),<span class="dv">1</span>,</span>
<span id="cb12-22"><a href="Bayes.html#cb12-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(x){<span class="fu">dbeta</span>(theta,x<span class="sc">+</span><span class="dv">1</span>,n<span class="sc">-</span>x<span class="sc">+</span><span class="dv">1</span>)}))</span>
<span id="cb12-23"><a href="Bayes.html#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="Bayes.html#cb12-24" aria-hidden="true" tabindex="-1"></a>z<span class="ot">&lt;-</span><span class="fu">length</span>(w)</span>
<span id="cb12-25"><a href="Bayes.html#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="Bayes.html#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">w=</span><span class="fu">as.factor</span>(<span class="fu">rep</span>(w,<span class="at">each=</span><span class="fu">length</span>(theta))),</span>
<span id="cb12-27"><a href="Bayes.html#cb12-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">w2=</span><span class="fu">rep</span>(w2,<span class="at">each=</span><span class="fu">length</span>(theta)),</span>
<span id="cb12-28"><a href="Bayes.html#cb12-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">theta=</span><span class="fu">rep</span>(theta,z), <span class="at">prior =</span> prior2, </span>
<span id="cb12-29"><a href="Bayes.html#cb12-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">post =</span> post2, <span class="at">vero =</span> vero) <span class="sc">%&gt;%</span> </span>
<span id="cb12-30"><a href="Bayes.html#cb12-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="at">colour =</span> w) <span class="sc">+</span> </span>
<span id="cb12-31"><a href="Bayes.html#cb12-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>post, <span class="at">colour=</span>w),<span class="at">lwd=</span><span class="fl">1.5</span>) <span class="sc">+</span> </span>
<span id="cb12-32"><a href="Bayes.html#cb12-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>prior,<span class="at">colour=</span><span class="st">&quot;Priori&quot;</span>)) <span class="sc">+</span></span>
<span id="cb12-33"><a href="Bayes.html#cb12-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>vero,<span class="at">colour=</span><span class="st">&quot;Verossimilhança&quot;</span>),<span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">2</span>)<span class="sc">+</span></span>
<span id="cb12-34"><a href="Bayes.html#cb12-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(theta)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;f(&quot;</span>,theta,<span class="st">&quot;|x)&quot;</span>)))<span class="sc">+</span></span>
<span id="cb12-35"><a href="Bayes.html#cb12-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb12-36"><a href="Bayes.html#cb12-36" aria-hidden="true" tabindex="-1"></a>  gganimate<span class="sc">::</span><span class="fu">transition_states</span>(w)</span></code></pre></div>
<pre><code>## NULL</code></pre>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
</div>
<div id="prioris-não-informativas" class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Prioris “Não-Informativas”</h3>
<p>Priors não-informativas são tentativas de representar formalmente um estado de ignorância. Contudo, não existe uma forma única de representar ignorância, tampouco uma priori “objetiva.” Além disso, é bastante raro um cenário onde não há nenhuma informação a priori. De qualquer modo, serão apresentadas aqui algumas formas de representar falta de informação mas a escolha da priori será sempre subjetiva.</p>
<p><span class="math inline">\(~\)</span></p>
<div id="priori-de-bayes-laplace" class="section level4" number="3.4.4.1">
<h4><span class="header-section-number">3.4.4.1</span> Priori de Bayes-Laplace</h4>
<p><strong>Princípio da Razão Insuficiente.</strong> <em>Quando não existe razão suficiente para acreditar mais em algum subconjunto do espaço paramétrico <span class="math inline">\(\Theta\)</span>, deve-se adotar equiprobabilidade.</em></p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 1.</strong> Se <span class="math inline">\(\Theta=\left\{\theta_1,\theta_2,\ldots,\theta_k\right\}\)</span> então a priori de Bayes-Laplace é <span class="math inline">\(f(\theta)=1/k\)</span>, <span class="math inline">\(\theta \in \Theta~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 2.</strong> Se <span class="math inline">\(\Theta=\left[a,b\right]\)</span> então a priori de Bayes-Laplace é <span class="math inline">\(f(\theta)=1/(b-a)\)</span>, <span class="math inline">\(\theta \in \Theta~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(f(\theta|\boldsymbol{x})\)</span>
<span class="math inline">\(= \dfrac{f(\theta)f(\boldsymbol{x}|\theta)}{\int_\Theta f(\theta)f(\boldsymbol{x}|\theta)~d\theta}\)</span>
<span class="math inline">\(= \dfrac{c~f(\boldsymbol{x}|\theta)}{c~\int_\Theta f(\boldsymbol{x}|\theta)~d\theta}\)</span>
<span class="math inline">\(= \dfrac{f(\boldsymbol{x}|\theta)}{\int_\Theta f(\boldsymbol{x}|\theta)~d\theta}\)</span>
<span class="math inline">\(\propto f(\boldsymbol{x}|\theta)~.\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p>As principais críticas da priori de Bayes-Laplace são</p>
<ol style="list-style-type: decimal">
<li><p>A distribuição é <em>imprópria</em> quando o espaço paramétrico <span class="math inline">\(\Theta\)</span> não é finito ou limitado. Por exemplo, <span class="math inline">\(\Theta=\mathbb{N}\)</span>, <span class="math inline">\(\Theta=\mathbb{Z}\)</span> ou <span class="math inline">\(\Theta=\mathbb{R}\)</span>. Nesses casos, a priori de Bayes-Laplace é <span class="math inline">\(f(\theta)\propto \mathbb{I}_\Theta(\theta)\)</span>, que não é uma distribuição de probabilidade.</p></li>
<li><p>Não é <em>invariante</em> a reparametrizações. Considere, por exemplo, <span class="math inline">\(f(\theta)\)</span> uma f.d.p. a priori para <span class="math inline">\(\theta\)</span> e <span class="math inline">\(g\)</span> uma transformação um-a-um (injetora) de <span class="math inline">\(\theta\)</span> tal que <span class="math inline">\(\psi=g(\theta)\)</span>. A distribuição de <span class="math inline">\(\psi\)</span> pode ser calculada por <span class="math inline">\(f_\psi(\psi) = f\left(g^{-1}(\psi)\right)\left|\dfrac{dg^{-1}(\psi)}{d\psi}\right|~.\)</span> Assim, se <span class="math inline">\(g\)</span> é uma transformação não linear e a distribuição a priori para <span class="math inline">\(\theta\)</span> é uniforme, a distribuição para <span class="math inline">\(\psi\)</span> não é uniforme, em geral.</p></li>
</ol>
<p><span class="math inline">\(~\)</span></p>
</div>
<div id="priori-de-jeffreys" class="section level4" number="3.4.4.2">
<h4><span class="header-section-number">3.4.4.2</span> Priori de Jeffreys</h4>
<p>Seja <span class="math inline">\(g\)</span> uma transformação um-a-um do parâmetro <span class="math inline">\(\theta\)</span> e defina <span class="math inline">\(\psi=g(\theta)\)</span>. Considere uma função <span class="math inline">\(h:\mathfrak{X}\times\Theta\longrightarrow\mathbb{R}\)</span>. Uma classe de distribuições a priori invariantes pode ser definida por
<span class="math display">\[f(\theta) \propto \left(\text{Var}_{X|\theta}\left[\dfrac{\partial h(\boldsymbol X | \theta)}{\partial\theta}~\bigg|~\theta\right]\right)^{1/2}~.\]</span></p>
<blockquote>
<p><strong>Demo.</strong> Para mostrar a invariância do método, considere o caso contínuo em que
<span class="math display">\[f_\psi(\psi) = f\left(g^{-1}(\psi)\right)\left|\dfrac{\partial g^{-1}(\psi)}{\partial\psi}\right|~.\]</span>
Seja <span class="math inline">\(h^*(x,\psi)=h\left(x,g^{-1}(\psi)\right)\)</span>. Então<br />
<span class="math inline">\(\dfrac{\partial h^*(x,\psi)}{\partial\psi}\)</span>
<span class="math inline">\(=\dfrac{\partial h\left(x,g^{-1}(\psi)\right)}{\partial\psi}\)</span>
<span class="math inline">\(=\left.\dfrac{\partial h(x,\theta)}{\partial\theta}\right|_{\theta=g^{-1}(\psi)}\cdot\dfrac{\partial g^{-1}(\psi)}{\partial\psi}~,\)</span><br />
e, portanto,<br />
<span class="math inline">\(\text{Var}\left[\dfrac{\partial h^*(\boldsymbol{X},\psi)}{\partial\psi}~\bigg|~\theta=g^{-1}(\psi)\right]\)</span>
<span class="math inline">\(=\text{Var}\left[\dfrac{\partial h(\boldsymbol{X},\theta)}{\partial\theta}~\bigg|~\theta=g^{-1}(\psi)\right]\cdot\left[\dfrac{\partial g^{-1}(\psi)}{\partial\psi}\right]^2\)</span>
<span class="math inline">\(=\left[f\left(g^{-1}(\psi)\right)\left(\dfrac{\partial g^{-1}(\psi)}{\partial\psi}\right)\right]^2~,\)</span><br />
de modo que<br />
<span class="math inline">\(f_\psi(\psi)\)</span> <span class="math inline">\(=f\left(g^{-1}(\psi)\right)\left|\dfrac{\partial g^{-1}(\psi)}{\partial\psi}\right|\)</span>
<span class="math inline">\(=\text{Var}\left[\dfrac{\partial h^*(\boldsymbol{X},\psi)}{\partial\psi}~\bigg|~\theta=g^{-1}(\psi)\right]^{1/2}~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p>A escolha mais usual para <span class="math inline">\(h\)</span> é <span class="math inline">\(h(\boldsymbol{x},\theta)=\log f(\boldsymbol{x}|\theta)~.\)</span> Assim, como <span class="math inline">\(E\left[\dfrac{\partial \log f(\boldsymbol{X}|\theta)}{\partial\theta}~\bigg|~\theta\right]=0\)</span>, temos<br />
<span class="math inline">\(f(\theta)\)</span>
<span class="math inline">\(\propto\text{Var}\left[\dfrac{\partial \log f(\boldsymbol{X}|\theta)}{\partial\theta}~\bigg|~\theta\right]^{1/2}\)</span>
<span class="math inline">\(=\text{E}\left[\left(\dfrac{\partial \log f(\boldsymbol{X}|\theta)}{\partial\theta}\right)^2~\bigg|~\theta\right]^{1/2}\)</span>
<span class="math inline">\(=\left[\mathcal{I}(\theta)\right]^{1/2}~,\)</span><br />
onde <span class="math inline">\(\mathcal{I}(\theta)\)</span> é a <em>Informação de Fisher</em> de <span class="math inline">\(\theta\)</span>. Neste caso, <span class="math inline">\(f(\theta)\propto\big|\mathcal{I}(\theta)\big|^{1/2}\)</span> é chamada <strong>priori de Jeffreys</strong>.</p>
<p>Uma motivação para o método de Jeffreys é que a informação de Fisher <span class="math inline">\(\mathcal{I}(\theta)\)</span> é um indicador da quantidade de informação trazida pelo modelo (observações) sobre o parâmetro <span class="math inline">\(\theta\)</span>. Favorecer os valores de <span class="math inline">\(\theta\)</span> para o qual <span class="math inline">\(\mathcal{I}(\theta)\)</span> é grande supostamente minimiza a influência da priori.</p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 1.</strong> Considere novamente o experimento de lançar uma moeda <span class="math inline">\(n\)</span> vezes e contar o número de caras, isto é, <span class="math inline">\(X|\theta \sim \text{Bin}(n,\theta)\)</span>. Então,<br />
<span class="math inline">\(f(x|\theta)=\displaystyle\binom{n}{x}\theta^x(1-\theta)^{n-x}\)</span>
<span class="math inline">\(\Longrightarrow~ \log f(x|\theta)=\log\binom{n}{x}+x\log\theta+(n-x)\log(1-\theta)\)</span><br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(\dfrac{\partial\log f(x|\theta)}{\partial\theta}\)</span>
<span class="math inline">\(=\dfrac{x}{\theta}-\dfrac{n-x}{1-\theta}\)</span>
<span class="math inline">\(=\dfrac{x-n\theta}{\theta(1-\theta)}~.\)</span><br />
<span class="math inline">\(~\)</span><br />
Como <span class="math inline">\(E\left[X|\theta\right]=n\theta\)</span> e <span class="math inline">\(Var(X|\theta)\)</span> <span class="math inline">\(=E\left[\left(X-E\left[X|\theta\right]\right)^2~\Big|~\theta\right]\)</span> <span class="math inline">\(=E\left[\left(X-n\theta\right)^2~\Big|~\theta\right]\)</span> <span class="math inline">\(=n\theta(1-\theta)\)</span>, a informação de Fisher neste caso é<br />
<span class="math inline">\(\mathcal{I}_x(\theta)\)</span> <span class="math inline">\(=\text{E}\left[\left(\dfrac{\partial\log f(x|\theta)}{\partial\theta}\right)^2~\bigg|~\theta\right]\)</span> <span class="math inline">\(=\text{E}\left[\left(\dfrac{X-n\theta}{\theta(1-\theta)}\right)^2~\bigg|~\theta\right]\)</span>
<span class="math inline">\(=\dfrac{1}{\theta^2(1-\theta)^2}~\text{E}\left[\left(X-n\theta\right)^2~|~\theta\right]\)</span>
<span class="math inline">\(=\dfrac{1}{\theta^2(1-\theta)^2}~\text{Var}\left(X~|~\theta\right)\)</span>
<span class="math inline">\(=\dfrac{n~\theta(1-\theta)}{\theta^2(1-\theta)^2}\)</span>
<span class="math inline">\(=\dfrac{n}{\theta(1-\theta)}\)</span> <span class="math inline">\(=n\theta^{-1}(1-\theta)^{-1}~,\)</span><br />
<span class="math inline">\(~\)</span><br />
de modo que a priori de Jeffreys é<br />
<span class="math inline">\(f(\theta)\)</span> <span class="math inline">\(\propto\left[\mathcal{I}_x(\theta)\right]^{1/2}\)</span>
<span class="math inline">\(\propto\theta^{-1/2}(1-\theta)^{-1/2}\)</span> <span class="math inline">\(~\Longrightarrow~ \theta \sim \text{Beta}\left(\frac{1}{2},\frac{1}{2}\right)~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 2.</strong> Considere agora que a mesma moeda é lançada e anota-se o número de caras <span class="math inline">\(Y\)</span> até que sejam observadas <span class="math inline">\(r\)</span> coroas, isto é, <span class="math inline">\(Y|\theta \sim \text{BinNeg}(r,\theta)\)</span>. Então,
<span class="math inline">\(f(y|\theta)=\displaystyle\binom{y+r-1}{y}\theta^y(1-\theta)^{r}\)</span>
<span class="math inline">\(\Longrightarrow~ \log f(y|\theta)=\log\binom{y+k-1}{y}+y\log\theta+r\log(1-\theta)\)</span><br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(\dfrac{\partial\log f(y|\theta)}{\partial\theta}\)</span>
<span class="math inline">\(=\dfrac{y}{\theta}-\dfrac{r}{1-\theta}\)</span>
<span class="math inline">\(=\dfrac{1}{\theta}\left[y-\dfrac{r~\theta}{1-\theta}\right]~.\)</span><br />
<span class="math inline">\(~\)</span><br />
Como <span class="math inline">\(E\left[X|\theta\right]=\dfrac{r~\theta}{1-\theta}\)</span> e <span class="math inline">\(Var(X|\theta)=\dfrac{r~\theta}{(1-\theta)^2}\)</span>, a informação de Fisher neste caso é<br />
<span class="math inline">\(\mathcal{I}_y(\theta)\)</span> <span class="math inline">\(=\text{E}\left[\dfrac{1}{\theta^2}\left(y-\dfrac{r~\theta}{1-\theta}\right)^2~\bigg|~\theta\right]\)</span>
<span class="math inline">\(=\dfrac{1}{\theta^2}~\text{Var}\left(Y~|~\theta\right)\)</span>
<span class="math inline">\(=\dfrac{r}{\theta(1-\theta)^2}\)</span> <span class="math inline">\(=r\theta^{-1}(1-\theta)^{-2}~,\)</span><br />
<span class="math inline">\(~\)</span><br />
de modo que a priori de Jeffreys é<br />
<span class="math inline">\(f(\theta)\)</span> <span class="math inline">\(\propto\left[\mathcal{I}_y(\theta)\right]^{1/2}\)</span>
<span class="math inline">\(\propto\theta^{-1/2}(1-\theta)^{-1}~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p>Note que nos exemplos apresentados, a priori depende da <em>regra de parada</em>, isto é, a forma como decidimos quando parar de lançar a moeda e que determina se o modelo estatístico é binomial ou binomial negativo. Em outras palavras, a opinião a priori definida dessa forma depende do modelo adotado, mesmo que o parâmetro seja o mesmo nos dois casos. Além disso, a priori de Jeffreys pode ser <em>imprópria</em>, como ocorre no exemplo anterior.</p>
<p><span class="math inline">\(~\)</span></p>
</div>
<div id="priori-de-máxima-entropia" class="section level4" number="3.4.4.3">
<h4><span class="header-section-number">3.4.4.3</span> Priori de Máxima Entropia</h4>
<p><strong>Entropia</strong> é um conceito físico que quantifica a desordem ou imprevisibilidade de um sistema, ou da falta de informação sobre ele. O conceito de entropia desempenha um importante papel na teoria da informação. O <em>princípio da máxima entropia</em> afirma que a distribuição de probabilidade que melhor representa a falta de informação é aquela com a maior entropia.</p>
<p><strong>Caso Discreto.</strong> Considere um espaço paramétrico enumerável <span class="math inline">\(\Theta = \{\theta_1,\theta_2,\ldots\}\)</span>. A <em>entropia</em> da distribuição <span class="math inline">\(h\)</span> <span class="citation">(<a href="#ref-Shannon48" role="doc-biblioref">Shannon 1948</a>)</span> é dada por</p>
<p><span class="math display">\[\mathcal{E}(h)=\text{E}[-\log h(\theta)]=\displaystyle-\sum_{\theta\in\Theta} \log\left[h(\theta)\right]~h(\theta)~.\]</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Definição.</strong> Considere um espaço paramétrico <span class="math inline">\(\Theta\)</span> e <span class="math inline">\(h\)</span> uma f.d.p. para <span class="math inline">\(\theta\)</span>. A <em>distribuição da máxima entropia</em> para <span class="math inline">\(\theta\)</span> é a função <span class="math inline">\(h\)</span> que maximiza <span class="math inline">\(\mathcal{E}(h)\)</span> <span class="citation">(<a href="#ref-Jaynes03" role="doc-biblioref">Jaynes 2003</a>)</span></p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 1.</strong> Considere o espaço paramétrico <span class="math inline">\(\Theta=\{\theta_1,\ldots,\theta_k\}\)</span> e <span class="math inline">\(h(\theta_i)=p_i\)</span> uma distribuição discreta para <span class="math inline">\(\theta\)</span>. A <em>distribuição da máxima entropia</em> para <span class="math inline">\(\theta\)</span> é a função <span class="math inline">\(h\)</span> que maximiza <span class="math inline">\(\mathcal{E}(h)=-\displaystyle\sum_{i=1}^{k} p_i\log(p_i)\)</span> com a restrição <span class="math inline">\(\displaystyle\sum_{i=1}^k h(\theta_i)=\sum_{i=1}^k p_i=1~.\)</span><br />
Utilizando o método de multiplicadores de Lagrange, deve-se maximizar a função lagrangiana<br />
<span class="math inline">\(\mathcal{E}^*(h)=\displaystyle-\sum_{i=1}^k p_i\log(p_i)+\lambda\left(\sum_{i=1}^k p_i-1\right)\)</span><br />
<span class="math inline">\(\dfrac{\partial\mathcal{E}^*(h)}{\partial p_i}=-\left[p_i~\dfrac{1}{p_i}+\log(p_i)\right]+\lambda=0\)</span> <span class="math inline">\(\Longleftrightarrow p_i = e^{\lambda-1}~~,~~~i=1,\ldots,k~.\)</span><br />
Assim, como <span class="math inline">\(p_i\)</span> deve ser constante e <span class="math inline">\(\sum p_i=1\)</span>, conclui-se que <span class="math inline">\(p_i=1/k\)</span>, para <span class="math inline">\(i=1,\ldots,k~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 2.</strong> Considere agora <span class="math inline">\(\Theta = \{\theta_1,\theta_2,\ldots\}\)</span> e suponha que há <span class="math inline">\(m\)</span> informações parciais a respeito do parâmetro <span class="math inline">\(\theta\)</span> que podem ser escritas como <span class="math inline">\(\text{E}[g_j(\theta)]=\mu_j~,~\)</span> <span class="math inline">\(j=1,\ldots,m~.\)</span><br />
Usando novamente o método de Lagrange, deve-se maximizar<br />
<span class="math inline">\(\mathcal{E}^*(h)\)</span> <span class="math inline">\(=\displaystyle\sum_{i=1}^\infty p_i\log(p_i)+\lambda\left(\sum_{i=1}^\infty p_i-1\right)+ \sum_{j=1}^m\lambda_j\left(\sum_{i=1}^\infty p_i~g_j(\theta_i)-\mu_j\right)\)</span><br />
<span class="math inline">\(\dfrac{\partial \mathcal{E}^*(h)}{\partial p_i}=\displaystyle-\log(p_i)-1+\lambda+\sum_{j=1}^m\lambda_j~g_j(\theta_i)=0\)</span>
<span class="math inline">\(\Longleftrightarrow p_i \propto e^{\lambda-1+\sum_{j=1}^m \lambda_j~g_j(\theta_i)}\)</span> <span class="math inline">\(\propto e^{\sum_{j=1}^m \lambda_j~g_j(\theta_i)}~~,~~~i=1,\ldots,k~.\)</span><br />
Como <span class="math inline">\(\sum p_i=1\)</span>, <span class="math inline">\(p_i = \dfrac{e^{\sum_{j=1}^m \lambda_j~g_j(\theta_i)}}{\sum_{i=1}^\infty e^{\sum_{j=1}^m \lambda_j~g_j(\theta_i)}}~\)</span> e <span class="math inline">\(\lambda_j\)</span> é obtido por meio das restrições.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 2a.</strong> Seja <span class="math inline">\(\Theta = \{0,1,2,\ldots\}\)</span> e suponha que <span class="math inline">\(\text{E}[\theta]=\mu.\)</span><br />
Usando o resultado do exemplo anterior com <span class="math inline">\(g(\theta)=\theta\)</span> e <span class="math inline">\(\theta_i=i\)</span>, <span class="math inline">\(i=0,1,2,\ldots~,\)</span><br />
<span class="math inline">\(p_i=\dfrac{e^{\sum_{j=1}^m \lambda_j~g_j(\theta_i)}}{\sum_{i=0}^\infty e^{\sum_{j=1}^m \lambda_j~g_j(\theta_i)}}\)</span>
<span class="math inline">\(=\dfrac{e^{\lambda~i}}{\sum_{i=0}^\infty e^{\lambda~i}}~\)</span>
<span class="math inline">\(\overset{\left|e^\lambda\right|&lt;1}{=}~\dfrac{e^{\lambda~i}}{1/\left(1-e^\lambda\right)}\)</span>
<span class="math inline">\(=\left(e^\lambda\right)^i\left(1-e^\lambda\right)\)</span>
<span class="math inline">\(\Longrightarrow \theta \sim \text{Geo}\left(1-e^\lambda\right)~.\)</span><br />
Como <span class="math inline">\(\text{E}\left[\theta\right]=\dfrac{e^\lambda}{\left(1-e^\lambda\right)}=\mu\)</span>, tem-se que <span class="math inline">\(\lambda=\log\dfrac{\mu}{1+\mu}~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 2b.</strong> Considere que <span class="math inline">\(\Theta = \{1,2,\ldots,k\}\)</span> e suponha que <span class="math inline">\(\text{Med}(\theta)=m~.\)</span><br />
Nesse caso, <span class="math inline">\(g(\theta)=\mathbb{I}\left(\theta\leq m\right)\)</span> e <span class="math inline">\(\theta_i=i\)</span>, <span class="math inline">\(i=1,2,\ldots,k~,\)</span> de modo que<br />
<span class="math inline">\(\text{E}\left[g(\theta)\right]\)</span> <span class="math inline">\(=\text{E}\left[\mathbb{I}(\theta\leq m)\right]\)</span> <span class="math inline">\(=\text{P}\left(\theta\leq m\right)=1/2\)</span> e, portanto, <span class="math inline">\(\displaystyle\sum_{i\leq m}p_i=\sum_{j&gt; m}p_j=1/2~.\)</span>
<span class="math inline">\(p_i=\dfrac{e^{\sum_{j=1}^m \lambda_j~g_j(\theta_i)}}{\sum_{i=1}^k e^{\sum_{j=1}^m \lambda_j~g_j(\theta_i)}}\)</span>
<span class="math inline">\(=\left\{\begin{array}{lll} \dfrac{e^\lambda}{\sum_{i\leq m} e^\lambda}&amp;,&amp; i\leq m \\ \dfrac{1}{\sum_{i\leq m} 1}&amp;,&amp; i&gt; m\end{array}\right.\)</span>
<span class="math inline">\(=\left\{\begin{array}{lll} \dfrac{1}{2m}&amp;,&amp; i\leq m \\ \dfrac{1}{2(k-m)}&amp;,&amp; i&gt; m\end{array}\right.\)</span><br />
(A distribuição de <span class="math inline">\(\theta\)</span> é uniforme por blocos.)</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Divergência de Kullbach-Leibler.</strong> Considere duas distribuições discretas <span class="math inline">\(\boldsymbol{p} = (p_1,\ldots,p_k)\)</span> e <span class="math inline">\(\boldsymbol{q} = (q_1,\ldots,q_k)~,\)</span> tal que <span class="math inline">\(p_i,q_i&gt;0~,\)</span> <span class="math inline">\(i=1,\ldots,k~,\)</span> e <span class="math inline">\(\sum p_i=\sum q_i=1\)</span>. A <em>divergência de Kullbach-Leibler</em> entre <span class="math inline">\(\boldsymbol{p}\)</span> e <span class="math inline">\(\boldsymbol{q}\)</span> <span class="citation">(<a href="#ref-Kullback51" role="doc-biblioref">Kullback and Leibler 1951</a>)</span> é dada por<br />
<span class="math display">\[D(\boldsymbol{p}~||~\boldsymbol{q})=\sum p_i\log\left(\dfrac{p_i}{q_i}\right)~.\]</span></p>
<p><span class="math inline">\(~\)</span></p>
<p>Suponha que <span class="math inline">\(g=(1/k,\ldots,1/k)\)</span></p>
<p><span class="math inline">\(D(\boldsymbol{p}~||~\boldsymbol{q})\)</span>
<span class="math inline">\(=\displaystyle\sum_{i=1}^{k} p_i\log\left(\dfrac{p_i}{1/k}\right)\)</span>
<span class="math inline">\(=\displaystyle\sum_{i=1}^{k}p_i\left[ln(p_i)-ln(1/k)\right]\)</span>
<span class="math inline">\(=\displaystyle\sum_{i=1}^kp_i ln(pi)+ln(k)\sum_{i=1}^k p_i\)</span> <span class="math inline">\(=ln(k)-\mathcal{E}(\boldsymbol p)\)</span></p>
<p>Assim, exceto por uma constante, <span class="math inline">\(\mathcal{E}(\boldsymbol p)\)</span> está associado com quanto a distribuição <span class="math inline">\(\boldsymbol p\)</span> “diverge” da distribuição uniforme (priori de referência na ausência total de informação).</p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Observação:</strong> No caso geral, se <span class="math inline">\(H\)</span> e <span class="math inline">\(H_0\)</span> são duas medidas definidas em <span class="math inline">\(\Theta\)</span> tais que <span class="math inline">\(H\)</span> é absolutamente contínua com relação à <span class="math inline">\(H_0\)</span> <span class="math inline">\((H\ll H_0)\)</span>, a divergência de Kullbach-Leibler é definida como<br />
<span class="math display">\[D(H~||~H_0)=\displaystyle\int_\Theta \log\left(\dfrac{dH}{dH_0}\right)dH~,\]</span>
em que <span class="math inline">\(\dfrac{dH}{dH_0}\)</span> é derivada de Radon-Nikodym. Se <span class="math inline">\(H\)</span> e <span class="math inline">\(H_0\)</span> são medidas de probabilidade absolutamente contínuas com relação a medida de Lebesgue <span class="math inline">\(\lambda\)</span> com f.d.p. <span class="math inline">\(\dfrac{dH}{d\lambda}=h\)</span> e <span class="math inline">\(\dfrac{dH_0}{d\lambda}=h_0\)</span>, temos que,</p>
<p><span class="math inline">\(D(H~||~H_0)\)</span> <span class="math inline">\(=\displaystyle\int_\Theta \log\left(\dfrac{dH/d\lambda}{dH_0/d\lambda}\right)\dfrac{dH}{d\lambda}d\lambda\)</span>
<span class="math inline">\(=\displaystyle\int_\Theta \log\left(\dfrac{h(\theta)}{h_0(\theta)}\right)h(\theta)~d\theta\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p>Como a definição anterior de entropia vale apenas para o caso discreto, <span class="citation"><a href="#ref-Jaynes03" role="doc-biblioref">Jaynes</a> (<a href="#ref-Jaynes03" role="doc-biblioref">2003</a>)</span> sugere que no caso contínuo seja utilizada a <strong>entropia relativa</strong>, dada por
<span class="math display">\[\mathcal{E}(h)=-\displaystyle\int_\Theta h(\theta)\log\left(\dfrac{h(\theta)}{h_0(\theta)}\right)d\theta=-D(h~||~h_0)~,\]</span>
onde <span class="math inline">\(h_0\)</span> é uma priori de referência na ausência total de informação, preferivelmente invariante.</p>
<p><span class="math inline">\(~\)</span></p>
<p>Assim como no caso discreto, se temos <span class="math inline">\(m\)</span> restrições <span class="math inline">\(E[g_i(\theta)]=\mu_i,\)</span> a densidade de máxima entropia é</p>
<p><span class="math inline">\(h(\theta)\propto h_0(\theta)\exp\left\{\displaystyle\sum_{j=1}^m\lambda_j~ g_j(\theta)\right\}\)</span> e os <span class="math inline">\(\lambda_j~,\)</span> <span class="math inline">\(j=1,\ldots,m~,\)</span> são obtidos das restrições.</p>
<p>Por exemplo, se <span class="math inline">\(g(\theta)=\theta\)</span> com <span class="math inline">\(E[\theta]=\mu\)</span>, basta fazer</p>
<p><span class="math inline">\(\mu = \displaystyle \int_\Theta \theta~c~ h_0(\theta)\exp\{\lambda\theta\}~d\theta\)</span> com <span class="math inline">\(c^{-1}=\displaystyle\int_\Theta h_0(\theta)exp\{\lambda \theta\}d\theta\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 1:</strong> <span class="math inline">\(\Theta = \mathbb{R}_+\)</span> e <span class="math inline">\(\text{E}[\theta]=\mu~.\)</span><br />
Tomando <span class="math inline">\(h_0(\theta) \propto \mathbb{I}_{\mathbb{R}_+}(\theta)\)</span> (f.d.p. imprópria), tem-se <span class="math inline">\(h(\theta )\propto e^{\lambda\theta}~\mathbb{I}_{\mathbb{R}_+}(\theta)\)</span> <span class="math inline">\(\propto-\lambda e^{\lambda\theta}~\mathbb{I}_{\mathbb{R}_+}(\theta)~\mathbb{I}_{\mathbb{R}_-}(\lambda)~.\)</span><br />
Como <span class="math inline">\(\text{E}[\theta]=-1/\lambda =\mu~,\)</span> tem-se que <span class="math inline">\(\lambda= -1/\mu\)</span>, isto é, <span class="math inline">\(\theta\sim\text{Exp}(1/\mu)~,\)</span> de modo que <span class="math inline">\(h(\theta)=\dfrac{1}{\mu}e^{-\frac{\theta}{\mu}}~,\)</span> <span class="math inline">\(\mu&gt;0~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 2</strong> <span class="math inline">\(\Theta = \mathbb{R}\)</span> e <span class="math inline">\(\text{E}[\theta]=\mu\)</span> e <span class="math inline">\(\text{Var}(\theta)=\text{E}[(\theta-\mu)^2]=\sigma^2~.\)</span><br />
Tomando <span class="math inline">\(g_1(\theta)=\theta\)</span> e <span class="math inline">\(g_2(\theta)=(\theta-\mu)^2\)</span>, tem-se pelo resultado anterior que<br />
<span class="math inline">\(h(\theta) \propto \exp\left\{\lambda_1\theta+\lambda_2(\theta-\mu)^2\right\}\)</span>
<span class="math inline">\(\propto \exp\left\{\lambda_1\theta+\lambda_2(\theta^2-2\theta\mu+\mu^2)\right\}\)</span>
<span class="math inline">\(\propto \exp\left\{\lambda_2\left[\theta^2-\left(2\mu-\dfrac{\lambda_1}{\lambda_2}\right)\theta\right]\right\}\)</span>
<span class="math inline">\(\propto \exp\left\{\lambda_2\left[\theta-\left(\mu-\dfrac{\lambda_1}{2\lambda_2}\right)\right]^2\right\}~.\)</span><br />
Considere que <span class="math inline">\(\theta\sim N(\mu,\sigma^2)\)</span>, isto é, <span class="math inline">\(f(\theta)=\dfrac{1}{\sqrt{2\pi}~\sigma}~\exp\left\{-\dfrac{1}{2\sigma^2}(x-\mu)^2\right\}\)</span> <span class="math inline">\(\propto \exp\left\{-\dfrac{1}{2\sigma^2}(x-\mu)^2\right\}~.\)</span><br />
Assim, para concluir que a distribuição de máxima entropia nesse caso é a Normal anterior, basta tomar <span class="math inline">\(\mu-\dfrac{\lambda_1}{2\lambda_2}=\mu\)</span> para ver que <span class="math inline">\(\lambda_1=0\)</span> e <span class="math inline">\(\lambda_2=-\dfrac{1}{2\sigma^2}~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<!-- ```{r} -->
<!-- a1=5; b1=12 -->
<!-- a2=10; b2=3 -->
<!-- n=5; x=2 -->
<!-- w=0.5 -->
<!-- t = seq(0,1,length.out = 1000) -->
<!-- A = w*choose(n,x)*gamma(a1+b1)/(gamma(a1)*gamma(b1))* -->
<!--     (gamma(a1+x)*gamma(b1+n-x))/gamma(a1+b1+n) -->
<!-- B = (1-w)*choose(n,x)*gamma(a2+b2)/(gamma(a2)*gamma(b2))* -->
<!--     (gamma(a2+x)*gamma(b2+n-x))/gamma(a2+b2+n) -->
<!-- w2 = A/(A+B) -->
<!-- BL_MaxEnt=dbeta(t,1,1) -->
<!-- BL_MaxEnt_Post=dbeta(t,1+x,1+n-x) -->
<!-- Subj=dbeta(t,2,2) -->
<!-- Subj_Post=dbeta(t,2+x,2+n-x) -->
<!-- Mistura = w*dbeta(t,a1,b1)+(1-w)*dbeta(t,a2,b2) -->
<!-- Mistura_Post = w2*dbeta(t,a1+x,b1+n-x)+(1-w2)*dbeta(t,a2+x,b2+n-x) -->
<!-- Jeffreys_1=dbeta(t,1/2,1/2) -->
<!-- Jeffreys_1_Post=dbeta(t,1/2+x,1/2+n-x) -->
<!-- Jeffreys_2=(t^(-1))*((1-t)^(-1/2))*4/1000 -->
<!-- Jeffreys_2_Post=dbeta(t,0+x,1/2+n-x) -->
<!-- dens=c(BL_MaxEnt,BL_MaxEnt_Post,Subj,Subj_Post,Mistura,Mistura_Post,Jeffreys_1,Jeffreys_1_Post,Jeffreys_2,Jeffreys_2_Post) -->
<!-- dens=ifelse(dens==Inf,NA,dens) -->
<!-- tibble( theta=rep(t,10),dens,Priori=rep(c("B-L_MaxEnt","Beta(2,2)","Mistura","Jeffreys_Bin","Jeffreys_BinNeg"),each=2*length(t)),Distr=rep(rep(c("Prior","Posterior"),each=length(t)),5) ) %>% -->
<!--   ggplot() + theme_bw() + -->
<!--   geom_line(aes(x=theta,y=dens,linetype=Distr,color=Priori)) + -->
<!--   geom_line(aes(x=t,y=dbinom(x,n,t),color="Verossimilhança")) + -->
<!--   facet_grid(~Priori) -->
<!-- ``` -->
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
</div>
</div>
</div>
<div id="alguns-princípios-de-inferência" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Alguns Princípios de Inferência</h2>
<p>Considere um experimento <span class="math inline">\(E=(\boldsymbol{X},\theta,\{f(\boldsymbol x|\theta)\})\)</span> que consiste em observar um particular valor <span class="math inline">\(\boldsymbol{x}\in\mathfrak{X}\)</span> do v.a. <span class="math inline">\(\boldsymbol{X}\)</span> que, para cada possível valor do parâmetro (desconhecido) <span class="math inline">\(\theta\in\Theta\)</span>, tem f.d.p. <span class="math inline">\(f(\boldsymbol x|\theta)\)</span>. De forma geral, uma <em>inferência</em> sobre <span class="math inline">\(\theta\)</span> baseada no resultado <span class="math inline">\(\boldsymbol x\)</span> do experimento <span class="math inline">\(E\)</span> será denotada por <span class="math inline">\(\text{Inf}(E,\boldsymbol x)~.\)</span></p>
<p><strong>Princípio de Suficiência.</strong> Considere um experimento <span class="math inline">\(E=(\boldsymbol{X},\theta,\{f(\boldsymbol{x}|\theta)\})\)</span> e suponha que <span class="math inline">\(T(\boldsymbol{X})\)</span> é uma estatística suficiente para <span class="math inline">\(\theta\)</span>. Se <span class="math inline">\(\boldsymbol x_1\)</span> e <span class="math inline">\(\boldsymbol x_2\)</span> são dois pontos amostrais tais que <span class="math inline">\(T(\boldsymbol{x}_1)=T(\boldsymbol{x}_2)\)</span> então <span class="math inline">\(\text{Inf}(E,\boldsymbol{x}_1)=\text{Inf}(E,\boldsymbol{x}_2)~.\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 1a.</strong> Seja <span class="math inline">\(X_1,\ldots,X_{n}\)</span> c.i.i.d. tais que <span class="math inline">\(X_1\sim Ber(\theta)~.\)</span><br />
Considere <span class="math inline">\(n=10\)</span> e os pontos amostrais <span class="math inline">\(\boldsymbol x_1=(1,1,1,1,1,1,0,0,0,0)\)</span> e <span class="math inline">\(\boldsymbol x_2=(1,0,1,0,1,0,1,0,1,1)\)</span> tais que <span class="math inline">\(T(\boldsymbol x_1)=\sum x_{1i}=6\)</span> e <span class="math inline">\(T(\boldsymbol x_2)=\sum x_{2i}=6\)</span>.<br />
Um possível estimador para <span class="math inline">\(\theta\)</span> nesse exemplo é a média amostral, de modo que <span class="math inline">\(\bar{x}_1=\bar{x}_2=\dfrac{\sum x_i}{n}=0,6~.\)</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo 1b.</strong> Ainda no contexto do exemplo anterior, considere que a priori <span class="math inline">\(\theta \sim \text{Beta}(a,b)~.\)</span> Então, se <span class="math inline">\(T(\boldsymbol x_1)=T(\boldsymbol x_2)=t\)</span>,
<span class="math display">\[\theta|\boldsymbol x_1\sim\theta|\boldsymbol x_2\sim\theta|T(\boldsymbol  x_1)=t~\sim~ Beta(a+t,b+n-t)~.\]</span></p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Princípio da Condicionalidade.</strong> Suponha que <span class="math inline">\(E_1=\left(\boldsymbol X_1,\theta,\{f(\boldsymbol x_1|\theta)\}\right)\)</span> e <span class="math inline">\(E_2=\left(\boldsymbol X_2,\theta,\{f(\boldsymbol x_2|\theta)\}\right)\)</span> são dois experimentos onde somente o parâmetro <span class="math inline">\(\theta\)</span> precisa ser comum. Considere um experimento misto em que é observada uma v.a. <span class="math inline">\(J\)</span>, com <span class="math inline">\(P(J=1)=P(J=2)=1/2\)</span>, independente de <span class="math inline">\(\boldsymbol X_1,~\boldsymbol X_2\)</span> e <span class="math inline">\(\theta\)</span>, e então o experimento <span class="math inline">\(E_J\)</span> é realizado. Formalmente, o experimento realizado nesse caso é <span class="math inline">\(E^*=(\boldsymbol X^*,\theta,\{f^*(\boldsymbol x^*|\theta)\})\)</span>, onde <span class="math inline">\(\boldsymbol X^*=(J,\boldsymbol X_J)\)</span> e <span class="math inline">\(f^*(\boldsymbol x|\theta)=\dfrac{1}{2}~f_j(\boldsymbol x_j|\theta)~.\)</span> Então, <span class="math inline">\(\text{Inf}\left(E^*,(j,x_j)\right) = \text{Inf}\left(E_j,x_j\right)~.\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Princípio da Verossimilhança.</strong> Suponha dois experimentos <span class="math inline">\(E_1=(\boldsymbol X_1,\theta,\{f_1(\boldsymbol x_1|\theta)\})\)</span> e <span class="math inline">\(E_2=(\boldsymbol X_2,\theta,\{f_2(\boldsymbol x_2|\theta)\})\)</span>, ambos com o mesmo parâmetro <span class="math inline">\(\theta\)</span>. Suponha que <span class="math inline">\(\boldsymbol x_1\)</span> e <span class="math inline">\(\boldsymbol x_2\)</span> são pontos amostrais de <span class="math inline">\(E_1\)</span> e <span class="math inline">\(E_2\)</span>, respectivamente, tais que <span class="math inline">\(f_1(\boldsymbol x_1|\theta)\propto c(\boldsymbol x_1,\boldsymbol x_2)f_2(\boldsymbol x_2|\theta)~,\)</span> <span class="math inline">\(\forall \theta\in\Theta~.\)</span>, então, <span class="math inline">\(\text{Inf}(E_1,\boldsymbol x_1)=\text{Inf}(E_2,\boldsymbol x_2)\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Teorema de Birnbaum.</strong> (<em>P. Suficiência</em> <span class="math inline">\(\wedge\)</span> <em>P. Condicionalidade</em>) <span class="math inline">\(\Longleftrightarrow\)</span> <em>P. Verossimilhança</em>.</p>
<blockquote>
<p><strong>Demo:</strong><br />
<span class="math inline">\((\boldsymbol{\Longrightarrow})\)</span><br />
Seja <span class="math inline">\(\boldsymbol x_1^*,\;\boldsymbol x_2^*,\;E_1,\;E_2\)</span> como no <em>P. Verossimilhança</em> e <span class="math inline">\(E^*\)</span> como no <em>P. Condicionalidade</em>. Então,<br />
<span class="math inline">\(f_1(\boldsymbol x_1|\theta)\propto c(\boldsymbol x_1,\boldsymbol x_2)f_2(\boldsymbol x_2|\theta)~.\)</span><br />
No espaço do experimento <span class="math inline">\(E^*\)</span>, defina <span class="math inline">\(T(j,\boldsymbol{x}_j)=\left\{\begin{array}{ll}(1,\boldsymbol{x}_1^*), &amp; \text{se } ~j=1,~\boldsymbol{x}_1=\boldsymbol{x}_1^*\\ (j,\boldsymbol{x}_j), &amp; \text{c. c.} \end{array}\right.~.\)</span><br />
Como <span class="math inline">\(f^*(\boldsymbol x^*|\theta)=f^*\left((j,\boldsymbol x_j)|\theta\right)=1/2~f_j(\boldsymbol x_j|\theta),\)</span> pelo o Teorema da Fatoração é possível concluir que <span class="math inline">\(T(j,\boldsymbol x_j)\)</span> é suficiente para <span class="math inline">\(\theta\)</span> no experimento <span class="math inline">\(E^*\)</span>.<br />
Então, pelo <em>P. Suficiência</em>, <span class="math inline">\(\text{Inf}\left(E^*,(1,\boldsymbol x_1)\right)=\text{Inf}\left(E^*,(2,\boldsymbol x_2)\right)\)</span> e, pelo <em>P. Condicionalidade</em>,<br />
<span class="math inline">\(\text{Inf}\left(E^*,(1,\boldsymbol x_1^*)\right)\)</span> <span class="math inline">\(=\text{Inf}\left(E_1,\boldsymbol x_1^*\right)\)</span> <span class="math inline">\(=\text{Inf}\left(E^*,(2,\boldsymbol x_2)\right)\)</span> <span class="math inline">\(=\text{Inf}\left(E_2,\boldsymbol x_2^*\right)~,\)</span><br />
de modo que <span class="math inline">\(\text{Inf}\left(E_1,\boldsymbol x_1^*\right)=\text{Inf}\left(E_2,\boldsymbol x_2^*\right)\)</span> e, portanto, vale o <em>P. Verossimilhança</em>.<br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\((\boldsymbol{\Longleftarrow})\)</span><br />
Como vale o <em>P. Verossimilhança</em>, <span class="math inline">\(f_1(x_1^*|\theta)\propto f_2(x_2^*|\theta)\)</span> e <span class="math inline">\(\text{Inf}(E_1,x_1^*)=\text{Inf}(E_2,x_2^*)~.\)</span><br />
Além disso, se <span class="math inline">\(x^*=(1,x_1^*)\)</span>,<br />
<span class="math inline">\(f^*(x^*|\theta)\)</span> <span class="math inline">\(=f^*\left((1,x_1^*)|\theta\right)\)</span> <span class="math inline">\(=1/2~f_1(x_1^*|\theta)\)</span> <span class="math inline">\(\propto f_1(x_1^*|\theta)\)</span> <span class="math inline">\(\propto 1/2~f_2(x_2^*|\theta)\)</span> <span class="math inline">\(=f^*\left((2,x_2^*)|\theta\right)~,\)</span><br />
e, como vale <em>P. Verossimilhança</em>, então <span class="math inline">\(\text{Inf}\left(E^*,(1,x_1^*)\right)=\text{Inf}(E_1,x_1^*)~.\)</span><br />
Usando o mesmo argumento, se <span class="math inline">\(x^*=(2,x_2^*)\)</span>, conclui-se que <span class="math inline">\(\text{Inf}\left(E^*,(2,x_2^*)\right)=Inf(E_2,x_2^*)~.\)</span><br />
Portando, vale o <em>P. Condicionalidade</em>.<br />
<span class="math inline">\(~\)</span><br />
Pelo Teorema de Fatoração, <span class="math inline">\(f(\boldsymbol x|\theta)\)</span> <span class="math inline">\(=g\left(T(\boldsymbol x),\theta\right)h(\boldsymbol x)\)</span> <span class="math inline">\(\propto g\left(T(\boldsymbol x),\theta\right)~.\)</span><br />
Se <span class="math inline">\(\boldsymbol x_1\)</span> e <span class="math inline">\(\boldsymbol x_2\)</span> são pontos amostrais tais que <span class="math inline">\(T(\boldsymbol x_1)=T(\boldsymbol x_2),\)</span> <span class="math inline">\(f_1(\boldsymbol x_1|\theta)\propto g\left(T(\boldsymbol x_1),\theta\right)\)</span> <span class="math inline">\(\propto g(T(\boldsymbol x_2),\theta)\)</span> <span class="math inline">\(\propto f_2(\boldsymbol x_2|\theta)~,\)</span> tem-se, pelo <em>P. Verossimilhança</em>, que <span class="math inline">\(\text{Inf}(E_1,\boldsymbol x_1)=\text{Inf}(E_2,\boldsymbol x_2)\)</span> e, portanto vale o <em>P. Suficiência</em>.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>
<p><span class="math inline">\(~\)</span></p>
<blockquote>
<p><strong>Exemplo.</strong> Seja <span class="math inline">\(X_1|\theta \sim Bin(n,\theta)\)</span> e <span class="math inline">\(X_2|\theta \sim BinNeg(r,\theta)\)</span>, onde <span class="math inline">\(n\)</span> é número total de lançamentos (fixado) e <span class="math inline">\(r\)</span> é número de fracassos (fixado). Então, <span class="math inline">\(E_1=\left(\boldsymbol X_1,\theta,\left\{\binom{n}{x_1}\theta^{x_1}(1-\theta)^{n-x_1}:\theta\in[0,1]\right\}\right)\)</span> e <span class="math inline">\(E_2=\left(\boldsymbol X_2,\theta,\left\{\binom{r+x_2-1}{x_2}\theta^{x_2}(1-\theta)^{r}:\theta\in[0,1]\right\}\right)~.\)</span> Note que em ambos os experimentos, o parâmetro <span class="math inline">\(\theta\)</span> é o mesmo!<br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(~\)</span><br />
<strong>(I)</strong> Estimação pontual usando Estimador Não-Viesado (ENV) para <span class="math inline">\(\theta\)</span>, isto é, <span class="math inline">\(\hat{\theta}_i(X_i)\)</span> tal que <span class="math inline">\(E\left[\hat{\theta}_i(X_i)|\theta\right]=\theta\)</span>. Nesse caso, <span class="math inline">\(\text{Inf}(E_i,x_i)=\hat{\theta}_i(x_i)\)</span> para <span class="math inline">\(i=1,2\)</span>.<br />
Então, <span class="math inline">\(\hat{\theta}_1(X_i)=\dfrac{X_1}{n}\)</span> e <span class="math inline">\(\hat{\theta_2}(X_2)=\dfrac{X_2-1}{X_2+r-1}\)</span> são ENV para <span class="math inline">\(\theta\)</span> em <span class="math inline">\(E_1\)</span> e <span class="math inline">\(E_2\)</span>, respectivamente.<br />
Suponha que <span class="math inline">\(n=12,r=3\)</span> e <span class="math inline">\(x_1=x_2=9\)</span>. Então, as funções de verossimilhança são <span class="math inline">\(f_1(x_1|\theta)=\binom{12}{9}\theta^9(1-\theta)^3\)</span> <span class="math inline">\(\propto \binom{11}{9}\theta^9(1-\theta)^3=f_2(x_2|\theta)\)</span>. Contudo, <span class="math inline">\(\hat{\theta}_1(x_1)=\dfrac{9}{12}=0,75\)</span> <span class="math inline">\(\neq\hat{\theta}_2(x_2)=\dfrac{8}{11}\approx0,72\bar{72}\)</span>, e portanto, o ENV <strong>viola</strong> o <em>P. Verossimilhança</em>.<br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(~\)</span><br />
<strong>(II)</strong> Estimador de Máxima Verossimilhança (EMV)<br />
<span class="math inline">\(\delta_{MV}\)</span> é um estimador tal que <span class="math inline">\(\delta_{MV}(\boldsymbol x)=\displaystyle\arg\sup_{\theta\in\Theta} f(\boldsymbol x|\theta)~.\)</span><br />
<span class="math inline">\(\delta_{MV}^1(x_1)=\dfrac{x_1}{n}\)</span> <span class="math inline">\(=\delta^2_{MV}(x_2)=\dfrac{x_2}{x_2+r}\)</span> <span class="math inline">\(=\dfrac{9}{12}=0,75~.\)</span><br />
Portanto, o EMV <strong>não viola</strong> o <em>P. Verossimilhança</em>.<br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(~\)</span><br />
<strong>(III)</strong> Suponha que deseja-se testar <span class="math inline">\(H_0:\theta\leq 1/2 \; (\Theta_0)\)</span> contra <span class="math inline">\(H_1:\theta &gt; 1/2 \; (\Theta_1)~,\)</span> com <span class="math inline">\(\Theta=\Theta_0 \cup \Theta_1~.\)</span><br />
<span class="math inline">\(\phi(x)=\left\{\begin{array}{ll} 1,&amp; T(x)\leq c(\alpha)\\ 0,&amp; T(x)&gt; c(\alpha)\end{array}\right.\)</span><br />
em que <span class="math inline">\(T\)</span> é uma estatística de teste (isto é, valores “grandes” de <span class="math inline">\(T(x)\)</span> indicam que <span class="math inline">\(x\)</span> é “favorável” a <span class="math inline">\(H_0\)</span>) e <span class="math inline">\(c(\alpha)\)</span> é tal que <span class="math inline">\(\alpha=\displaystyle\sup_{\theta_0\in\Theta_0}\text{P}(\text{Rejeitar } H_0~|~\theta_0)\)</span> <span class="math inline">\(\displaystyle\sup_{\theta_0\in\Theta_0}\text{P}\left(\left\{x\in\mathfrak{X} : T(x)\leq c(\alpha)\right\}~|~\theta_0\right)~.\)</span><br />
Considere <span class="math inline">\(T(x)=RV(x)=\dfrac{\underset{\Theta_0}{sup}f(x|\theta)}{\underset{\Theta}{sup}f(x|\theta)}\)</span>, de modo que um <span class="math inline">\(p\)</span><em>-value</em> pode ser calculado por <span class="math inline">\(p(x)=\displaystyle\sup_{\Theta_0} \text{P}\left(T(X)\geq T(x)|\theta\right)\)</span>. Assim, um teste que conduz a uma decisão equivalente ao descrito anteriormente é <em>rejeitar</em> <span class="math inline">\(H_0\)</span> se, e somente se, <span class="math inline">\(p(x)\leq \alpha\)</span>. Considere a escolha usual <span class="math inline">\(\alpha=0.05\)</span>. Então,<br />
<span class="math inline">\(p_1(x_1)=P(X_1\geq 9|\theta=1/2)=0.073&gt;0.05\Rightarrow\)</span> Não rejeita <span class="math inline">\(H_0~.\)</span><br />
<span class="math inline">\(p_2(x_2)=P(X_2\geq 9|\theta=1/2)=0.0327&lt;0.05\Rightarrow\)</span> Rejeita <span class="math inline">\(H_0~.\)</span><br />
Portanto, o Teste da Razão de Verossimilhanças viola o <em>P. Verossimilhança</em>.<br />
<span class="math inline">\(~\)</span><br />
<span class="math inline">\(~\)</span></p>
</blockquote>
<blockquote>
<p><strong>(IV)</strong> Aboragem Bayesiana <span class="math inline">\(\Rightarrow Inf(E_i,x_i)=f_i(\theta|x_i)\)</span><br />
a) <em>Bayesiano Subjetivista</em><br />
Como o parâmetro <span class="math inline">\(\theta\)</span> é o mesmo nos dois experimentos, a priori deve ser a mesma.<br />
<span class="math inline">\(f(\theta)\)</span> não depende de <span class="math inline">\(\{f_i(\boldsymbol x|\theta):\theta\in\Theta\}\)</span><br />
<span class="math inline">\(f(\theta|\boldsymbol x)\propto f(\theta)f(x_1|\theta)\propto f(\theta)f(x_2|\theta)\)</span><br />
e, portanto, satisfaz o <em>P. Verossimilhança</em>.<br />
<span class="math inline">\(~\)</span><br />
b) <em>Bayesiano Objetivista</em> (p.e., usando priori de Jeffreys)<br />
Para <span class="math inline">\(E_1\)</span>, <span class="math inline">\(f_1(\theta)\propto |I_F(\theta)|^{1/2}\propto\)</span> <span class="math inline">\(\theta^{-1/2}(1-\theta)^{-1/2}\sim Beta (1/2,1/2)\)</span><br />
Para <span class="math inline">\(E_2\)</span>, <span class="math inline">\(f_2(\theta)\propto\)</span> <span class="math inline">\(\theta^{-1}(1-\theta)^{-1/2}\sim Beta (0,1/2)\)</span> (distribuição imprópria).<br />
Se o número de sucessos é <span class="math inline">\(x=x_1=x_2\)</span> e número de fracassos é <span class="math inline">\(y=n-x_1=r\)</span>, temos que<br />
<span class="math inline">\(\theta|X_1=x_1\sim Beta(x+1/2,y+1/2)~\)</span> e <span class="math inline">\(~\theta|X_2=x_2\sim Beta(x,y+1/2)~.\)</span><br />
Como <span class="math inline">\(f_1(x_1|\theta)\propto f_2(x_2|\theta)\)</span> mas <span class="math inline">\(f_1(\theta)\neq f_2(\theta)\)</span>, tem-se que <span class="math inline">\(f_1(\theta|x_1)\neq f_2(\theta|x_2)\)</span> e, portanto, esse procedimento viola o <em>P. Verossimilhança</em>.</p>
</blockquote>
<p><span class="math inline">\(~\)</span></p>

</div>
</div>
<h3>Referências</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Albert09" class="csl-entry">
Albert, Jim. 2009. <em>Bayesian Computation with r</em>. Springer.
</div>
<div id="ref-Jaynes03" class="csl-entry">
Jaynes, E. T. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge Univ. Press.
</div>
<div id="ref-Kullback51" class="csl-entry">
Kullback, S., and R. A. Leibler. 1951. <span>“On Information and Sufficiency.”</span> <em>The Annals of Mathematical Statistics</em> 22 (1): 79–86.
</div>
<div id="ref-Schervish12" class="csl-entry">
Schervish, M. J. 2012. <em>Theory of Statistics</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-Shannon48" class="csl-entry">
Shannon, C. E. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>The Bell System Technical Journal</em> 27 (3): 379–423. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ProbSubj.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="TeoDec.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["InfBayes.pdf", "InfBayes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
